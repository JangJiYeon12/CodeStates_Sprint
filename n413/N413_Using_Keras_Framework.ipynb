{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N413_Using_Keras_Framework.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux75wlubDlpP"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyFFTzxjDoAC"
      },
      "source": [
        "# N413. 신경망 구현을 위한 프레임워크 - Tensorflow, Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3BER3dDy8d"
      },
      "source": [
        "## 🛫 Warm Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLDv5OiXD02j"
      },
      "source": [
        "다음 영상을 시청해 주세요 :)\n",
        "\n",
        "- [Overfitting, Regularization](https://youtu.be/_sz3KTyB9Lk?t=1005) - 딥러닝 홀로서기 : Regularization 에 대한 설명 영상입니다. **16:45 - 34:00(약 17분)**만 시청하도록 합시다.\n",
        "    - [L1 and L2 Regularization Methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) : 대표적인 과적합 방지 방법으로 가중치 감소(Weight Decay)가 있는데요.<br/>이에 해당하는 L1/L2 Regularization 에 대해 다룬 글입니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6rtWGerD2N1"
      },
      "source": [
        "### 지난 시간 내용 복습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llptNHKdD6Rw"
      },
      "source": [
        "- **신경망이 학습하는 원리**\n",
        "    1. 데이터 전처리 및 입력\n",
        "    2. 모델 제작 및 가중치 초기화\n",
        "    3. 모델에 데이터를 넣고 출력값(Output)을 얻음 : 순전파\n",
        "    4. 출력값과 레이블(정답지, Target)과 비교 후 손실(Loss) 계산\n",
        "    5. 손실 정보를 반영하여 가중치 업데이트 -> 역전파(BackPropagation) + 경사하강법(Gradient Descent)\n",
        "\n",
        "- **역전파 원리 및 실습**\n",
        "    - 손실 함수(Loss function)의 계산 방식\n",
        "    - 옵티마이저(Optimizer)\n",
        "        - 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
        "        - 경사하강법의 변형들(Adam)\n",
        "    - 2x2x2 neural network의 역전파 수학식\n",
        "\n",
        "- **Keras 를 사용한 Fashion MNIST 예제 실습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQxQ_B7i0c5B"
      },
      "source": [
        "## 🏆 학습목표"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww2ZnWFwEIg9"
      },
      "source": [
        "- 케라스를 이용하여 모델 구조를 구축해보고 각 함수(메서드)의 의미를 이해한다.\n",
        "- 과적합(Overfitting) 방지를 위한 **가중치 Regularization**의 방법을 알아보고 각 방법의 정의와 특징을 설명할 수 있습니다. \n",
        "- 다양한 **활성화 함수(Activation function)**를 알아보고 특정 활성화 함수가 어떤 경우에 사용 되는지 설명할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwbGO4htEHZw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZpXkmrZ-_GL"
      },
      "source": [
        "## 딥러닝 라이브러리를 사용해봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCvt5WsFBH0Z"
      },
      "source": [
        "첫 번째 강의에서는 **신경망의 배경 및 기초 개념, 용어, 구조**를 배웠습니다.<br/>\n",
        "그리고 두 번째 강의에서는 **순전파와 역전파의 개념, 손실 함수와 경사 하강법, 옵티마이저 등**에 대해서 배웠습니다.\n",
        "\n",
        "그리고 파이썬 코드를 사용하여 **퍼셉트론(Perceptron)과 간단한 신경망을 직접 코딩**해보고<br/>\n",
        "**케라스(Keras)를 통해 예제를 수행**해보며 신경망으로 어떤 일을 수행할 수 있는 지에 대해서도 알아보았습니다.\n",
        "\n",
        "케라스와 같은 도구 없이 수작업으로 한 땀, 한 땀 신경망을 만드는 것도 의미있는 일입니다.<br/>\n",
        "하지만 **<font color=\"ff6f61\">딥러닝의 목적은 거대하고 복잡한 모델로 어려운 문제를 푸는 데</font>**에 있는 만큼<br/>\n",
        "케라스를 활용하여 어떻게 신경망을 더 빠르게 만들 수 있을지에 대해 알아보겠습니다.\n",
        "\n",
        "실무에서 사용할 예측 모델 역시 모두 이러한 케라스와 같은 도구를 사용하여 만들어졌는데요.<br/>\n",
        "***이번 강의에서는 케라스와 친해져보는 시간을 가져보겠습니다. :)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGTpTxQBBJ0J"
      },
      "source": [
        "> ❓ ***(들어가기에 앞서) 어떤 모델이 더 좋은 모델일까요?<br/><br/>\n",
        "딥러닝 연구자 중 일부는 \"신경망 설계는 과학보다는 예술에 가깝다\"고 말하기도 합니다.<br/>\n",
        "또는 속된 말로 \"노가다\"라고 말하기도 하죠. 0.1%의 성능을 올리기 위해서 수많은 작업을 수행하기 때문입니다.<br/>\n",
        "데이터셋마다 높은 성능을 보이는 신경망 구조가 달라지기 때문에 신경망으로 좋은 결과를 만들어내기 위해서는 수많은 실험이 필요한데요.<br/>\n",
        "이런 사실을 기억하면서 아래 내용을 학습해보도록 합시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkN73cHwSzvV"
      },
      "source": [
        "### 간단한 신경망을 만들어봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04j6FMOS6fJ"
      },
      "source": [
        "- **케라스(Keras)를 이용하여 간단한 신경망을 만들어봅시다.**\n",
        "\n",
        "> ❗️ ***아래 코드에는 주석이 잘 달려있는데요.<br/>\n",
        "여러분이 작성한 코드를 다른 사람이 보고 이해하기 위해서는 주석을 잘 작성해주어야 합니다.<br/>\n",
        "앞으로 코드를 짤 때마다 주석과 docstring을 잘 작성해주도록 합시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gw8pphsTrWP"
      },
      "source": [
        "# 파일 선택을 통해 예제 데이터를 내 컴퓨터에서 불러오는 코드를 포함(주석)\n",
        "# 강의 목적상 내 데이터를 대신하여 서버에서 불러오도록 하겠습니다. 직접 가지고 있는 데이터를 사용하기 위해서는 주석처리된 files.upload()를 이용하시면 됩니다.\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload() # 파일을 불러올 수 있는 코드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHcrZGTPB_C3"
      },
      "source": [
        "1. **필요한 패키지 및 라이브러리를 불러옵니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqRFnRUdTrIa"
      },
      "source": [
        "# 케라스 외의 필요한 라이브러리를 불러옵니다.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# 딥러닝을 구동하는 데 필요한 케라스 함수(메서드)를 불러옵니다.\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import tensorflow.keras.layers as Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1_kw336CE2h"
      },
      "source": [
        "2. **재현을 위해서 시드를 고정합니다. Numpy 뿐만 아니라 Tensorflow 의 시드도 고정해주어야 합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoP9v8MyBNLv"
      },
      "source": [
        "# 실행할 때마다 같은 결과를 출력하기 랜덤함수를 고정하는 부분입니다.\n",
        "# 랜덤함수의 Seed를 고정하게 되면 랜덤함수가 항상 일정하게 나옵니다. \n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9GiN5g0DA_2"
      },
      "source": [
        "3. **데이터를 불러온 후 Feature와 Label 을 나누어 줍니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkOEMPPGTrQH"
      },
      "source": [
        "# 폐암 수술 환자의 특정기간 생존 데이터\n",
        "# 속성(정보)은 종양의 유형, 폐활량, 호흡곤란 여부, 기침, 흡연, 천식여부 등의 17가지 환자 상태. 수술 후 생존(1), 사망(0) \n",
        "my_data = \"https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/everydeep/ThoraricSurgery.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuPethrFTy4c"
      },
      "source": [
        "# 불러온 데이터를 적용합니다.\n",
        "# pandas외에도 읽을 수 있는 방법이 있습니다. 편하신 방법을 사용하시면 됩니다.\n",
        "Data_set = np.loadtxt(my_data, delimiter=\",\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG1dJQGBD_l1"
      },
      "source": [
        "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.\n",
        "X = Data_set[:,0:17]\n",
        "Y = Data_set[:,17]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga-xXvWJEB3c"
      },
      "source": [
        "4. **신경망을 구축합니다.**\n",
        "\n",
        "> ❗️ ***이번에도 코드에서 출력층의 노드 수는 몇 개인지, 출력층의 활성화 함수는 무엇인지, 손실 함수는 어떻게 지정하였는지에 주목해봅시다.<br/>\n",
        "그리고 왜 그렇게 지정하여 주었는지 생각해보고 토론해봅시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5JF2QMfBY64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d96511-d131-4997-8d7d-7bae8cd4d173"
      },
      "source": [
        "# 딥러닝 구조를 결정합니다(모델을 설정하고 실행하는 부분입니다).\n",
        "model = Sequential([\n",
        "    Dense(30, activation='relu'),\n",
        "    Layer.Dropout(0.5),\n",
        "    Dense(30, activation='relu'),\n",
        "    Dense(1, activation='sigmoid') # 분류할 방법에 따라 개수를 조정해야 합니다. \n",
        "])\n",
        "# 딥러닝을 실행합니다.\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='adam', metrics=['accuracy']) # mean_squared_error # binary_crossentropy # mean_absolute_error # poisson\n",
        "history = model.fit(X, Y, epochs=30, batch_size=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 0.1633 - accuracy: 0.6404\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 0.7830\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.8319\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.8447\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.8383\n",
            "Epoch 6/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.8468\n",
            "Epoch 7/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.8468\n",
            "Epoch 8/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0725 - accuracy: 0.8468\n",
            "Epoch 9/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.8468\n",
            "Epoch 10/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.8511\n",
            "Epoch 11/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.8468\n",
            "Epoch 12/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.8447\n",
            "Epoch 13/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.8447\n",
            "Epoch 14/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.8489\n",
            "Epoch 15/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.8489\n",
            "Epoch 16/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.8511\n",
            "Epoch 17/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.8511\n",
            "Epoch 18/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.8489\n",
            "Epoch 19/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.8511\n",
            "Epoch 20/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.8489\n",
            "Epoch 21/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.8511\n",
            "Epoch 22/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.8511\n",
            "Epoch 23/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0718 - accuracy: 0.8511\n",
            "Epoch 24/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.8489\n",
            "Epoch 25/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.8489\n",
            "Epoch 26/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.8511\n",
            "Epoch 27/30\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.8532\n",
            "Epoch 28/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.8511\n",
            "Epoch 29/30\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.8489\n",
            "Epoch 30/30\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.8511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32WvpyvhSoZ1"
      },
      "source": [
        "### 손실 함수(Cost function) 혹은 오차 함수(Error function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXCjU6MNUruc"
      },
      "source": [
        "문제의 종류에 따라 손실 함수를 잘 선택해주어야 합니다.\n",
        "\n",
        "손실 함수의 종류는 크게 2가지로 나눌 수 있는데요.<br/>\n",
        "평균 제곱을 기반으로 하는 손실 함수가 있고 엔트로피를 기반으로 하는 손실 함수가 있습니다.\n",
        "\n",
        "이진 분류, 다중 분류, 회귀 문제에서 각각 어떤 종류의 손실 함수를 선택해주어야 할 지를 알아보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdl4SOqOVc_I"
      },
      "source": [
        "- **평균 제곱을 기반으로 하는 손실 함수**\n",
        "\n",
        "    1. MSE(Mean Squared Error)\n",
        "$$\n",
        "    \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}\n",
        "$$\n",
        "\n",
        "    2. RMSE(Root Mean Squared Error)\n",
        "$$\n",
        "    \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^{2}}\n",
        "$$\n",
        "\n",
        "    3. MAE(Mean Absolute Error)\n",
        "$$\n",
        "    \\frac{1}{n}\\sum_{i=1}^{n}\\left \\vert y_{i} - \\hat{y_{i}} \\right \\vert\n",
        "$$\n",
        "\n",
        "    4. 이 외에도 R-Squared 등의 오차 함수를 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxQTbhv1BL25"
      },
      "source": [
        "- **엔트로피를 기반으로 하는 손실 함수**\n",
        "\n",
        "    1. BCE(Binary Crossentropy)\n",
        "$$\n",
        "    -\\sum_{c=1}^{C} q(y_c) log(q(y_c)), \\quad q(y_c) \\in (1, -1)\n",
        "$$\n",
        "\n",
        "    2. CCE(Categorical Crossentropy)\n",
        "$$\n",
        "    -\\sum_{c=1}^{C} q(y_c)log(q(y_c)) \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqcVIPy0W551"
      },
      "source": [
        "> ❗️ ***나중에 [케라스에서 제공하는 여러 가지 손실 함수](https://keras.io/api/losses/)에 대해서도 둘러보도록 합시다.<br/>\n",
        "(당연히 모든 손실 함수를 외울 필요는 없습니다.)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmiq3F2iBeoL"
      },
      "source": [
        "## 과적합을 방지하기 위한 방법 (Regularization Strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XST54wtZXV0s"
      },
      "source": [
        "신경망(Neural Network)는 매개 변수가 상당히 많은 복잡한 모델입니다.<br/>\n",
        "이런 모델은 머신 러닝에서 배웠던 것처럼 **<font color=\"ff6f61\">훈련 데이터에 쉽게 과적합(Overfit)</font>**되는 경향이 있습니다.<br/>\n",
        "신경망에서 과적합을 방지하는 방법에 대해서 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJvlyNZHBgIO"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/1920px-Regularization.svg.png\" width = \"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foM-_A66S-hK"
      },
      "source": [
        "### 일반적인 Regularization 방법들"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3ru4YF_T5M5"
      },
      "source": [
        "이번 시간에는 신경망에 쉽게 적용할 수 있는 네 가지 Regularization 방법에 대해서 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQgw3J6WUFDr"
      },
      "source": [
        "1. 첫 번째 방법은 **<font color=\"ff6f61\">조기 종료(Early Stopping)</font>**입니다.<br/>\n",
        "아래 그림에서 볼 수 있는 것처럼 **학습(Train) 데이터에 대한 손실은 계속 줄어들지만<br/>\n",
        "검증(Validation) 데이터셋에 대한 손실은 증가한다면 학습을 종료**하도록 설정하는 방법입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWQF_TG2Bjz3"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/474/1*wZg_RQHPRtn62dDp2Ez86A.jpeg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDu_JeCwVyO1"
      },
      "source": [
        "2. 두 번째 방법은 **<font color=\"ff6f61\">가중치 감소(Weight Decay)</font>**입니다.<br/>\n",
        "과적합은 가중치의 값이 클 때 주로 발생하는데요.<br/>\n",
        "가중치 감소에서는 **손실 함수(Cost function)에 가중치와 관련된 항을 추가하여 값이 너무 커지지 않도록 조정**합니다.<br/>\n",
        "가중치 항을 어떻게 추가할지에 따라 L1 Regularization, L2 Regularization 으로 나뉩니다.<br/>\n",
        "(손실 함수 식이 어떻게 변형되는 지는 아래에서 자세히 알아보겠습니다.)\n",
        "\n",
        "> ❗️ ***머신 러닝에서 배웠던 Ridge, Lasso 와 동일한 개념입니다. 둘 중 어느 것이 L1, L2 일지 생각해봅시다.***\n",
        "\n",
        "> ❗️ ***아래 그림에 대해서 당장 이해하지 못해도 좋습니다. 이해가 안된다면 일단은 넘어가봅시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYaOV8_3VsUj"
      },
      "source": [
        "<img src=\"https://camo.githubusercontent.com/7d9e05f214d77fcb9ce7b13d56448b51ed4169ed63495778678c201da61f4436/68747470733a2f2f692e737461636b2e696d6775722e636f6d2f6f564a44422e706e67\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3yBwSLkdw4-"
      },
      "source": [
        "3. 세 번째 방법은 **<font color=\"ff6f61\">가중치 제한(Weight Constraint)</font>**입니다.<br/>\n",
        "가중치 감소와 유사한 목적을 달성하기 위해서 특정 가중치를 제거하거나 범위를 제한하는 방법입니다.<br/>\n",
        "유사한 방법으로 Weight Decusion and Weight Restriction 이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPqO0dqd6qx"
      },
      "source": [
        "\n",
        "\n",
        "4. 마지막 방법은 **<font color=\"ff6f61\">드롭아웃(Dropout)</font>**입니다.<br/>\n",
        "[Dropout](https://m.blog.naver.com/PostView.nhn?blogId=isu112600&logNo=221578533182&proxyReferer=https:%2F%2Fwww.google.com%2F)은 iteration 마다 **레이어 노드 중 일부를 사용하지 않으면서 학습을 진행하는 방법**입니다.<br/>\n",
        "매 번 다른 노드가 학습되면서 전체 가중치가 과적합되는 것을 방지할 수 있습니다.<br/>\n",
        "아래는 Dropout을 적용했을 때의 신경망에서 학습되는 노드를 나타낸 그림입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w_Fx_UKe487"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2NTuuXBf4rk"
      },
      "source": [
        "### Keras 로 구축한 신경망에 조기 종료(Early Stopping) 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmM2shRVgBON"
      },
      "source": [
        "Fashion Mnist 데이터셋을 학습할 신경망에 조기 종료(Early Stopping)를 적용하여 보겠습니다.<br/>\n",
        "대부분의 코드는 이전 강의에서 했던 예제와 동일합니다. Early Stopping 이 적용된 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqzU0lDGHAL"
      },
      "source": [
        "1. **데이터셋을 불러옵니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHcSlQ-qBfN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536872df-e4a5-426c-c3c8-9a88ec32427b"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# 데이터 불러오기\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "# 데이터를 정규화 합니다\n",
        "X_train = X_train / 255.\n",
        "X_test = X_test /255.\n",
        "\n",
        "# 클래스를 확인합니다.\n",
        "np.unique(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (10000, 28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZYAJhXfHxee"
      },
      "source": [
        "2. **필요한 라이브러리를 불러온 후 모델을 구축합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UJM9JUIDSEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d85bc3-d2a1-4510-81fb-1674d12a8440"
      },
      "source": [
        "# 기본적인 신경망을 만드는 코드\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import keras, os\n",
        "\n",
        "# 모델 구성을 확인합니다.\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "# 업데이트 방식을 설정합니다.\n",
        "model.compile(optimizer='adam'\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])\n",
        "model.summary()\n",
        "# 총 7850 parameters (10 bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbU1Iv0gJ7Tx"
      },
      "source": [
        "3. **조기 종료(Early Stopping)를 위한 콜백 함수를 지정하고 모델을 학습합니다.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1XlC-eLBqJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e607f6-399c-4752-f7b1-6763a7e2df7d"
      },
      "source": [
        "# 모델 학습을 위한 코드\n",
        "\n",
        "# 변수 설정을 따로 하는 방법을 적용하기 위한 코드입니다. \n",
        "batch_size = 30\n",
        "epochs_max = 1\n",
        "\n",
        "# 학습시킨 데이터를 저장시키기 위한 코드입니다. \n",
        "checkpoint_filepath = \"FMbest.hdf5\"\n",
        "\n",
        "# overfitting을 방지하기 위해서 학습 중 early stop을 수행하기 위한 코드입니다.\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "# Validation Set을 기준으로 가장 최적의 모델을 찾기 위한 코드입니다.\n",
        "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "    save_weights_only=True, mode='auto', save_freq='epoch', options=None)\n",
        "\n",
        "# 모델 학습 코드 + early stop + Best model\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs_max, verbose=1, \n",
        "          validation_data=(X_test,y_test), \n",
        "          callbacks=[early_stop, save_best])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5993 - accuracy: 0.7951 - val_loss: 0.5107 - val_accuracy: 0.8221\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.51069, saving model to FMbest.hdf5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f26eb3c50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hi-rd0GKMwg"
      },
      "source": [
        "4. **학습한 모델을 평가(Evaluation)합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23YVV6qoBsA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdbecb2-01ea-4a59-c052-c3de58dc325e"
      },
      "source": [
        "# 학습된 모델을 이용하여 테스트하는 코드\n",
        "\n",
        "model.predict(X_test[0:1])\n",
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 0s - loss: 0.5107 - accuracy: 0.8221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E980Nh9wKwUV"
      },
      "source": [
        "5. **콜백(Callback)에 의해 함수가 체크포인트에서 제대로 저장 되었는지를 확인하고 해당 모델로 평가를 진행합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X71Li6tHBtVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36401a0-90f2-48c6-bf58-83c995f9a9da"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ds-lecture-data.s3.ap-northeast-2.amazonaws.com  FMbest.hdf5  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQFleciGBt1s"
      },
      "source": [
        "# 체크포인트에 저장된 가중치들을 불러들이는 코드\n",
        "\n",
        "model.load_weights(checkpoint_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10nIbEzqBvGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efd6018-131e-4e7f-df9d-ff97eca4e439"
      },
      "source": [
        "# best model을 이용한 테스트 데이터 예측 정확도 재확인 코드\n",
        "\n",
        "model.predict(X_test[0:1])\n",
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.8221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxfQH4oBodh"
      },
      "source": [
        "## 케라스(Keras)에서 Regularization 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGQy1usCiXI5"
      },
      "source": [
        "### 가중치 감소(Weight Decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9-NDUfuLG97"
      },
      "source": [
        "- 다음과 같은 코드로 이루어진 가중치 감소를 적용하여 보겠습니다.<br/>\n",
        "```\n",
        "Dense(64, input_dim=64,\n",
        "            kernel_regularizer=regularizers.l2(0.01),\n",
        "            activity_regularizer=regularizers.l1(0.01))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hX2awX-B1vu"
      },
      "source": [
        "- **L1 Regularization**\n",
        "\n",
        "$$\n",
        "    L(\\theta_w) = \\frac{1}{2} \\sum_i (output_i - target_i)^2 + \\color{blue}{\\lambda} \\cdot \\color{red}{\\vert \\theta_w \\vert}\n",
        "$$ \n",
        "\n",
        "- **L2 Regularization**\n",
        "\n",
        "$$\n",
        "    L(\\theta_w) = \\frac{1}{2} \\sum_i (output_i - target_i)^2 + \\color{blue}{\\lambda} \\cdot \\color{red}{\\Vert \\theta_w \\Vert_2}\n",
        "$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKuy2fCxB2Z5"
      },
      "source": [
        "먼저 가중치 감소(Weight decay)를 케라스에서 수행하여 보겠습니다.<br/>\n",
        "가중치 감소 방식에서는 위와 같이 손실 함수에 가중치와 관련된 항을 추가하여 손실 함수를 최적화 하는 과정에서 가중치가 너무 커지지 않도록 조정합니다.\n",
        "\n",
        "아래 그림을 보면 과적합(Over-fitting)을 나타낼 때 구불구불한 경계면이 형성되는 것을 확인할 수 있습니다.<br/>\n",
        "이렇게 굽이치는 그래프가 형성될 때의 가중치는 커지기 마련인데요.<br/>\n",
        "가중치 감소는 애초부터 가중치가 큰 값을 갖지 못하게 하여 과적합이 나타나지 않도록 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRLu_TBuks0t"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/1400/0*CmDTGlQyibHUORQ0.png\" width=\"800\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbN_7g1vkrKg"
      },
      "source": [
        "위에서 제시된 손실 함수 식을 다시 보도록 하겠습니다.<br/>\n",
        "Regularization 이 적용되지 않은 손실 함수는 출력값(output)과 타겟(Target)의 차이(의 제곱)만을 손실로 계산합니다.<br/>\n",
        "가중치 감소를 적용하면 **손실 함수에 가중치$(\\theta)$로 구성된 항이 추가**됩니다.<br/>\n",
        "이렇게 되면 손실 함수를 최적화 하는 과정에서 **가중치$(\\theta)$의 크기가 커지는 것을 막기 때문에 과적합을 방지**할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnhDN3aNzmq"
      },
      "source": [
        "1. **(데이터는 이전의 것을 그대로 사용하므로) 가중치 감소가 적용된 신경망 모델을 구축합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww8zzG6vB6Nd"
      },
      "source": [
        "# 가중치 감소(Weight Decay)를 반영한 예시 코드\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# 모델 구성을 확인합니다.\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(64, \n",
        "            kernel_regularizer=regularizers.l2(0.01),    # L2 Regularization 적용\n",
        "            activity_regularizer=regularizers.l1(0.01)), # L1 Regularization 적용 (kernel, activity 의 차이에 대해서는 일단 넘어갑시다.)\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B4wdEHaNuvn"
      },
      "source": [
        "# 업데이트 방식을 설정합니다.\n",
        "model.compile(optimizer='adam'\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXVJ7eZPPUk"
      },
      "source": [
        "2. **`.summary()` 함수를 통해서 모델이 잘 구축되었는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teV1ZFCBNwe1",
        "outputId": "28d05b70-e711-4748-b898-5502f436abdb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMlgtuHCPb5S"
      },
      "source": [
        "3. **새로운 모델을 학습하고 결과가 어떻게 달라졌는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XciM9qviNx8I",
        "outputId": "733d8bff-a609-4877-ad64-64f53641b984"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 5s 2ms/step - loss: 1.0249 - accuracy: 0.7979 - val_loss: 0.8034 - val_accuracy: 0.8059\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f2874b5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSyv7n5cB-or"
      },
      "source": [
        "### 가중치 제한(Weight Constraints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPEB-lkICAcp"
      },
      "source": [
        "가중치 감소에서는 손실 함수에 가중치 항을 추가함으로써 가중치가 커지는 것을 방지하였습니다.<br/>\n",
        "그렇다면 가중치의 범위를 직접 제한하여 가중치 값이 커지는 것을 막을 수도 있지 않을까요?<br/>\n",
        "가중치 제한(Weight Constraints)은 이렇게 가중치의 범위를 수동으로 제한하는 방법입니다.\n",
        "\n",
        "> ❗️ ***케라스에서는 여러 가지 가중치 제한 방법을 제공하고 있습니다. 나중에 [링크](https://keras.io/api/layers/constraints/)를 참고해보도록 합시다.<br/>\n",
        "(당연히 지금 모든 것을 알아야 할 필요는 없습니다.)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M1z7T06PrJH"
      },
      "source": [
        "1. **기존 신경망에 가중치 제한이 추가로 적용된 신경망 모델을 구축합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUbvbkgKB7Js"
      },
      "source": [
        "# 가중치 제한(Weight Constraints)를 반영한 예시 코드\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "# 모델 구성을 확인합니다.\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(64, \n",
        "            kernel_regularizer=regularizers.l2(0.01),\n",
        "            activity_regularizer=regularizers.l1(0.01),\n",
        "            kernel_constraint=MaxNorm(2.)),             # 가중치 제한(Weight Constraints)을 추가로 적용\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7um_u-C4QD6z"
      },
      "source": [
        "# 업데이트 방식을 설정합니다.\n",
        "model.compile(optimizer='adam'\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AWt-KifQenU"
      },
      "source": [
        "2. **`.summary()` 함수를 통해서 모델이 잘 구축되었는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heSOuaQBQELV",
        "outputId": "88ab1406-d745-4045-a6e1-87b94fef5f63"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPZ5M9WUQgWD"
      },
      "source": [
        "3. **새로운 모델을 학습하고 결과가 어떻게 달라졌는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5fSlRi6QD_g",
        "outputId": "956d5f10-689e-44ae-af9d-0de1d09b8589"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 5s 2ms/step - loss: 1.0224 - accuracy: 0.7966 - val_loss: 0.7866 - val_accuracy: 0.8137\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f7b570850>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ozJ63PCEqg"
      },
      "source": [
        "### 드롭아웃(Dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC7UL5sLCGsm"
      },
      "source": [
        "Dropout은 가중치의 값을 줄이는 방법(Weight decay, Weight Constraints)과는 다른 방식으로 과적합을 방지합니다.<br/>\n",
        "지정해 준 비율만큼 모델 내에 있는 특정 레이어의 노드 연결을 강제로 끊어주게 됩니다.\n",
        "\n",
        "매 Iteration 마다 랜덤하게 노드를 차단하여 다른 가중치를 학습하도록 조정하기 때문에 과적합을 방지할 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0DlOnyQ7Gi"
      },
      "source": [
        "1. **기존 신경망에 Dropout이 추가로 적용된 신경망 모델을 구축합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxO4ep5CHkJ"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# 모델 구성을 확인합니다.\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(64, \n",
        "            kernel_regularizer=regularizers.l2(0.01),\n",
        "            activity_regularizer=regularizers.l1(0.01),\n",
        "            kernel_constraint=MaxNorm(2.)),             \n",
        "    Dropout(0.5),                           # 드롭아웃(Dropout)을 추가합니다.\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSOkdNtnQt5b"
      },
      "source": [
        "# 업데이트 방식을 설정합니다.\n",
        "model.compile(optimizer='adam'\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5ajUygRAKv"
      },
      "source": [
        "2. **`.summary()` 함수를 통해서 모델이 잘 구축되었는지 확인합니다.**\n",
        "\n",
        "> ❗️ ***Dropout 은 `.summary()` 의 출력을 통해서도 확인할 수 있습니다. 제대로 드롭아웃이 추가되었는지 확인해봅시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvqHqGJ6Qt0-",
        "outputId": "6790a2e1-c027-4e34-abf2-74689a510402"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGSto6u3RBzs"
      },
      "source": [
        "3. **새로운 모델을 학습하고 결과가 어떻게 달라졌는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIVd8rq3QvJL",
        "outputId": "c53a1418-aa95-48b3-e968-17a19a3929a9"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 5s 3ms/step - loss: 1.2128 - accuracy: 0.7680 - val_loss: 0.8783 - val_accuracy: 0.8052\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f7b3adf90>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGjlQfXrCLZ4"
      },
      "source": [
        "## 조금 더 나은 모델학습을 위하여!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOitOTc8CL-r"
      },
      "source": [
        "이번에는 Regularization 이외에도 더 좋은 학습을 위해 적용할 수 있는 **학습률 감소/계획(Learning rate Decay/Scheduling)**에 대해서 배워보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv0IZdXk2K7g"
      },
      "source": [
        "> ❗️ ***아래는 우리가 살펴볼 기법들 외에도 다양한 신경망 학습 스킬이 소개되어 있는 논문입니다.<br/>\n",
        "지금은 거의 다 이해할 수 없는 것이 정상이지만 나중에는 한 번 쯤 읽어보시면 좋겠습니다. (지금 읽지 않아도 됩니다 !!)<br/>\n",
        "[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufP3TDG3AYE"
      },
      "source": [
        "### 학습률 감소(Learning rate decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KltTUr61CRKz"
      },
      "source": [
        "- 학습률 감소(Learning rate decay)는 아래와 같이 옵티마이저(Optimizer)의 다양한 하이퍼파라미터를 조정하여 적용할 수 있습니다.<br/>\n",
        "```\n",
        "tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "    name='Adam'\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTkdccKxTPNE"
      },
      "source": [
        "1. **기존 신경망을 컴파일 하는 코드에 하이퍼파라미터를 조정한 옵티마이저를 적용합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni5MvxklCQME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad80f65-f3d8-45c5-dd1d-1e35cc47dce8"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1 = 0.89)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb5po4llTfxu"
      },
      "source": [
        "2. **`.summary()` 함수를 통해서 모델이 잘 구축되었는지 확인합니다.**\n",
        "\n",
        "> ❗️ ***신경망에 어느 정도 익숙해졌다면 `.summary()` 결과로부터 신경망 파라미터의 개수가 왜 50,240 개가 될 지도 유추해봅시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWNHdSqBTciX",
        "outputId": "3bf3caba-a1be-4ead-e2e6-66bfe49fdc9f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-j_SnGXTgcm"
      },
      "source": [
        "3. **새로운 모델을 학습하고 결과가 어떻게 달라졌는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULS9mCrJTcYe",
        "outputId": "da4b0c62-1161-4f24-b988-d27e7e1272aa"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 6s 3ms/step - loss: 0.9253 - accuracy: 0.7900 - val_loss: 0.8332 - val_accuracy: 0.8120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f2873cb10>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2OumroPCZhK"
      },
      "source": [
        "### 학습률 계획(Learning rate scheduling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9bITrJkCbY6"
      },
      "source": [
        "- 학습률 계획(Learning rate scheduling)은 아래와 같이 학습률에 대한 다양한 파라미터를 지정한 뒤에 옵티마이저(Optimizer)의 학습률에 이를 적용할 수 있습니다.<br/>\n",
        "```\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "```\n",
        "```    \n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnXlBN4iU0Kb"
      },
      "source": [
        "1. **학습률 계획을 지정하고 이를 옵티마이저에 반영합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDhJM9XhCdby"
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgCg2UEyU-GI"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbtLTA9VwYd"
      },
      "source": [
        "2. **`.summary()` 함수를 통해서 모델이 잘 구축되었는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvCG5IPU_4I",
        "outputId": "8ea9bfaa-9c90-4409-c41c-408326e1834b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh87zD6qVyh5"
      },
      "source": [
        "3. **새로운 모델을 학습하고 결과가 어떻게 달라졌는지 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6PM35kSU-EB",
        "outputId": "e0eaa8a2-6b23-4b86-b5b4-f6e5e6a784d8"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1.9780 - accuracy: 0.6663 - val_loss: 1.7156 - val_accuracy: 0.7162\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f23339e10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRq98CKnV1cZ"
      },
      "source": [
        "> ❗️ ***다음과 같이 학습률 계획을 커스터마이징 할 수도 있습니다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urBo1Xa9CfI1"
      },
      "source": [
        "def decayed_learning_rate(step):\n",
        "  step = min(step, decay_steps)\n",
        "  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n",
        "  decayed = (1 - alpha) * cosine_decay + alpha\n",
        "  return initial_learning_rate * decayed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clso3S5zXt38"
      },
      "source": [
        "> ❗️ ***코사인(Cosine) 계획법 역시 많이 사용되는 계획법 중 하나입니다. 이를 적용해보고 결과값을 확인해보도록 합시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LRNtOJxVlt1"
      },
      "source": [
        "first_decay_steps = 1000\n",
        "initial_learning_rate = 0.01\n",
        "lr_decayed_fn = (\n",
        "  tf.keras.experimental.CosineDecayRestarts(\n",
        "      initial_learning_rate,\n",
        "      first_decay_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_gcBoIEVp6o"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_decayed_fn)\n",
        "             , loss='sparse_categorical_crossentropy'\n",
        "             , metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6li9q-VVp37",
        "outputId": "0ee126e0-dd43-4770-bce2-051bcfedd53b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wHOGSRjVssk",
        "outputId": "c93b4516-ac8c-4de4-b37a-e4525ee2940a"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, \n",
        "          validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 5s 2ms/step - loss: 1.6750 - accuracy: 0.6972 - val_loss: 1.2450 - val_accuracy: 0.7753\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6f2331c590>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW8FX1PnChV4"
      },
      "source": [
        "## 활성화 함수(Activation Functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEsFi76f4AKO"
      },
      "source": [
        "### $\\tanh$ (Hyperbolic Tangent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2XVMv8wCiKe"
      },
      "source": [
        "![Tanh Function](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTlFKU704G3v"
      },
      "source": [
        "시그모이드 함수를 변형한 $\\tanh$(Hyperbolic Tangent)도 활성화 함수로 사용됩니다.<br/>\n",
        "0 부근에서 시그모이드 함수보다 더 가파르기 때문에 더 빠르게 최저점에 수렴할 수 있다는 장점을 가지고 있습니다.\n",
        "\n",
        "시그모이드 함수와 $\\tanh$ 함수의 수식을 알아봅시다.\n",
        "\n",
        "<br/>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{sigmoid :} &\\frac{1}{1+e^{-x}} \\\\\n",
        "\\tanh : &\\frac{1-e^{-x}}{1+e^{-x}} = \\frac{2}{1+e^{-2x}} - 1 = 2 \\text{sigmoid}(2x) - 1\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "위 식에서 볼 수 있듯 시그모이드 함수를 적절히 조정하면 $\\tanh$ 함수를 만들 수 있습니다.<br/>\n",
        "시그모이드와 $\\tanh$ 를 함께 그린 그래프를 보면서 두 함수를 비교해봅시다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o441At7l6tNL"
      },
      "source": [
        "<img src=\"https://taewanmerepo.github.io/2017/12/tanh/010.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5fc23xY6459"
      },
      "source": [
        "### Leaky ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3UPIiax68df"
      },
      "source": [
        "Leaky ReLU는 ReLU를 개선한 함수입니다.<br/>\n",
        "ReLU 함수는 0보다 작은 값이 입력될 때에는 어떤 값이 입력 되든지 0의 값을 반환합니다.\n",
        "\n",
        "입력값이 음수인 경우를 살려주기 위해서 음수 부분에도 기울기를 넣어준 ReLU 함수를 Leaky ReLU 라고 합니다.<br/>\n",
        "일반적으로 Leaky ReLU 의 음수 부분은 0.01 의 기울기를 갖습니다.<br/>\n",
        "물론 Parametric ReLU 처럼 기울기를 하이퍼 파라미터로 정해줄 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeBdW2f5CkkA"
      },
      "source": [
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6ZfSt-A3oLG"
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otmrX0wACn5r"
      },
      "source": [
        "소프트맥스(Softmax) 함수는 시그모이드 함수를 다중 분류에 사용할 수 있도록 변형한 활성화 함수입니다.<br/>\n",
        "각 클래스에 대한 가중합 값을 소프트맥스에 통과시키면 모든 클래스의 값의 합이 1이 되는 확률값으로 변환됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlvFT9D33tUf"
      },
      "source": [
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS8m2ZVz5srx"
      },
      "source": [
        "## 🧐  Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq9TEUceCpxn"
      },
      "source": [
        "- 손글씨 MNIST, Fashion MNIST와 같은 예제를 Keras 를 사용하여 스스로의 힘으로 풀 수 있는지 확인해봅시다.<br/>\n",
        "모든 코드를 외워야 할 필요는 없지만 **학습 메커니즘 흐름과 포인트(출력층 노드 수, 출력층 활성화 함수, 손실 함수 설정)**는 알고 있어야 합니다.\n",
        "\n",
        "- 신경망의 과적합을 막기 위해 적용할 수 있는 Regularization 방법에 대한 대략적인 개념을 설명해봅니다.\n",
        "    - 콜백(Callback)을 이용한 학습 **조기 종료(Early Stopping)**\n",
        "    - **가중치 감소(Weight Decay)**\n",
        "    - **가중치 제한(Weight constriants)**\n",
        "    - **드롭아웃(Dropout)**\n",
        "    - **학습률 감소(Learning rate decay)**와 **학습률 계획(Learning rate scheduling)**\n",
        "\n",
        "- 새롭게 배운 활성화 함수와 각 함수의 특징에 대해 여러분의 언어로 설명해봅니다.\n",
        "    - Hyperbolic Tangent (tanh) : 시그모이드(Sigmoid)와의 차이점\n",
        "    - Leaky ReLU : 일반 ReLU 와의 차이점\n",
        "    - **소프트맥스(Softmax) : 언제 사용하며 어떤 특징을 가지고 있는지**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrmSP0JC5zQ5"
      },
      "source": [
        "### 🔝 References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flKvU1VE55C_"
      },
      "source": [
        "- [Overfitting](https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c)\n",
        "\n",
        "- [L2 regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)\n",
        "\n",
        "- [L1 vs L2 한국어 설명](https://light-tree.tistory.com/125)\n",
        "\n",
        "- [OpenCV를 이용한 MNIST 인식 모델 만들어보기](https://www.youtube.com/watch?v=TV3oplqa5VA) + 더보기 (코드 링크 있음)\n",
        "- [Tensorflow를 이용한 CNN 실습영상](https://www.youtube.com/watch?v=pZGvMhhawy8) + 더보기 (코드 링크 있음)"
      ]
    }
  ]
}