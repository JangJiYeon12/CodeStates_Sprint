{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N423a_Word2Vec_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skn9ZDVioced"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / Assignment 3*\n",
        "\n",
        "--- \n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "### 문항 1) Word2Vec에 대해서 틀린 것을 모두 고르시오. \n",
        "1. CBOW, Skip-gram 2가지 모델로 나눌 수 있는데, 이는 서로 반대의 개념이라고 생각할 수 있습니다. \n",
        "2. Skip-gram의 경우는 어떤 단어를 문맥 안의 주변 단어들을 통해서 예측하는 방법이고, \n",
        "3. 반대로 CBOW(Continuous Bag of Words)의 경우에는 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 방법입니다.\n",
        "4. 단어벡터 간의 유사도를 잘 측정한다. 단어 벡터들은 의미적인 거리를 계산할 수 있어서 서로에게 유의미한 관계를 측정할 수 있다. \n",
        "5. 임베딩 차원의 수는 반드시 정해진 룰대로 만들어야 한다. \n",
        "\n",
        "\n",
        "\n",
        "### 문항 2) RNN에 대한 설명으로 틀린 것을 모두 고르시오. \n",
        "\n",
        "1. 시계열의 데이터를 적용하기 위해서 재귀적(Recurrent)한 특성을 만들기 위해서 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 은닉층의 입력으로 재입력한다. \n",
        "2. 재귀적 특성에서 1보다 약간 큰 1.1 혹은 약간 적은 0.9의 값을 갖는 경우도 vanishing gradient가 발생할 수 있다. \n",
        "3. RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 각각 입력 벡터와 출력 벡터, 은닉층에서는 은닉 상태라는 표현을 주로 사용합니다. \n",
        "4. RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용할 수 있습니다.\n",
        "\n",
        "### 문항 3) RNN과 LSTM에 대한 설명으로 틀린 것을 모두 고르시오. \n",
        "\n",
        "1. RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다.\n",
        "2. RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다.\n",
        "3. 시점이 지날수록 정보량이 손실되어가기 때문입니다.\n",
        "4. LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다.\n",
        "5. LSTM과 GRU는 3개의 gate를 이용한다. input gate, forget gate, output gate\n",
        "6. 파라미터의 개수가 줄어들지만, 기존에 LSTM을 사용하면서 최적의 하이퍼파라미터를 찾아낸 상황이라면 굳이 GRU로 바꿔서 사용할 필요는 없습니다.\n",
        "7. 데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있습니다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문입니다.\n",
        "\n",
        "\n",
        "<br>\n",
        "오늘의 추가과제는 아주 간단한 미니 프로젝트처럼 구성되어있다.<br>\n",
        "아래 체크리스트를 점검하면서 과제를 수행해보자. \n",
        "- Keras, tensorflow, pytorch 등 프레임워크에서 제공하는 예제가 아닌 단어/문장 생성기 등 시퀀스 예제 찾기 (RNN, LSTM, GRU, Attention 사용예제)\n",
        "  - 문항 4) 예제 키워드(주제)를 입력하시오. \n",
        "  - 문항 5) 찾는 과정에서 사용한 **검색어**를 모두 입력하시오. \n",
        "  - 문항 6) 사용할 레포지토리를 찾은 검색어를 입력하시오. <br>\n",
        "\n",
        "- 찾아본 코드(레포지토리)를 Colab, Conda, jupyter등 내 환경에서 구현해본다. (참고, github 등에서 readme.md를 잘 따라하면 수행하기가 쉽다. 샘플링크 참조)\n",
        "  - 문항 7) 위 방법을 수행하면서 발생한 오류를 찾아 해결하고 블로그에 해당 내용을 정리하여 게시하고, 블로그 링크를 입력하시오. (오류가 없었다면 공백)\n",
        "<br>\n",
        "\n",
        "- 만약 찾은 코드가 제대로 돌아가지 않는다면, 수정하는 것도 좋지만, 같은 기능을 하는 다른 github를 찾아보며 위과정을 반복한다.<br>\n",
        "\n",
        "- 레포지토리 코드를 이용하여 데모가 잘 구현되었다면, 이제 디테일을 파악해보자\n",
        "  - 문항 8) 어떤 데이터(데이터 설명)를 사용하였는가?\n",
        "  - 문항 9) 전처리과정에 사용한 내용은 무엇인가?\n",
        "  - 문항 10) 어떤 구조(LSTM의 구조 및 node개수 등)의 모델인가? <br>\n",
        "\n",
        "- 이해하기 어려운 코드를 포함하고 있고 직접 설명하기 어렵다면, 인용된 논문을 찾아 주석을 달아두고 이해한 부분만으로 정리한 뒤 Reference에 해당 논문을 남겨두자.\n",
        "\n",
        "c.f. [샘플링크](https://github.com/keithito/tacotron)를 보면 Git에 잘 구성된 페이지를 볼 수 있다. README와 Requirements, 데모코드, notebook 등이 잘 되어있는 레포지토리를 활용할 것. (**단, 이 링크는 샘플일 뿐, 이 링크를 통해서 과제를 해서는 안 된다**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltj1je1fp5rO"
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# 도전과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "\n",
        "- 위의 과제에서 사용한 네트워크에 적용할 다른 데이터를 찾아본다. 예) 셰익스피어 텍스트를 이용한 과제라면, 니체의 글을 적용해보는 등의 다른 데이터를 적용해보기\n",
        "- 해당 모델에서 가장 쉬운 부분을 변경해보기. 예) hidden layer의 node수를 변경, activation function을 바꿔보기, LSTM을 GRU, RNN 등으로 변경해보기 등\n",
        "- readme에서 제공한 자료보다 성능을 올려볼 것\n",
        "\n",
        "## 피해야 할 Resources:\n",
        "- https://www.tensorflow.org/guide/keras/rnn\n",
        "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "- https://victorzhou.com/blog/keras-rnn-tutorial/\n",
        "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "- https://www.kaggle.com/thousandvoices/simple-lstm\n"
      ]
    }
  ]
}