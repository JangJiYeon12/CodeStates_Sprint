{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "ds-cs-N431a_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10OXb-oNKVzO"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 3 / Assignment 1*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model 을 사용해보기\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model을 제작해보기\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "케라스를 이용한 바이너리 이미지 분류 모델에 3가지 CNN 모델을 적용한다.  <br>\n",
        "[데이터 다운로드](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/mountainForest.zip) <br>\n",
        "산의 이미지(./data/mountin/*)와 숲의 이미지(./data/forest/*)를 분류하십시오. 산을 포스티브 클래스(1)로, 숲 이미지를 네거티브(0)로 처리한다.\n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/mountain/art1131.jpg?raw=1)|![](https://github.com/codestates/ds-cs-section4-sprint3/blob/main/N431/data/forest/cdmc317.jpg?raw=1)|\n",
        "\n",
        "표본이 작다는 점을 감안하면 문제는 현실적으로 어렵다. 즉, 클래스당 약 350개의 관측치가 있다. 이 샘플 크기는 직장에서 이미지 분류 문제/솔루션을 프로토타이핑할 때 기대할 수 있는 것일 수 있다. 이럴 때에는 여러가지 모델을 적용해보는 것이 중요하기 때문에 이번 sprint에서는 가능한 여러 모델을 평가하는 데 익숙해지는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI3qR2nVKVzT"
      },
      "source": [
        "# Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Keras에서 제공하는 pre-trained 모델인 ResNet50을 불러오고, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - 이 모델은 50 개의 layer를 가진  CNN기반의 모델입니다. 총 [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt)를 분류하는 모델로 아래와 같이 사용할 수 있습니다. 우리가 풀어야 할 과제는 2가지 이니, 마지막 출력 단을 변경해서 사용하면 됩니다.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "`ResNet50`의 `include_top` 매개 변수는 ResNet 모델에서 전체 연결된 레이어를 제거합니다. 이후에는 ResNet 레이어들의 훈련모드를 끄는 것입니다. 학습 된 매개 변수는 설령 지금 네트워크를 통과할 때 오류가 있더라도 향후 학습 과정에서 업데이트하지 않고 그대로 사용하는 것입니다.\n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Keras 기능 API를 사용하여 모델에 추가로 `Fully-conneted layer`(기본 NN모델)을 추가해야합니다. 우리는 최상위 레이어를 제거하고 모든 이전의 `Fully-conneted layer`는 제거합니다. 출력층의 숫자가 맞지 않기 때문이죠. 네트워크의 특징을 추출하는 부분 만 유지하는 것입니다. `GlobalAveragePooling2D` 레이어는 마지막 컨벌루션 레이어 출력 (2 차원) 각각의 평균을 취함으로써 정말 멋진 평탄화 기능으로 작동합니다.\n",
        "\n",
        "```python\n",
        "x = res.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x) # 출력층 설계\n",
        "model = Model(res.input, predictions)\n",
        "```\n",
        "\n",
        "이 과제는 산의 이미지 (`. / data / mountain / *`)와 숲의 이미지 (`. / data / forest / *`)를 분류하기 위해 위의 전이 학습(transfer learning)을 적용하는 것입니다. 산을 postive 클래스 (1)로, 숲 이미지를 음수 (0)로 취급합니다.\n",
        "\n",
        "Steps to complete assignment: \n",
        "1. 이미지 데이터를 numpy 배열 (`X`)로 불러옵니다.\n",
        "2. 레이블에 대한 `y`를 만듭니다.\n",
        "3. resnet의 사전 훈련 된(pre-trained) 레이어로 모델 훈련\n",
        "4. 모델의 정확성보고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvJd3ItAKVzU"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "![skimage-logo](https://scikit-image.org/_static/img/logo.png)\n",
        "\n",
        "기존 학습된 네트워크(모델)을 사용하려면 기존 입력값을 동일하게 맞춰주어야 합니다.  [`skimage`](https://scikit-image.org/) 의 문서를 확인하시면 보다 쉽게 사용하실 수 있을 것입니다. 다음 함수를 사용해보세요. `skimage.io.imread_collection` 와 `skimage.transform.resize`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4c0M58cKVzW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMzezBMBKVza"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0i14sSCKVzb",
        "outputId": "74a33d44-bf1f-4542-8339-f02985a1b731"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/jonathansokoll/anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtQX3ALdKVzf"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPEt-wwuKVzg",
        "outputId": "1545ee98-10d5-4253-d11e-13effb509682"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 561 samples, validate on 141 samples\n",
            "Epoch 1/5\n",
            "561/561 [==============================] - 39s 69ms/sample - loss: 0.1216 - accuracy: 0.9715 - val_loss: 1.4221 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "561/561 [==============================] - 35s 62ms/sample - loss: 0.0410 - accuracy: 0.9875 - val_loss: 2.9222 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "561/561 [==============================] - 35s 62ms/sample - loss: 0.0646 - accuracy: 0.9697 - val_loss: 0.9962 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "561/561 [==============================] - 39s 70ms/sample - loss: 0.0723 - accuracy: 0.9733 - val_loss: 0.2627 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "561/561 [==============================] - 38s 68ms/sample - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.4859 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f980e9c6f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpEjsFpKVzj"
      },
      "source": [
        "# Custom CNN Model\n",
        "\n",
        "\n",
        "이 단계에서는 Keras를 사용하여 자신 만의 CNN을 작성하고 훈련합니다. 네트워크 시작 부분에 적어도 하나의 Conv 레이어와 pooling 레이어가있는 아키텍처를 사용할 수 있습니다. 원하는 경우 더 추가 해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgwLFkhUKVzj",
        "outputId": "5f9184eb-4a1d-4f2e-a83e-ad6bdf063a71"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 215, 215, 32)      9632      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 43, 43, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 39, 39, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 9, 9, 64)          102464    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5184)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 64)                331840    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 495,265\n",
            "Trainable params: 495,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5NrX3K9KVzm"
      },
      "source": [
        "# Compile Model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yju2nvVNKVzp",
        "outputId": "0cd03a87-2b28-4ade-a721-086627273a94"
      },
      "source": [
        "# Fit Model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 561 samples, validate on 141 samples\n",
            "Epoch 1/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.2667 - accuracy: 0.9073 - val_loss: 0.1186 - val_accuracy: 0.9858\n",
            "Epoch 2/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.2046 - accuracy: 0.9073 - val_loss: 0.3342 - val_accuracy: 0.8511\n",
            "Epoch 3/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1778 - accuracy: 0.9287 - val_loss: 0.2746 - val_accuracy: 0.8723\n",
            "Epoch 4/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1681 - accuracy: 0.9323 - val_loss: 0.8487 - val_accuracy: 0.5957\n",
            "Epoch 5/5\n",
            "561/561 [==============================] - 18s 32ms/sample - loss: 0.1606 - accuracy: 0.9394 - val_loss: 0.3903 - val_accuracy: 0.8582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97388777f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt1dVfOlKVzs"
      },
      "source": [
        "# Image Manipulations(Augmentation)을 이용에 참고할 수 있는 자료\n",
        "\n",
        "이미지 샘플의 증가를 시뮬레이션하기 위해 자르기, 회전, 늘이기 등과 같은 이미지 조작 기술을 적용 할 수 있습니다. 운 좋게도 Keras에는 이러한 기술을 산과 숲 예제에 적용 할 수있는 몇 가지 편리한 기능이 있습니다. 시작하는 데 도움이되는 다음 리소스를 확인하세요.\n",
        "\n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        "3. [Data Augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ARrf4VKVzt"
      },
      "source": [
        "# State Code for Image Manipulation Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "# 도전과제\n",
        "\n",
        "- 코드 향상시키기 : 지난 주 강의 자료를 참조하여 클래스와 내부 method를 이용하여 동작하고, 직접 다운로드 한 이미지를 인식시켜 범주(Class)를 분류하도록 코드개선합니다 (예 : 비행기, 풍선 이미지 다운로드 및 인식되는 지 확인)\n",
        "- 데이터 augmentation을 이용하여 학습 및 분류 진행해보기 (단 Training Set, Test Set의 데이터가 섞이지 않도록 주의 필요)\n",
        "- [기타 사용 가능한 사전 훈련 된 네트워크](https://tfhub.dev)를 확인하고 몇 가지 시도하고 비교해보세요.\n",
        "- 전이 학습을 이용하되, 직접 가져온 이미지를 넣고 새로운 카테고리로 분류해보세요. [Classifier 재학습](https://www.tensorflow.org/hub/tutorials/image_retraining)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI8uH_YBSpqg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}