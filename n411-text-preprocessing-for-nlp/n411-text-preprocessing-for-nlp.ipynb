{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "P41_NLP",
      "language": "python",
      "name": "p41_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "colab": {
      "name": "n411-text-preprocessing-for-nlp.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzqaQLJuikfe"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 1*\n",
        "\n",
        "---\n",
        "\n",
        "# Text Preprocessing for NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPUOhK6Eikft"
      },
      "source": [
        "* 텍스트 토큰화(Tokenization)\n",
        "* 불용어(Stopwords) 제거하기\n",
        "* 표제어 추출(lemmatization)과 어간추출(stemming)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SFgeWPNikft"
      },
      "source": [
        "### Warm up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "8p_lCKXiikfu"
      },
      "source": [
        "자연어처리, NLP(Natural Language Processing)에서 \"Natural Language(자연어)\" 란 사람들이 일상 샐활에 사용하는 언어를 말합니다.\n",
        "\n",
        "자연어를 컴퓨터에게 이해시키는 작업이 NLP 입니다. 자연어는 프로그래밍 언어보다 훨씬 비구조적인 특성을 가집니다. 그래서 컴퓨터는 자연어를 이해하기가 어렵습니다.\n",
        "\n",
        "특히 우리가 서로 대화할 때 섬세한 뜻을 전달하는 뉘앙스라든지, 풍자적인 말투, 아이러니한 상황 등을 컴퓨터가 이해하고 잘 구분하게 할 수 있을까요?\n",
        "\n",
        "우리도 여전히 실수를 많이 하고 있는 상황인데, 컴퓨터에게도 쉽지는 않겠지요?\n",
        "\n",
        "#### 자연어처리 관련 용어\n",
        "- **코퍼스**(Corpus, 말뭉치)란 특정한 목적을 가지고 수집한 텍스트 데이터를 말합니다.\n",
        "- **문서**(Document)란 문장(Sentence)들의 집합입니다\n",
        "- **문장**(Sentence)이란 여러개의 **토큰**(단어, 형태소 등)으로 구성된 문자열 입니다. 마침표, 느낌표 같은 기호로 주로 구분됩니다.\n",
        "- **어휘집합**(Vocabulary)는 코퍼스에 있는 모든 문서, 문장을 토큰화한 후 중복을 제거한 토큰의 집합을 말합니다.\n",
        "    \n",
        "#### 다음 영상들을 시청하세요\n",
        "- [NLP for Developers: Tokenization | Rasa](https://youtu.be/Z_GGVn6LBRI)\n",
        "- [Stemming to Consolidate Vocabulary](https://youtu.be/gBwGPI0srBE)\n",
        "- [Stopwords - Intro to Machine Learning](https://youtu.be/E63RZli2F2o)\n",
        "\n",
        "\n",
        "#### Conda 환경설정\n",
        "\n",
        "Conda는 데이터사이언스 커뮤니티에서 가장 많이 사용하는 환경관리툴 입니다. 이번 유닛 부터는 Conda를 사용하여 파이썬 패키지 간의 의존성을 관리할 것입니다.\n",
        "\n",
        "> __conda__: Package, dependency and environment management for any language—Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN, and more.\n",
        "\n",
        "**가장 빠르고 쉽게 Conda를 사용하는 방법은 지금 여러분이 사용하는 운영체제에 바로 [Anaconda Individual Edition](https://www.anaconda.com/products/individual)을 설치하는 것 입니다.** \n",
        "\n",
        "Conda 인스톨 후 다음 가이드를 읽고 예시를 따라 진행해 보세요\n",
        "\n",
        "- [\"A Guide to Conda Environments\"](https://towardsdatascience.com/a-guide-to-conda-environments-bc6180fc533)\n",
        "- [\"Setting started with conda\"](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html)\n",
        "\n",
        "#### 그럼 이제 NLP 스프린트를 진행하기 위한 환경을 마련해 보겠습니다:\n",
        "\n",
        "1. 각자 운영체제에 있는 커맨드라인 툴을 오픈합니다. (터미널 or Anaconda Prompt)\n",
        "\n",
        "2. `requirements.txt` 파일이 있는 폴더로 이동합니다.\n",
        "\n",
        "3. `conda create -n P41_NLP python==3.7` 명령어를 실행해서 P41_NLP env를 생성합니다. env 명은 여러분이 원하시는 것을 사용해도 괜찮습니다.\n",
        "\n",
        "4. `conda activate P41_NLP` 명령어를 실행해 생성한 환경으로 진입합니다. 진입에 성공하면 `conda install --force-reinstall -y --name P41_NLP -c conda-forge --file requirements.txt`을 실행하여 필요한 패키지를 설치합니다.\n",
        "\n",
        "5. 설치가 잘 완료되면, `python -m ipykernel install --user --name P41_NLP --display-name P41_NLP` 를 실행하여 JupyterLab에서 이 커널을 선택할 수 있도록 합니다.\n",
        "\n",
        "6. 마지막으로 Spacy의 언어 모델을 다운로드 받습니다. 다음 두 명령어를 차례로 실행합니다. `python -m spacy download en_core_web_md` `python -m spacy download en_core_web_lg`\n",
        "\n",
        "7. 이제 모든 설정이 끝났습니다. `conda deactivate P41_NLP` 를 실행하여 환경에서 빠져 나온 후 JupyterLab을 실행하여 여러분이 추가한 P41_NLP 커널이 존재하는지 확인해 보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZcbM2r3ikfu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WAjm6oiqUe"
      },
      "source": [
        "자연어 처리로 첫 걸음을 내딛게 되신 것을 환영합니다. <br>\n",
        "자연어 처리가 다루는 분야는 다양하지만, 본질은 우리 말을 컴퓨터가 알아듣게 하는 것입니다. 그 과정에서 필요한 기술들에 대해서 하나씩 공부해봅시다. \n",
        "\n",
        "이전 과정에서도 어느정도 다뤄보았겠지만, Python을 이용해서 텍스트를 다루는 연습도 하게됩니다. 텍스트를 단어로 분할하거나, 캐릭터로 변환하는 것, 그리고 단어를 단어 고유의 ID로 변환(벡터화)하는 방법들을 공부하게 될 것입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqp5IVC_ikfv"
      },
      "source": [
        "## NLP pipeline\n",
        "\n",
        "한국어와 영어 등 우리가 쓰는 말을 자연어(Natural Language)라고합니다. 그러니 자연어 처리는 문자그대로 해석하면 우리의 말을 어떻게 처리할 것인지를 의미합니다. 우리가 python을 배우고 코딩하고있는 것처럼, 우리의 언어를 인식시키는 과정이라고 생각하면 됩니다. 코딩과 다른 점이라고 하면 규칙이 조금 더 유연하다는 특징이 있습니다. 그렇기 때문에 일반 코딩언어와는 다르게 조금 더 복잡한 과정이 필요하게 됩니다. \n",
        "\n",
        "\n",
        "<img src=\"https://i.imgur.com/Gfdf4Uz.png\" width=\"700\"/>\n",
        "\n",
        "위와 같이 복잡한 과정들을 거쳐야 합니다. \n",
        "\n",
        "그러니 복잡한 언어를 컴퓨터로 넣기 위해서 우리가 살펴보아야 할 것들이 많이 있습니다. 단어의 의미, Thesaurus(유의어), 통계기반 기법, 추론 기반 기법(Word2Vec, 다음 수업에서 학습) 등을 알아야 합니다. 참고로 이 수업방식은 스탠퍼드 대학교의 커리큘럼(CS224d)을 참고하여 제작하였습니다. \n",
        "\n",
        "유의어 (Thesaurus, 시소러스)\n",
        "  사람이 직접 의미를 입력해주어도 좋습니다. 사전이 대표적인 예시라고 할 수 있습니다. 사전에 하단을 보면, 보통 유의어, 동의어 등이 적혀있는 것을 보실 수 있습니다. 한편 상위개념과 하위개념도 있습니다. vehicle - car, train, bus 등이 알기쉬운 예시입니다. car의 추가 하위는 sedan, SUV, batch-back 등이 있을 수 있습니다. \n",
        "  이런 유의어 정보를 이용해서 단어들의 네트워크를 이용하면 단어사이의 유사도를 구할 수 있고, 이 외에도 우리에게 유용한 일들을 컴퓨터가 수행할 수 있게 됩니다. \n",
        "\n",
        "- WordNet : 자연어 분야의 가장 유명한 시소러스 (프린스턴 대학)\n",
        "\n",
        "시소러스의 문제점, \n",
        "- 1) 사람이 정의한 것이기 때문에 시대에 따라서 변하는 특징이 있습니다. 우리가 배우고 있는 딥러닝도 20년 전에는 사용하지 않던 신조어입니다. 또 의미가 변하기도 합니다. \n",
        "- 2) 단어의 미묘한 차이를 표현하기가 어렵다. - 빈티지/레트로, 노랗다/누렇다 ?\n",
        "- 3) 인건비 - 누군가는 정의를 해줘야 하기 때문입니다. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siHx14cHpsAV"
      },
      "source": [
        "### 통계 기반 기법, 말뭉치(corpus) 이용\n",
        "  말뭉치라는 것은 단한히 말하면 대량의 텍스트 데이터입니다. 최근 데이터 경진대회들을 보면 트위터 글, 아마존 리뷰 등의 말뭉치 데이터들을 볼 수 있는데 이것이 대표적인 사례입니다. 이 데이터에는 사람들이 글을 어떻게 사용하는지가 담겨있기 때문에 단순히 데이터이지만, 그 활용성을 담고 있습니다. 이런 뭉치에서부터 통계적으로 유의미한 데이터를 추출하는데 목적이 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VRiB2ALlpweg",
        "outputId": "bad4ca13-7efa-4c37-e54a-126e1b93c00a"
      },
      "source": [
        "#말뭉치 전처리\n",
        "text = \"Hello world! nice to meet you! python is nice language!\"\n",
        "text = text.lower()\n",
        "text = text.replace('!', ' .')\n",
        "text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello world . nice to meet you . python is nice language .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vKlUwKeqO_p",
        "outputId": "9f157536-bf1f-465e-b1bc-8eeb6ec2af92"
      },
      "source": [
        "# split words\n",
        "word = text.split(' ')\n",
        "word"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'world',\n",
              " '.',\n",
              " 'nice',\n",
              " 'to',\n",
              " 'meet',\n",
              " 'you',\n",
              " '.',\n",
              " 'python',\n",
              " 'is',\n",
              " 'nice',\n",
              " 'language',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3A_tY5JqiTC"
      },
      "source": [
        "이렇게 단순한 기능부터 차근차근히 배워보겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "8DEaKnIPikfv"
      },
      "source": [
        "### 텍스트 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "YThr_noIikfv"
      },
      "source": [
        "> **token**: an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing\n",
        "\n",
        "> 토큰: 가장 낮은 단위로 어휘 항목들을 구분할 수 있는 분류 요소. 의미를 가지는 최소한의 문자 덩어리\n",
        "\n",
        "토큰은 보통 자연어 처리에서 최소단위로 사용되는데 단어, 형태소 등이 될 수 있습니다. 앞으로 우리는 문장이나 문서를 토큰화해 사용할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6HnOYA7ikfw"
      },
      "source": [
        "### 올바른 토큰화\n",
        "\n",
        "* 반복가능한(iterable) 데이터 구조(list, generator 등)에 저장 되어야 합니다.\n",
        "    - 의미단위(semantic unit)를 분석하기 위해서 입니다.\n",
        "    - ```str.split(' ')```\n",
        "* 가능하면 문자를 모두 대문자 or 소문자로 통일 합니다.\n",
        "    - 복잡도를 줄이고 읽기 쉽게 합니다.\n",
        "    - ```str.lower()```\n",
        "* 가능하면 문장 부호, 공백 등 영문자, 숫자가 아닌 문자들을 제거해야 합니다.\n",
        "    - 분석에 관련 없는 정보를 제거합니다.\n",
        "    - ```re.sub(r'[^a-zA-Z ^0-9]', '')```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUdQHtvTikfw"
      },
      "source": [
        "무작위로 영문시퀀스를 만들어 토큰화 연습을 해 보도록 하겠습니다. **이 문장에서 가장 많은 빈도로 나타나는 문자는 무엇일까요?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0EzcsZmikfw"
      },
      "source": [
        "random_seq = \"ABJIOASJFIOJADFIJOQIJPOOSIUDFUIOHQOIJAAAJIJAPSIDJFAIAAIJAPOAJFB\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZePLxccikfx"
      },
      "source": [
        "이 문자열을 보았을때 가장 유용한 토큰은 문자(character) 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByxymNHmikfx",
        "outputId": "331afbf2-2e34-4ebb-cee6-e8a25e64c8e1"
      },
      "source": [
        "tokens = list(random_seq)\n",
        "print(tokens)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'B', 'J', 'I', 'O', 'A', 'S', 'J', 'F', 'I', 'O', 'J', 'A', 'D', 'F', 'I', 'J', 'O', 'Q', 'I', 'J', 'P', 'O', 'O', 'S', 'I', 'U', 'D', 'F', 'U', 'I', 'O', 'H', 'Q', 'O', 'I', 'J', 'A', 'A', 'A', 'J', 'I', 'J', 'A', 'P', 'S', 'I', 'D', 'J', 'F', 'A', 'I', 'A', 'A', 'I', 'J', 'A', 'P', 'O', 'A', 'J', 'F', 'B']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhzdN8uxikfx"
      },
      "source": [
        "- list 함수로 토큰을 리스트(반복가능한 데이터 구조)에 저장을 하였고,\n",
        "- 모두 대문자로 통일 되었으며\n",
        "- 알파벳 외에 구두점 등 다른 문자가 없습니다.\n",
        "\n",
        "토큰화가 잘 된 것으로 보이니 바로 분석을 시도해 볼 수 있겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vgAS2ZYAikfy",
        "outputId": "467c4d12-d0f7-4dd5-ad4e-815196629360"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x=tokens);"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIF6lupKikfy"
      },
      "source": [
        "가장 많은 빈도를 보이는 문자는 'A' 입니다. 우리는 이같이 복잡하고 섞여있는 문자들을 눈으로 직접 나누는 작업에 매우매우 취약합니다.\n",
        "그래서 이 복잡한 데이터를 이해 쉽고 카운트를 하는 등 분석이 가능한 토큰으로 바꾸었습니다.\n",
        "\n",
        "이러한 작업이 오늘 여러분과 다룰 핵심 내용입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck3Qrd98rHMk"
      },
      "source": [
        "### 단어의 분산 표현\n",
        "  다양한 색상으로 디스플레이가 되었습니다. 잠깐 색상에 대해서 얘기를 해볼까요? 섹션1에서 디스플레이 색상에 대해서 많이 배웠겠지만, 간단히 언급하면 색상은 정말 다양합니다. 파란색(blue)뿐만 아니라 하늘색(ski-blue)도 있죠. RGB라는 빛의 삼원색으로 구성하여 수많은 색상을 만들어낼 수 있습니다. 파란색, 하늘색이라는 이름대신에 3차원의 벡터형태로 색상을 표현할 수 있습니다. 또 색상을 정확하게 전달할 수 있는 능력도 가지고 있습니다. 디자이너와 혹시 대화할 기회가 생기면, 색상코드를 주고받을 일이 많이 생길텐데, 그만큼 명확한 특징도 없습니다. \n",
        "  이렇게 색처럼 단어도 이런 벡터로 표현할 수 있을까요? 벡터를 통해서 색상이 정해지듯이 단어도 정해질 수 있다는 것입니다. 그러면서도 그 의미가 정확하게 전달되어야겠죠. 이를 자연어 처리 분야에서는 단어의 분산표현(distributional representation)이라고 말합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhtGVUG-sf6b"
      },
      "source": [
        "### 분포 가설\n",
        "  자연어 처리의 역사에서 벡터화 연구는 끊임없이 있었습니다. 간단한 아이디어에서 출발한 연구가 많았는데, 중요한 아이디어는 바로 단어의 물리적인 거리입니다. 단어의 의미는 주변 단어들에 의해서 형성된다는 것이죠. 이를 분포가설(Distributional hypothesis)이라고 합니다. \n",
        "  단어 자체의 의미보다는 그 단어가 사용된 맥락(context)가 중요하다는 것이죠. 의미가 같은 단어들은 같은 맥락에서 더 많이 등장하게 되는 것도 우리가 잘 알고 있습니다. \n",
        "  이후의 모델을 배울 때에도 이 맥락, Context에 대해서는 많이 다뤄질 예정입니다. \n",
        "\n",
        "#### 동시발생 행렬\n",
        "  분포 가설을 기초로하여 단어를 벡터화하는 것을 생각해봅시다. 주변 단어를 '세어보는' 방법이 자연스럽게 떠오릅니다. 단어 하나를 선택하면, 그 단어 주변의 단어들이 어떻게 구성되어있는 지 알아보는 방법인 것이죠. 이를 '통계 기반(statistical based) 방법이라고 합니다. \n",
        "\n",
        "  <img src=\"https://i.stack.imgur.com/y3oku.png\">\n",
        "\n",
        "베스트 커플을 뽑아야 하는 설문조사 결과를 자동으로 분석해야 한다고 하면, 모든 조사 문항헤서 같이 언급된 이름의 빈도를 세어서 정리할 수 있다. 이런 그림이 동시발생행렬(co-occurrence matrix)라고 합니다. \n",
        "\n",
        "  이 외에도 문장 내에서 단어의 개수(TF), 여러 문장에서 사용된 단어와 그렇지 않는 단어(IDF)들 이런 특징을 이용해서 정보를 추출할 수도 있습니다. 이 내용은 다음 수업시간에 조금 더 자세히 배워보도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5FmzeSHuJT1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "XiJdCK97ikfy"
      },
      "source": [
        "## 파이썬 기본 함수로 토큰화(Tokenizing)를 수행해 봅시다.\n",
        "\n",
        "이번에는 단어들로 구성된 문장을 넣어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lff_pD7ikfy"
      },
      "source": [
        "# 단어가 토큰입니다\n",
        "words = \"Constructor, Leadership, refer, Yeah, way to go, buddy!;\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzjgsJklikfy"
      },
      "source": [
        "##### 토큰을 Iterable 데이터 구조에 저장합니다.\n",
        "\n",
        "파이썬에서 스트링 객체는 iterable 입니다. 하지만 지금 sample 은 단어 토큰이 아닌 문자가 반복되는 구조입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MizTKUcLikfz",
        "outputId": "d1f0658a-9108-462f-f553-fed02b9abc16"
      },
      "source": [
        "type(words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpNvd8kBikfz"
      },
      "source": [
        "python `enumerate` 함수는 String, List 등 자료형을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 리턴합니다.\n",
        "for문에서 많이 쓰입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU4vwDd9ikfz",
        "outputId": "09fb0cfb-3588-4c81-836a-95419fec1781"
      },
      "source": [
        "enumerate(words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<enumerate at 0x7fe18c772f30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHWHOzerikfz",
        "outputId": "168af292-b7ec-40a0-c64b-7ca101074bc3"
      },
      "source": [
        "for count, ele in enumerate(words[:5]): \n",
        "    print(count, ele)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 C\n",
            "1 o\n",
            "2 n\n",
            "3 s\n",
            "4 t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-rOlrIeikfz",
        "outputId": "026de2fe-b9f3-452b-92f9-3c63a65871fd"
      },
      "source": [
        "list(enumerate(words))[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'C'),\n",
              " (1, 'o'),\n",
              " (2, 'n'),\n",
              " (3, 's'),\n",
              " (4, 't'),\n",
              " (5, 'r'),\n",
              " (6, 'u'),\n",
              " (7, 'c'),\n",
              " (8, 't'),\n",
              " (9, 'o')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHqgJ0ptikf0"
      },
      "source": [
        "words 에서 의미 단위를 단어라고 했을 때, `.split()`를 사용해 공백으로 문장을 나누면 단어가 iterable 단위가 되어 리스트에 저장할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsW65iGeikf0",
        "outputId": "9262ee50-c59f-4f67-9087-83369438c278"
      },
      "source": [
        "words.split(\" \")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Constructor,',\n",
              " 'Leadership,',\n",
              " 'refer,',\n",
              " 'Yeah,',\n",
              " 'way',\n",
              " 'to',\n",
              " 'go,',\n",
              " 'buddy!;']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzHa5cIsikf0",
        "outputId": "c8a1da6b-ec12-4727-ac9c-e529a71d88dc"
      },
      "source": [
        "words.split(',')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Constructor', ' Leadership', ' refer', ' Yeah', ' way to go', ' buddy!;']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcsUO2OAikf0"
      },
      "source": [
        "#### 대소문자를 정제 합니다\n",
        "\n",
        "가장 일반적인 토큰정제과정은 대소문자를 일치시키는 것입니다. 대소문자를 일치시켜 의미적으로 같은 여러 토큰을 한 토큰으로 만들 수 있습니다.\n",
        "\n",
        "`.lower()`/`.upper()` 문자열 메소드를 사용해 소/대문자로 정제합니다.\n",
        "\n",
        "예제에 사용할 데이터를 불러오겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7F8oB1Tikf0"
      },
      "source": [
        "# sampling from: 'https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products'\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/amazon/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19_sample.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXMyTUN-ikf1"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoQz4724ikf1"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWXVR4Etikf1"
      },
      "source": [
        "아마존 브랜드가 보이네요, 이상한 점을 찾아 보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LjA2e3Iikf1"
      },
      "source": [
        "df['brand'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsfdNDdbikf1"
      },
      "source": [
        "대소문자를 일치시켜 보겠습니다. 카테고리가 줄겠지요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLeDuXBikf2"
      },
      "source": [
        "df['brand'] = df['brand'].apply(lambda x: x.lower())\n",
        "df['brand'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcp5-xUCikf2"
      },
      "source": [
        "##### 알파벳/숫자만 남겨 봅시다.\n",
        "\n",
        "이번에는 알파벳과 숫자만 남기기 원합니다. 문장 부호나 공백문자, 다른 기호들은 대부분 노이즈라고 볼 수 있습니다 (항상 그런 것은 아닙니다). 이번에는 `re` (regular expressions, 정규식) 패키지를 사용해서 정제를 해 보겠습니다.\n",
        "\n",
        "사용할 정규식 표현은 `'[^a-zA-Z0-9 ]'` 입니다. 이 표현은 소문자(a-z), 대문자(A-Z), 숫자(0-9), 공백문자(space)를 제외한 모든 문자를 제거합니다.\n",
        "\n",
        "- [참고: Python RegEx](https://www.w3schools.com/python/python_regex.asp#sub)\n",
        "- [정규 표현식 시작하기](https://wikidocs.net/4308#_2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKBJfdinikf2"
      },
      "source": [
        "# 정규식 라이브러리\n",
        "import re\n",
        "\n",
        "# 정규식\n",
        "# []: [] 사이 문자를 매치, ^: not\n",
        "regex = r\"[^a-zA-Z0-9 ]\"\n",
        "\n",
        "# 정규식을 적용할 스트링\n",
        "test_str = (\"(Natural Language Processing) is easy!, DS!\\n\")\n",
        "\n",
        "# 치환할 문자\n",
        "subst = \"\"\n",
        "\n",
        "result = re.sub(regex, subst, test_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agAQ94mJikf2"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsmFFy6Zikf2"
      },
      "source": [
        "#### 정규식 정의에서 치환까지 수행하는 tokenize 함수를 만들어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BhNbBhcikf3"
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"text 문자열을 의미있는 단어 단위로 list에 저장합니다.\n",
        "    Args:\n",
        "        text (str): 토큰화 할 문자열\n",
        "    Returns:\n",
        "        list: 토큰이 저장된 리스트\n",
        "    \"\"\"\n",
        "    # 정규식 적용\n",
        "    tokens = re.sub(regex, subst, text)\n",
        "\n",
        "    # 소문자로 치환\n",
        "    tokens = tokens.lower().split()\n",
        "    \n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BNgt4NPikf3"
      },
      "source": [
        "tokenize(test_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "u3JDHMnlikf3"
      },
      "source": [
        "### 실제 데이터를 가지고 진행해 봅시다.\n",
        "\n",
        "이번에는 아마존 제품에 리뷰 데이터 다루어 보겠습니다.\n",
        "Alexa, Echo와 같은 아마존 제품에 대한 리뷰를 토큰화 하고 분석해보겠습니다!\n",
        "- [Kaggle](https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdcf8DCZikf3"
      },
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# squarify treemap\n",
        "import squarify\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# # spacy: NLP library\n",
        "# import spacy\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# # Load general-purpose pretrained models to predict named entities, part-of-speech tags and syntactic dependencies\n",
        "# ## https://spacy.io/models\n",
        "# ## python -m spacy download en_core_web_lg\n",
        "# nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eebHa2pvikf3"
      },
      "source": [
        "df.head(2).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQyR9JDyikf4"
      },
      "source": [
        "reviews.text 문장의 단어를 카운트 해서 사용해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL245zXMikf4"
      },
      "source": [
        "df['reviews.text'].value_counts(normalize=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgb_HHVWikf4"
      },
      "source": [
        "각 리뷰텍스트를 토크나이즈 하여 tokens 칼럼으로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlATIP2Jikf4"
      },
      "source": [
        "df['tokens'] = df['reviews.text'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmYZSEevikf4"
      },
      "source": [
        "df['tokens'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvaYRbH-ikf4"
      },
      "source": [
        "df[['reviews.text', 'tokens']][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc8URY1Yikf5"
      },
      "source": [
        "어떤 카테고리가 있는지 살펴보겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR3sbfOIikf5"
      },
      "source": [
        "df['primaryCategories'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QRFX8Sikf5"
      },
      "source": [
        "Electronics 카테고리에 해당하는 문장들만 제한해서 분석해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j55Zy7kTikf5"
      },
      "source": [
        "\n",
        "df = df[df['primaryCategories'] == 'Electronics']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzKDkNv5ikf5"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18UZWIeKikf5"
      },
      "source": [
        "#### 토큰을 분석해 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK1kVdPyikf6"
      },
      "source": [
        "Counter를 사용하여 모든 리뷰 토큰을 카운트 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C5uNrl8ikf6"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Counter 객체는 리스트요소의 값과 요소의 갯수를 카운트 하여 저장하고 있습니다.\n",
        "# 카운터 객체는 .update 메소드로 계속 업데이트 가능합니다.\n",
        "word_counts = Counter()\n",
        "\n",
        "# 토큰화된 각 리뷰 리스트를 카운터 객체에 업데이트 합니다. \n",
        "df['tokens'].apply(lambda x: word_counts.update(x))\n",
        "\n",
        "# 가장 많이 존재하는 단어 순으로 10개를 나열합니다\n",
        "word_counts.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQrRGkrWikf6"
      },
      "source": [
        "the, and, a, is... 같은 어떤 문서에든지 많이 나타나는 단어들은 사실 큰 의미를 찾기 어려운 단어들 입니다.\n",
        "\n",
        "이런 단어들을 **stopword(불용어)** 라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_404LZ0ikf6"
      },
      "source": [
        "이와같은 프로세스를 사용해서 코퍼스의 전체 워드 카운트, 랭크 등 정보가 담긴 데이터프레임을 리턴하는 함수를 만들어 봅시다. 이 함수는 토큰화된 문서들이 들어있는 코퍼스를 입력으로 받습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NQNBwVTikf6"
      },
      "source": [
        "\n",
        "def word_count(docs):\n",
        "    \"\"\" 토큰화된 문서들을 입력받아 토큰을 카운트 하고 관련된 속성을 가진 데이터프레임을 리턴합니다.\n",
        "    Args:\n",
        "        docs (series or list): 토큰화된 문서가 들어있는 list\n",
        "    Returns:\n",
        "        list: Dataframe\n",
        "    \"\"\"\n",
        "    # 전체 코퍼스에서 단어 빈도 카운트\n",
        "    word_counts = Counter()\n",
        "\n",
        "    # 단어가 존재하는 문서의 빈도 카운트, 단어가 한 번 이상 존재하면 +1\n",
        "    word_in_docs = Counter()\n",
        "\n",
        "    # 전체 문서의 갯수\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    for doc in docs:\n",
        "        word_counts.update(doc)\n",
        "        word_in_docs.update(set(doc))\n",
        "\n",
        "    temp = zip(word_counts.keys(), word_counts.values())\n",
        "\n",
        "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "    # 단어의 순위\n",
        "    # method='first': 같은 값의 경우 먼저나온 요소를 우선\n",
        "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "    total = wc['count'].sum()\n",
        "\n",
        "    # 코퍼스 내 단어의 비율\n",
        "    wc['percent'] = wc['count'].apply(lambda x: x / total)\n",
        "\n",
        "    wc = wc.sort_values(by='rank')\n",
        "\n",
        "    # 누적 비율\n",
        "    # cumsum() : cumulative sum\n",
        "    wc['cul_percent'] = wc['percent'].cumsum()\n",
        "\n",
        "    temp2 = zip(word_in_docs.keys(), word_in_docs.values())\n",
        "    ac = pd.DataFrame(temp2, columns=['word', 'word_in_docs'])\n",
        "    wc = ac.merge(wc, on='word')\n",
        "    \n",
        "    # 전체 문서 중 존재하는 비율\n",
        "    wc['word_in_docs_percent'] = wc['word_in_docs'].apply(lambda x: x / total_docs)\n",
        "\n",
        "    return wc.sort_values(by='rank')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3-2DTRikf7"
      },
      "source": [
        "wc = word_count(df['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOM20Zmaikf8"
      },
      "source": [
        "wc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvVUaaJFikf8"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# 누적분포그래프(CDF)\n",
        "sns.lineplot(x='rank', y='cul_percent', data=wc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tNVq6Ihikf8"
      },
      "source": [
        "wc[wc['rank'] <= 1000]['cul_percent'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX1wxsp-ikf8"
      },
      "source": [
        "import squarify\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "color=['viridis']\n",
        "\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiT6BULfikf9"
      },
      "source": [
        "### NLP 라이브러리(Spacy)를 사용해 텍스트 데이터를 처리해 보겠습니다.\n",
        "\n",
        "\n",
        "Spacy(아직 한글 사전 학습 모델이 지원 안됩니다)는 문서 구성요소를 다양한 구조에 나누어 저장하는 대신, 요소를 색인화하고 검색정보를 간단히 저장합니다. 그래서 실제 배포 단계에서 NLTK 같은 라이브러리보다 Spacy가 유리할 수 있습니다.\n",
        "\n",
        "- 참고: Spacy를 공부하기 위해 [Spacy course]('https://course.spacy.io/en/')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zq-sXyXikf9"
      },
      "source": [
        "# spacy.io\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "# Load general-purpose pretrained models to predict named entities, part-of-speech tags and syntactic dependencies\n",
        "## https://spacy.io/models\n",
        "## python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Tokenizer 생성\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1kr-6-Xikf9"
      },
      "source": [
        "Spacy 모델에 문장을 넣어 보겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VBnez7Mikf9"
      },
      "source": [
        "doc = nlp('hello data science world!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_kGREvWikf9"
      },
      "source": [
        "type(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBBf1-FDikf-"
      },
      "source": [
        "# https://spacy.io/usage/linguistic-features\n",
        "# Text: The original word text.\n",
        "# Lemma: The base form of the word.\n",
        "# POS: The simple UPOS part-of-speech tag.\n",
        "# Tag: The detailed part-of-speech tag.\n",
        "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "# Shape: The word shape – capitalization, punctuation, digits.\n",
        "# is alpha: Is the token an alpha character?\n",
        "# is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7F3qLlOikf-"
      },
      "source": [
        "spacy로 토큰화를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9USzjqFuikf-"
      },
      "source": [
        "# 토큰을 반환합니다\n",
        "sample = \"Constructor, Leadership, refer, Yeah, way to go, buddy!;\"\n",
        "[token.text for token in tokenizer(sample)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gaYgygHikf-"
      },
      "source": [
        "tokenizer.pipe를 사용해서 토큰 스트림을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaI9fyRJikf-"
      },
      "source": [
        "# Tokenizer Pipe\n",
        "\n",
        "tokens = []\n",
        "\n",
        "# 토큰화 (in stream process)\n",
        "# .pipe streams input, and produces streaming output\n",
        "for doc in tokenizer.pipe(df['reviews.text']):\n",
        "    doc_tokens = [token.text for token in doc]\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['tokens'] = tokens\n",
        "df['tokens'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKRcxdLdikf-"
      },
      "source": [
        "wc = word_count(df['tokens'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjaDpkWlikf_"
      },
      "source": [
        "wc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XJyaUOikf_"
      },
      "source": [
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "VoZDYhRQikf_"
      },
      "source": [
        "## 불용어 (Stop Words) 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSKHE3u8ikf_"
      },
      "source": [
        "다음과 같은 내용을 배워보겠습니다: \n",
        "- 불용어는 무엇일까요?\n",
        "- Spacy를 사용해 불용어를 처리하는 방법?\n",
        "- 불용어 시각화\n",
        "- 불용어 사전\n",
        "- 불용어 확장\n",
        "- 통계적 트리밍(trimming)\n",
        "\n",
        "\n",
        "위에서 시각화한 단어 대부분은 제품 리뷰를 이해하는데 그다지 도움이 되지 않습니다. 'I', 'and', 'of' 같은 단어들은 리뷰 관점에서 아무런 의미가 없습니다. 이런 단어들을 우리는 **'Stop words(불용어)'** 라고 합니다. 왜냐하면 저 단어들을 분석할 때 넣지 않아야(stop) 하기 때문입니다.\n",
        "\n",
        "대부분의 NLP 라이브러리는 **접속사, 관사, 부사, 대명사, 일반동사** 등을 포함한 일반적인 **불용어 사전**을 내장하고 있습니다. 가장 좋은 방법은 우리가 다루는 도메인에서 많이 나오는 불필요한 단어를 불용어 사전에 추가하여 **불용어 확장**을 하는 것 입니다. 예를 들어 지금 사용하는 아마존 데이터에서는 'Amazon' 이라는 단어가 상당히 많이 나올 텐데, 이처럼 우리가 분석할 때 큰 의미가 없어 보이는 단어들을 불용어로 확장해 사용하는 것 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "rf9dvclqikf_"
      },
      "source": [
        "### 기본 불용어\n",
        "\n",
        "Spacy 모델에서 가져올 수 있는 기본 불용어를 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8q5a3aHikgA"
      },
      "source": [
        "print(nlp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt-JB_7bikgA"
      },
      "source": [
        "tokens = []\n",
        "# 토큰에서 불용어 제거, 소문자화 하여 업데이트\n",
        "for doc in tokenizer.pipe(df['reviews.text']):\n",
        "    doc_tokens = []\n",
        "\n",
        "    # A doc is a sequence of Token(<class 'spacy.tokens.doc.Doc'>)\n",
        "    for token in doc:\n",
        "        # 토큰이 불용어와 구두점이 아니면 저장\n",
        "        if (token.is_stop == False) & (token.is_punct == False):\n",
        "            doc_tokens.append(token.text.lower())\n",
        "\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['tokens'] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heOpH7nOikgA"
      },
      "source": [
        "df.tokens.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky2RfDqgikgA"
      },
      "source": [
        "불용어들이 모두 제거가 되어 완전히 다른 단어들이 상위에서 보입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1q-lr9CikgB"
      },
      "source": [
        "wc = word_count(df['tokens'])\n",
        "\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVKrKQWbikgB"
      },
      "source": [
        "### 불용어 확장\n",
        "\n",
        "중요하지 않지만 상위에 나오는 몇몇 단어들을 불용어에 추가해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyguAFtAikgB"
      },
      "source": [
        "print(type(nlp.Defaults.stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dTke6T1ikgB"
      },
      "source": [
        "STOP_WORDS = nlp.Defaults.stop_words.union(['batteries','I', 'amazon', 'i', 'Amazon', 'it', \"it's\", 'it.', 'the', 'this'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPxeF-CIikgB"
      },
      "source": [
        "print(STOP_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUNQBZpMikgB"
      },
      "source": [
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['reviews.text']):\n",
        "    \n",
        "    doc_tokens = []\n",
        "    \n",
        "    for token in doc: \n",
        "        if token.text.lower() not in STOP_WORDS:\n",
        "            doc_tokens.append(token.text.lower())\n",
        "   \n",
        "    tokens.append(doc_tokens)\n",
        "    \n",
        "df['tokens'] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21z9GcNlikgC"
      },
      "source": [
        "wc = word_count(df['tokens'])\n",
        "wc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buABonj8ikgC"
      },
      "source": [
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTN7ZUZKikgC"
      },
      "source": [
        "추가적으로 킨들의 리뷰와 전체 리뷰의 토큰을 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM8Co7iWikgD"
      },
      "source": [
        "df['kindle'] = df['name'].str.contains('kindle', case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3ehrnybikgE"
      },
      "source": [
        "wc_kindle = word_count(df[df['kindle'] == 1]['tokens'])\n",
        "wc.shape, wc_kindle.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvmpZkwikgE"
      },
      "source": [
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "wc_kindle_top20 = wc_kindle[wc_kindle['rank'] <= 20]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "axes[0].set_title('All Reviews')\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6, ax=axes[0], text_kwargs={'fontsize':14})\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].set_title('Kindle Reviews')\n",
        "squarify.plot(sizes=wc_kindle_top20['percent'], label=wc_kindle_top20['word'], alpha=0.6, ax=axes[1], text_kwargs={'fontsize':14})\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fy07JNikgE"
      },
      "source": [
        "### 통계적 트리밍(Trimming)\n",
        "\n",
        "기존에 알려진 불용어를 제거하는 대신 코퍼스에서 통계적인 방법을 통해 단어를 제거하는 방법이 있습니다.\n",
        "\n",
        "단어들의 누적분포 그래프를 다시 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX1ocoDRikgE"
      },
      "source": [
        "sns.lineplot(x='rank', y='cul_percent', data=wc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7O5-5eeikgE"
      },
      "source": [
        "이 그래프에서 알 수 있는 것은 몇몇 소수의 단어들이 전체 코퍼스의 80%를 차지한다는 것입니다. 이런 현상은 다음과 같이 두 가지로 해석할 수 있습니다.\n",
        "\n",
        "1. 자주 나타나는 단어들(그래프의 왼쪽)은 여러 문서에 두루 나타나기 때문에 문서의 의미를 이해하는데 통찰력을 제공하지 않습니다.\n",
        "2. 자주 나타나지 않는 단어들(그래프의 오른쪽)도 역시 너무 드믈게 나타나기 때문에 큰 의미가 없을 확률이 높습니다.\n",
        "\n",
        "랭크가 높거나 낮은 단어들을 한 번 살펴 보고 제거해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKk6FpBzikgE"
      },
      "source": [
        "wc.tail(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq05xHlqikgF"
      },
      "source": [
        "wc['word_in_docs_percent'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVtOMSn3ikgF"
      },
      "source": [
        "wc['word_in_docs_percent']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gajnG4nZikgF"
      },
      "source": [
        "# 문서에 나타나는 빈도\n",
        "sns.displot(wc['word_in_docs_percent'],kind='kde');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF9dJuh4ikgF"
      },
      "source": [
        "# 최소한 1% 이상 문서에 나타나는 단어들만 선택합니다.\n",
        "\n",
        "wc = wc[wc['word_in_docs_percent'] >= 0.01]\n",
        "\n",
        "sns.displot(wc['word_in_docs_percent'], kind='kde');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAZ8kI3BikgF"
      },
      "source": [
        "wc.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bUbAioikgG"
      },
      "source": [
        "wc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "tP3M0BQxikgG"
      },
      "source": [
        "## 표제어추출(lemmatization)과 어간추출(stemming) 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "bzy_EJGPikgG"
      },
      "source": [
        "토큰화된 단어들을 보면, 조금 더 수정이 필요한 부분이 보입니다. 예를 들어 'batteries' 와 'battery'를 보면 이 둘은 어근(root)이 같은 단어입니다.\n",
        "이런 단어들을 표제어추출(lemmatization) 이나 어간추출(stemming)을 통해 추출 하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "id": "og-a_0NyikgG"
      },
      "source": [
        "### Stemming(어간추출)\n",
        "\n",
        "- [PorterStemmer](https://tartarus.org/martin/PorterStemmer/)\n",
        "\n",
        "> 어간(stem): 단어의 의미가 포함된 부분으로 접사등이 제거된 형태 입니다. 어근이나 단어의 원형과 같지 않을 수 있습니다.\n",
        "> 예를 들어 argue, argued, arguing, argus의 어간은 단어들의 뒷 부분이 제거된 argu가 어간입니다.\n",
        "\n",
        "다음과 같은 부분을 제거합니다:\n",
        "- 'ing'\n",
        "- 'ed'\n",
        "- 's'\n",
        "\n",
        "Stemming 방법은 Porter, Snowball, Dawson 등 알고리즘으로 잘 정립되어 있습니다. 이 알고리즘들에 대한 더 자세한 정보는 다음을 참고하세요. \n",
        "- [A Comparative Study of Stemming Algorithms](https://pdfs.semanticscholar.org/1c0c/0fa35d4ff8a2f925eb955e48d655494bd167.pdf) \n",
        "\n",
        "\n",
        "Spacy는 Stemming을 제공하지 않고 **lemmatization**만 제공하기 때문에, `nltk` 를 사용해 stemming을 진행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wKSxXdWikgG"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"wolf\", \"wolves\"]\n",
        "\n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQMp_UKSikgG"
      },
      "source": [
        "### 아마존 리뷰데이터를 Stemming 해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h-CF8gRikgG"
      },
      "source": [
        "tokens = []\n",
        "for doc in df['tokens']:\n",
        "    doc_tokens = []\n",
        "    for token in doc:\n",
        "        doc_tokens.append(ps.stem(token))\n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "df['stems'] = tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYB3VQfSikgH"
      },
      "source": [
        "wc = word_count(df['stems'])\n",
        "\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": false,
        "id": "erDvhklXikgH"
      },
      "source": [
        "### Lemmatization(표제어 추출)\n",
        "\n",
        "Porter 알고리즘은 단지 단어의 끝 부분을 자르는 역할을 합니다. 그래서 사전에도 없는 단어가 많이 나오게 됩니다. 조금 이상하긴 해도 현실적으로 사용하기에 Stemming 은 성능이 나쁘지 않습니다. 알고리즘이 간단해 매우 빠르기 때문에 검색엔진과 정보검색 분야에서 많이 사용하고 있습니다.\n",
        "\n",
        "Lemmatization은 어간추출보다 체계적입니다. 단어들은 기본 사전형 단어 형태인 lemma(표제어)로 변환됩니다. 명사의 복수형은 단수형으로, 동사는 모두 타동사로 변환됩니다. 이렇게 단어들로부터 표제어를 찾아가는 과정은 Stemming 보다 많은 연산이 필요합니다. 그럼 Stacy를 통해 Lemmatization을 진행해 보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r88C4u2vikgH"
      },
      "source": [
        "lem = \"The social wolf. Wolves are complex.\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(lem)\n",
        "\n",
        "# wolf, wolve가 어떤 Lemma로 추출되는지 확인해 보세요\n",
        "for token in doc:\n",
        "    print(token.text, \"  \", token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X6okY3pikgH"
      },
      "source": [
        "# Lemmatization 과정을 함수로 만들어 봅시다\n",
        "def get_lemmas(text):\n",
        "\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl1OE7RhikgH"
      },
      "source": [
        "df['lemmas'] = df['reviews.text'].apply(get_lemmas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwBrTAVtikgH"
      },
      "source": [
        "df['lemmas'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIPA6sbIikgH"
      },
      "source": [
        "wc = word_count(df['lemmas'])\n",
        "wc_top20 = wc[wc['rank'] <= 20]\n",
        "\n",
        "squarify.plot(sizes=wc_top20['percent'], label=wc_top20['word'], alpha=0.6 )\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lGzRc9YikgI"
      },
      "source": [
        "## 참고자료\n",
        "\n",
        "- [Spacy 101](https://course.spacy.io)\n",
        "- [NLTK Book](https://www.nltk.org/book)\n",
        "- [An Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)\n",
        "- [regex101.com(정규식 연습)](https://regex101.com/)\n",
        "- [Python RegEx](https://www.w3schools.com/python/python_regex.asp#sub)\n",
        "- [정규 표현식 시작하기](https://wikidocs.net/4308#_2)"
      ]
    }
  ]
}