{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N423_Language Modeling with RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DcDD5N0WLl5W",
        "mTGgyQtTTiIN",
        "hz6d-H76Pf31",
        "QzDfikzTSE-b",
        "hV7LD6HQSI1D",
        "aT9JUEMYULmX",
        "l5VoE0ohXw2d",
        "svBQyfcmYMLJ",
        "GHoCbrgyYdeE",
        "w9SIBLuVYmjS",
        "ZvuSPMf_ZIji"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_eOv4kAYb0Z"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## ***DATA SCIENCE / SECTION 4 / SPRINT 2 / NOTE 3***\n",
        "\n",
        "---\n",
        "\n",
        "# 언어 모델과 RNN(Recurrent Neural Network, 순환 신경망)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSdN5u4fJ6ol"
      },
      "source": [
        "## 🏆 학습 목표\n",
        "\n",
        "- 언어 모델 (Language Model)\n",
        "    - 통계 기반 언어모델을 이해하고 설명할 수 있습니다.\n",
        "    - 통계 기반 언어모델의 한계를 이해하고 이를 극복하기 위해 등장한 신경망 언어 모델의 장점을 설명할 수 있습니다. \n",
        "\n",
        "- 순환 신경망 (Recurrent Neural Network, RNN)\n",
        "    - RNN의 구조와 작동 방식을 이해하고 설명할 수 있습니다.\n",
        "    - RNN의 장점과 단점을 설명하고 이해할 수 있습니다.\n",
        "\n",
        "- LSTM & GRU\n",
        "    - LSTM과 GRU가 고안된 배경과 구조를 연관지어 설명할 수 있습니다.\n",
        "    - 두 방법의 차이에 대해서 설명할 수 있습니다.\n",
        "\n",
        "- Attention\n",
        "    - Attention이 탄생하게 된 배경에 대해서 설명할 수 있습니다.\n",
        "    - Attention의 장점에 대해서 설명하고 Attention 으로도 해결할 수 없는 RNN의 구조적 단점에 대해서도 이해할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAS2HDrvZKE-"
      },
      "source": [
        "## Warm up\n",
        "\n",
        "- [RNN 소개 영상](https://youtu.be/PahF2hZM6cs)\n",
        "\n",
        "- [LSTM 소개 영상](https://youtu.be/bX6GLbpw-A4)\n",
        "\n",
        "- [Seq2Seq 구조와 Attention 소개 영상](https://youtu.be/WsQLdu2JMgI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcDD5N0WLl5W"
      },
      "source": [
        "## 1. 언어 모델 (Language Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTGgyQtTTiIN"
      },
      "source": [
        "### 1) 언어 모델(Language Model)이란?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZMGEO6aZuze"
      },
      "source": [
        "언어 모델이란 문장과 같은 단어 시퀀스에서 각 단어의 확률을 계산하는 모델입니다.<br/>\n",
        "이전 시간에 배운 **`Word2Vec`** 역시 여러 가지 언어 모델 중 하나입니다.<br/>\n",
        "**`CBoW`** 에서는 주변 단어의 정보를 바탕으로 타겟 단어의 확률을 할당했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx5Bdjcdd2rA"
      },
      "source": [
        "익숙하지 않을 수 있겠지만 수식으로 먼저 생각해보겠습니다.<br/>\n",
        "$l$개의 단어로 구성된 문장은 아래와 같이 나타낼 수 있겠습니다.\n",
        "\n",
        "> $w_1, w_2, w_3, ..., w_l$\n",
        "\n",
        "`CBoW`가 타겟 단어(target word)를 예측할 확률 $P(w_t)$ 은 아래와 같이 구해집니다.\n",
        "\n",
        "> $P(w_t \\vert w_{t-2},w_{t-1},w_{t+1},w_{t+2})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ2SV8W8ghGk"
      },
      "source": [
        "`Word2Vec` 이 나오기 전까지 많은 언어 모델은 목표 단어 왼쪽의 단어만을 고려하여 확률을 계산하였습니다.<br/>\n",
        "$t$ 번째로 단어를 예측하기 위해서 0번째 부터 $t-1$ 번째 까지의 모든 단어 정보를 사용합니다.\n",
        "\n",
        "언어 모델이 목표 단어 왼쪽의 단어만을 고려할 때 문장에서 $t$ 번째에 해당하는 단어를 예측할 확률은 아래와 같이 나타낼 수 있습니다.\n",
        "\n",
        "> $P(w_t \\vert w_{t-1},w_{t-2}, \\cdots ,w_1,w_0)$\n",
        "\n",
        "$l$ 개의 단어로 이루어진 문장이 만들어질 확률은 아래 식과 같아집니다.\n",
        "\n",
        "> $P(w_0,w_1, \\cdots, w_{l-1}, w_l) = P(w_0)P(w_1 \\vert w_0) \\cdots P(w_{l-1} \\vert w_{l-2}, \\cdots, w_1, w_0)P(w_l \\vert w_{l-1}, w_{l-2}, \\cdots, w_1, w_0)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tWjA2bwWIqy"
      },
      "source": [
        "수식이 익숙하지 않을 수 있으니 예시로 한 번 더 살펴보겠습니다.\n",
        "\n",
        "위 언어 모델을 사용하여 \"I am a student\" 라는 문장이 만들어질 확률을 구하면 아래와 같습니다.\n",
        "\n",
        "> $P(\\text{\"I\",\"am\",\"a\",\"student\"}) = P(\\text{\"I\"}) \\times P(\\text{\"am\"} \\vert \\text{\"I\"}) \\times P(\\text{\"a\"} \\vert \\text{\"I\",\"am\"}) \\times P(\\text{\"student\"} \\vert \\text{\"I\",\"am\",\"a\"})$\n",
        "\n",
        "앞 단어 들이 등장했을 때 특정 단어가 등장할 확률을 조건부 확률로 구하게 됩니다.<br/>\n",
        "언어 모델을 잘 나타내는 영상 하나 보고 가도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "72MjG-GF_t4o",
        "outputId": "0adfe000-a2ca-4112-ded1-d0687a90459d"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9GTr4rqlRyw?start=180&end=200\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9GTr4rqlRyw?start=180&end=200\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz6d-H76Pf31"
      },
      "source": [
        "### 2) 통계적 언어 모델 (Statistical Language Model, SLM)\n",
        "\n",
        "통계적 언어 모델은 신경망 언어 모델이 주목받기 전부터 연구되어 온 전통적인 접근 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmvDGKrbZy0z"
      },
      "source": [
        "\n",
        "- **통계적 언어 모델의 확률 계산**\n",
        "\n",
        "통계적 언어 모델에서는 단어의 등장 횟수를 바탕으로 조건부 확률을 계산합니다.\n",
        "\n",
        "다시 _\"I am a student\"_ 라는 문장을 만드는 예시를 생각해보겠습니다.\n",
        "\n",
        "> $P(\\text{\"I\",\"am\",\"a\",\"student\"}) = P(\\text{\"I\"}) \\times P(\\text{\"am\"} \\vert \\text{\"I\"}) \\times P(\\text{\"a\"} \\vert \\text{\"I\",\"am\"}) \\times P(\\text{\"student\"} \\vert \\text{\"I\",\"am\",\"a\"})$\n",
        "\n",
        "첫 번째 항인 $P(\\text{\"I\"})$ 를 구해봅시다. <br/> 전체 말뭉치의 문장 중에서 시작할 때 _\"I\"_ 로 시작하는 문장의 횟수를 구합니다. 전체 말뭉치의 문장이 1000개이고, 그 중 _\"I\"_ 로 시작하는 문장이 100개라면\n",
        "\n",
        "> $$P(\\text{\"I\"}) = \\frac{100}{1000} = \\frac{1}{10}$$\n",
        "\n",
        "다음으로, _\"I\"_ 로 시작하는 100개의 문장 중 바로 다음에 _\"am\"_ 이 등장하는 문장이 50개라면 \n",
        "\n",
        "> $$P(\\text{\"am\"} \\vert \\text{\"I\"}) = \\frac{50}{100} = \\frac{1}{2}$$\n",
        "\n",
        "이런 방식으로 모든 조건부 확률을 구한 뒤 서로를 곱해주면 문장이 등장할 확률 $P(\\text{\"I\",\"am\",\"a\",\"student\"})$ 을 구할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wjMRaIFfZPw"
      },
      "source": [
        "- **통계적 언어 모델의 한계점**\n",
        "\n",
        "통계적 언어 모델은 횟수 기반으로 확률을 계산하기 때문에 희소성(Sparsity) 문제를 가지고 있습니다. 예를 들어, 학습시킬 말뭉치에 _\"1 times\", \"2 times\", ..._ 라는 표현은 등장하지만 _\"7 times\"_ 라는 표현은 없다고 해보겠습니다.\n",
        "\n",
        "그렇다면 이 말뭉치를 학습한 통계적 언어 모델은 아래와 같은 문장을 절대 만들어 낼 수 없게 됩니다.\n",
        "\n",
        "> \"I studied this section 7 times\"\n",
        "\n",
        "_\"7\"_ 이라는 단어가 등장한 순간 바로 다음 _\"times\"_ 가 등장할 확률은 0이 되어버리기 때문입니다.<br/>\n",
        "이렇게 실제로 사용되는 표현임에도 말뭉치에 등장하지 않았다는 이유로 많은 문장이 등장하지 못하게 되는 문제를 희소 문제라고 합니다.<br/>\n",
        "통계적 언어 모델에서 이런 문제를 개선하기 위해서 N-gram 이나 스무딩(smoothing), 백오프(back-off)와 같은 방법이 고안되었습니다.\n",
        "\n",
        "> **더 알아보기** <br/>\n",
        "> 1. N-gram : 통계적 언어 모델을 고도화 하기 위한 방법 중 하나인 N-gram에 대해 조사해봅시다.<br/>\n",
        "> 2. Back-off, Smoothing : 희소 문제를 보완하기 위한 장치인 back-off 와 smoothing에 대해 알아봅시다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzDfikzTSE-b"
      },
      "source": [
        "### 3) 신경망 언어 모델 (Neural Langauge Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keNrz_89m3HV"
      },
      "source": [
        "신경망 언어 모델에서는 횟수 기반 대신 `Word2Vec`이나 `fastText` 등의 출력값인 임베딩 벡터를 사용합니다. <br/>\n",
        "그렇기 때문에 말뭉치에 등장하지 않더라도 의미적, 문법적으로 유사한 단어라면 선택될 수 있습니다.\n",
        "\n",
        "임베딩 벡터에서는 _\"7\"_ 이라는 단어의 벡터가 _\"1\", \"2\" ..._ 등의 단어와 유사한 곳에 위치합니다. <br/> 그렇기 때문에 말뭉치에 _\"7 times\"_ 라는 표현이 등장하지 않더라도 _\"1 times\", \"2 times\"_ 라는 표현이 등장한다면 언어 모델은\n",
        "\n",
        "> \"I studied this section 7 times\"\n",
        "\n",
        "라는 문장을 만들어 낼 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV7LD6HQSI1D"
      },
      "source": [
        "## 2. 순환 신경망 (RNN, Recurrent Neural Network)\n",
        "\n",
        "인공 신경망 언어 모델에서 사용되는 순환 신경망에 대해 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJSMs2CSkj4"
      },
      "source": [
        "### 연속형 데이터 (Sequential Data)\n",
        "\n",
        "- Sequential Data란?\n",
        "    - 어떤 순서로 오느냐에 따라서 단위의 의미가 달라지는 데이터\n",
        "    - Non-sequential Data\n",
        "    ![]()\n",
        "    ![]()\n",
        "    - Sequential Data\n",
        "    ![]()\n",
        "    ![]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT9JUEMYULmX"
      },
      "source": [
        "### RNN의 구조"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPrEp2CZIAjT"
      },
      "source": [
        "가장 간단한 RNN은 Hidden-layer가 1개인 RNN입니다.\n",
        "\n",
        "![RNN, Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "\n",
        "화살표 왼쪽 그림에서 $h$ 라고 쓴 은닉층에 자기 자신의 입력으로 돌아가는 $V$ 가 있습니다.<br/>\n",
        "이 루프는 지금 시점의 $t$의 출력을 위해서 $t-1$ 값이 $U$와 합쳐져 $h$에 다시 반영되는 것을 의미합니다.<br/>\n",
        "출력 벡터가 다시 입력되는 특성 때문에 '순환(Recurrent) 신경망' 이라는 이름이 붙었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyIzz4Mt3lM9"
      },
      "source": [
        "### time-step 별로 펼쳐서 RNN 알아보기\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVipPZJGIEiS"
      },
      "source": [
        "기본 네트워크가 왼쪽 그림처럼 표시되지만 신경망을 시점에 따라 이해하도록 펼쳐보면 오른쪽 그림처럼 나타낼 수 있습니다.<br/>\n",
        "$t-1$ 시점에서는 $x_{t-1}$ 와 $h_{t-2}$가 입력되고 $o_{t-1}$ 이 출력됩니다.<br/>\n",
        "$t$ 시점에서는 $x_t$ 와 $h_{t-1}$ 가 입력되고 $o_t$ 이 출력됩니다.<br/>\n",
        "$t+1$ 시점에서는 $x_{t+1}$ 와 $h_t$ 가 입력되고 $o_{t+1}$ 이 출력됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQ_Fz4jIMJp"
      },
      "source": [
        "t 시점의 RNN 계층은 그 계층으로의 입력 벡터 $x_t$ 와 1개 전의 RNN 계층의 출력 벡터 $h_{t-1}$ 를 받아들입니다.<br/>\n",
        "입력된 두 벡터를 바탕으로 해당 시점에서의 출력을 아래와 같이 계산합니다. \n",
        "\n",
        "> $h_t = sigmoid(h_{t-1}W_h + x_tW_x + b)$\n",
        "\n",
        "가중치는 $W_h, W_x$ 2개가 있습니다. 각각 입력 x를 h로 변환하기 위한 $W_x$와 RNN의 은닉층의 출력을 다음 h로 변환해주는 $W_h$ 입니다. 신경망에서 bias도 있다는 것은 잊지 않으셨으리라 생각합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU7nJUV8IO5A"
      },
      "source": [
        "이 과정을 그림으로 나타내면 다음과 같습니다.\n",
        "\n",
        "![rnn2](https://i.imgur.com/nFMF0Nc.png)\n",
        "\n",
        "이렇게 하면 t 시점에 생성되는 hidden-state 벡터인 $h_t$ 는 해당 시점까지 입력된 벡터 $x_1, x_2, \\cdots, x_{t-1}, x_t$ 의 정보를 모두 가지고 있습니다.<br/>  Sequential 데이터의 순서 정보를 모두 기억하기 때문에 이를 다룰 때 RNN을 많이 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuFkM-j1IWiW"
      },
      "source": [
        "# 배웠던 RNN을 간단한 코드로 살펴보면 다음과 같습니다. \n",
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
        "    h_next = np.sigmoid(t)\n",
        "\n",
        "    self.cache = (x, h_prev, h_next)\n",
        "    return h_next"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49vchZtrXkMI"
      },
      "source": [
        "### 다양한 형태의 RNN\n",
        "\n",
        "실제로 다양한 형태의 RNN이 있습니다. 아래 그림에서 가장 왼쪽에 위치한 one-to-one은 실질적으로 순환이 적용되지는 않은 형태입니다.<br/>\n",
        "나머지 4개의 RNN이 각각 어떤 분야에 사용되는지 알아보겠습니다.\n",
        "\n",
        "![various_rnn](http://karpathy.github.io/assets/rnn/diags.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg522yiFIVYH"
      },
      "source": [
        "1. one-to-many : 1개의 벡터를 받아 Sequential한 벡터를 반환합니다. 이미지를 입력받아 이를 설명하는 문장을 만들어내는 **이미지 캡셔닝(Image captioning)**에 사용됩니다.\n",
        "2. many-to-one : Sequential 벡터를 받아 1개의 벡터를 반환합니다. 문장이 긍정인지 부정인지를 판단하는 **감성 분석(Sentiment analysis)**에 사용됩니다.\n",
        "3. many-to-many(1) : Sequential 벡터를 모두 입력받은 뒤 Sequential 벡터를 출력합니다. **시퀀스-투-시퀀스(Sequence-to-Sequence, Seq2Seq) 구조**라고도 부릅니다. 번역할 문장을 입력받아 번역된 문장을 내놓는 **기계 번역(Machine translation)**에 사용됩니다.\n",
        "4. many-to-many(2) : Sequential 벡터를 입력받는 즉시 Sequential 벡터를 출력합니다. **비디오를 프레임별로 분류(Video classification per frame)**하는 곳에 사용됩니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSVc5mbjXsr5"
      },
      "source": [
        "### RNN의 장점과 단점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcaHkCQJX1o"
      },
      "source": [
        "- **장점**\n",
        "\n",
        "RNN은 모델이 간단하고 (이론적으로는) 어떤 길이의 sequential 데이터라도 처리할 수 있다는 장점을 가지고 있습니다.\n",
        "\n",
        "하지만 RNN은 몇 가지 단점을 가지고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99JK6sFUJdQ"
      },
      "source": [
        "- 단점 1 : **병렬화(parallelization) 불가능**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN5LGndWUYxj"
      },
      "source": [
        "RNN 구조가 가지고 있는 단점 중 하나는 벡터가 순차적으로 입력된다는 점입니다.<br/>\n",
        "이는 sequential 데이터 처리를 가능하게 해주는 요인 중 하나이기도 합니다.<br/>\n",
        "하지만 이러한 구조는 GPU 연산의 장점인 병렬화를 불가능하게 하기도 합니다.<br/>\n",
        "그렇기 때문에 RNN 기반의 모델은 GPU 연산을 하였을 때 이점이 거의 없다는 단점을 가지고 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl5vjneMJDhO"
      },
      "source": [
        "- 단점 2: **기울기 폭발(Exploding Gradient), 기울기 소실(Vanishing Gradient)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRpwbxXBXuLM"
      },
      "source": [
        "단순 RNN의 치명적인 문제점은 역전파 과정에서 발생합니다.<br/>\n",
        "역전파 과정에서 RNN의 활성화 함수인 $\\tanh$ 의 미분값을 전달하게 됩니다. \n",
        "$\\tanh$ 를 미분한 함수의 값은 아래와 같습니다.<br/>\n",
        "\n",
        "![tanh](https://user-images.githubusercontent.com/45377884/91560164-52a14400-e974-11ea-8bf4-bbfc7fd42deb.png)\n",
        "\n",
        "위 그래프에서 최댓값이 1이고, (-4,4) 이외의 범위에서는 거의 0에 가까운 값을 나타내는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FShzAI7VV-G"
      },
      "source": [
        "문제는 역전파 과정에서 이 값을 반복해서 곱해준다는 점입니다.<br/>\n",
        "이 Recurrent가 10회, 100회 반복된다고 보면, 이 값의 100제곱, 1000제곱이 식 내부로 들어가게 됩니다.<br/>\n",
        "만약 이 값이 0.9 일 때 10제곱이 된다면 0.349가 됩니다. 이렇게 되면 시퀀스 앞쪽에 있는 hidden-state 벡터에는 역전파 정보가 거의 전달되지 않게 됩니다.<br/>\n",
        "이런 문제를 **기울기 소실(Vanishing Gradient)**이라고 합니다.\n",
        "반대로 이 값이 1.1 이면 10제곱만해도 2.59배로 커지게 됩니다. 이렇게 되면 시퀀스 앞쪽에 있는 hidden-state 벡터에는 역전파 정보가 과하게 전달됩니다.<br/>\n",
        "이런 문제를 **기울기 폭발(Exploding Gradient)**이라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWGOzd8QVZtA"
      },
      "source": [
        "> \"그렇다면 전달되는 기울기 정보의 크기를 적절하게 조절해주면 되지 않을까?\"\n",
        "\n",
        "라는 생각이 들기 시작합니다.<br/>\n",
        "전달되는 역전파 값의 크기를 조정하는 Gate를 만들어서 RNN의 기울기 소실 문제를 해결하자는 방향에서 출발한 것이 바로 **장단기 기억망(Long-Short Term Memory, LSTM)**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5VoE0ohXw2d"
      },
      "source": [
        "## 3. LSTM & GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svBQyfcmYMLJ"
      },
      "source": [
        "### LSTM (Long Term Short Memory, 장단기기억망)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PqJ2l6YSLx"
      },
      "source": [
        "이렇게 **RNN에 Gate를 추가한 모델을 LSTM**이라고 합니다.<br/>\n",
        "요즘에는 단순한 RNN은 사용하지 않고 대부분 장기 단기 기억 장치 (LSTM)를 사용합니다.<br/>\n",
        "요즘 RNN이라고 하면 당연히 LSTM이나 이후에 배울 GRU를 지칭할 정도로 LSTM이 대표적인 RNN의 모델이 되었습니다.<br/>\n",
        "오히려 전에 배운 RNN을 `기본적인 RNN(Vanilla RNN)`이라고 따로 구별하여 표현하기도 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aocYxj7AVtrf"
      },
      "source": [
        "- **LSTM의 구조**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2GN3aS0V1dT"
      },
      "source": [
        "먼저 그림을 통해 LSTM 셀 하나의 구조를 알아보겠습니다.\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F9905CF385BD5F5EC027F20\"/>\n",
        "\n",
        "RNN의 셀 구조보다 뭔가 상당히 복잡해졌습니다.\n",
        "\n",
        "위에서 살펴본 것처럼 LSTM이 등장한 배경은 앞쪽 시퀀스까지 역전파 정보가 제대로 전달되지 않는 [기울기 소실(Vanishing gradient)](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) 문제를 해결하기 위함입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNZi7DwbXRI2"
      },
      "source": [
        "LSTM은 기울기 소실 문제를 해결하기 위해 3가지 게이트(gate)를 추가하였습니다. 각 게이트는 다음과 같은 역할을 합니다.\n",
        "\n",
        "1. forget gate ($f_t$): 과거 정보를 얼마나 유지할 것인가?\n",
        "2. input gate ($i_t$) : 새로 입력된 정보는 얼마만큼 활용할 것인가?\n",
        "3. output gate ($o_t$) : 두 정보를 계산하여 나온 출력 정보를 얼마만큼 넘겨줄 것인가?\n",
        "\n",
        "그리고 hidden-state 말고도 활성화 함수를 직접 거치지 않는 상태인 cell-state 가 추가되었습니다.<br/>\n",
        "cell-state는 역전파 과정에서 정보 손실이 없기 때문에 **최근(short) 이벤트에 비중을 결정할 수 있으면서 동시에 오래된(long) 정보를 완전히 잃지 않을 수 있다**는 LSTM의 장점을 살릴 수 있는 필수적 요소가 되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1vIvtc0YYjJ"
      },
      "source": [
        "- **LSTM의 역전파**\n",
        "\n",
        "*(다소 복잡하므로 당장은 LSTM의 역전파 과정을 전부 다 이해하려 하지 않아도 괜찮습니다!)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJpuM01Y404"
      },
      "source": [
        "<img src=\"http://i.imgur.com/2BZtc2l.gif\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-Jo5pjsYxWj"
      },
      "source": [
        "- **LSTM의 사용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcSKg-1kYxOt"
      },
      "source": [
        "LSTM은 실제로 굉장히 많은 곳에 사용됩니다.\n",
        "\n",
        "여러 언어 모델에서 LSTM을 사용하고 있습니다.<br/>\n",
        "Gate가 적용되지 않은 RNN, 즉 Vanilla RNN은 10~20 단어로 이루어진 문장에 대한 분류/생성/번역 등의 성능이 매우 낮습니다.<br/>\n",
        "Vanilla RNN이 가지고 있는 기울기 소실/폭발 문제 때문입니다.<br/>\n",
        "\n",
        "언어 모델 뿐만 아니라 신경망을 활용한 시계열 알고리즘에는 대부분 LSTM을 사용하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoCbrgyYdeE"
      },
      "source": [
        "### GRU (Gated Recurrent Unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI_lVKKRYxPv"
      },
      "source": [
        "한편, 이 LSTM의 간소한 버전인 GRU도 가볍게 소개하겠습니다.<br/>\n",
        "그림을 통해 GRU 셀의 구조를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvD-d3PhYlKm"
      },
      "source": [
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F99F0EC3E5BD5F6460255CF\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T74E2DdeY57x"
      },
      "source": [
        "- GRU 셀의 특징\n",
        "\n",
        "1. LSTM에서 있었던 cell-state가 사라졌습니다. cell-state 벡터 $c_t$ ​와 hidden-state 벡터 $h_t$​가 하나의 벡터 $h_t$​로 통일되었습니다.\n",
        "2. 하나의 Gate $z_t$가 forget, input gate를 모두 제어합니다.  $z_t$가 1이면 forget 게이트가 열리고, input 게이트가 닫히게 되는 것과 같은 효과를 나타냅니다. 반대로 $z_t$가 0이면 input 게이트만 열리는 것과 같은 효과를 나타냅니다.\n",
        "3. GRU 셀에서는 output 게이트가 없어졌습니다. 대신에 전체 상태 벡터 $h_t$가 각 time-step에서 출력되며, 이전 상태의 $h_{t-1}$의 어느 부분이 출력될 지 새롭게 제어하는 Gate인 $r_t$ 가 추가되었습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9SIBLuVYmjS"
      },
      "source": [
        "### LSTM 코드 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjz6_OkgbHU9"
      },
      "source": [
        "이제부터 TensorFlow와 Keras를 사용하여 자연어로 RNN을 훈련시켜 보겠습니다.\n",
        "\n",
        "- https://www.tensorflow.org/guide/keras/rnn\n",
        "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "- https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL4Mv82gxlrv"
      },
      "source": [
        "시퀀스는 주가부터 텍스트까지 다양한 모양과 형태로 제공됩니다. 우리는 주로 텍스트에 초점을 맞춰 공부하도록 하겠습니다. 왜냐하면 텍스트를 시퀀스로 모델링하는 것은 신경망의 강점이기 때문입니다. 먼저 TensorFlow 튜토리얼을 사용하여 간단한 분류 작업부터 시작하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18BNmDLxowg"
      },
      "source": [
        "- **Keras를 이용한 RNN/LSTM 감정 분류(Sentiment classification)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSEKGkIx17L"
      },
      "source": [
        "'''\n",
        "# IMDB 감성 분류 작업에 대한 LSTM 모델을 학습합니다.\n",
        "데이터 집합이 사실 너무 작아서 LSTM이 강점을 발휘할 수 없습니다.\n",
        "TF-IDF + LogReg와 같은 간단하고 빠른 방법이 LSTM에 비해 훨씬 빠릅니다.\n",
        "**Notes**\n",
        "- RNN은 까다롭습니다. 배치 크기 선택이 중요하고, 손실 및 최적화 도구 선택이 중요합니다. 일부 구성은 수렴되지 않을 것입니다.\n",
        "- 교육 중 LSTM 손실 감소 패턴은 CNN/MLP/etc에서 보는 것는 상당히 다를 수 있습니다.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imd\n",
        "\n",
        "# 이 단어 랭크 수 뒤에 텍스트는 사용하지 않도록 잘라냅니다(단어 등장 순위 : max_feature)\n",
        "# 참조링크 : https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
        "max_features = 20000\n",
        "# 최대 단어 길이\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT7elx2Tx6hm"
      },
      "source": [
        "print('Pad Sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_rru-5x-5L"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpqT7b_KyBJh"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(max_features, 128))\n",
        "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Embedding(max_features, 128),\n",
        "  tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq459yjQyDyj"
      },
      "source": [
        "unicorns = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size, \n",
        "          epochs=3, \n",
        "          validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDXWi9TByHBj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQdFbh68xyHc"
      },
      "source": [
        "- **Keras를 이용한 LSTM 텍스트 생성기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnYYB89fYmLe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(unicorns.history['loss'])\n",
        "plt.plot(unicorns.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySeBtkvIyW-R"
      },
      "source": [
        "# max length를 이용하여 문자열의 크기 정렬\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7wqty4RyZoI"
      },
      "source": [
        "# LSTM 모델 제작\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0huQTyya9e"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQr_UmPaycjd"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=60,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvuSPMf_ZIji"
      },
      "source": [
        "## 4. RNN 구조에 Attention 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7LxiXv-kK9j"
      },
      "source": [
        "### 기존 RNN 기반(LSTM, GRU) 번역 모델의 단점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8W05Lab6Iw"
      },
      "source": [
        "RNN이 가진 가장 큰 단점 중 하나는 기울기 소실로부터 나타나는 **장기 의존성(Long-term dependency)** 문제입니다.<br/>\n",
        "장기 의존성 문제란 문장이 길어질 경우 앞 단어의 정보를 잃어버리게 되는 현상입니다.<br/>\n",
        "장기 의존성 문제를 해결하기 위해 나온 것이 셀 구조를 개선한 LSTM과 GRU입니다.<br/>\n",
        "기계 번역에서 RNN 기반의 모델(LSTM, GRU)이 단어를 처리하는 방법은 아래와 같다고 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ox-uhBb72u"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif\" alt=\"seq2seq_6\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyaxMWVVkYwm"
      },
      "source": [
        "### Attention의 등장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEoreUZQb-bz"
      },
      "source": [
        "위 구조의 문제는 고정 길이의 hidden-state 벡터에 모든 단어의 의미를 담아야 한다는 점입니다.<br/>\n",
        "아무리 LSTM, GRU가 장기 의존성 문제를 개선하였더라도 문장이 매우 길어지면(30-50 단어) 모든 단어 정보를 고정 길이의 hidden-state 벡터에 담기 어렵습니다.<br/>\n",
        "이런 문제를 해결하기 위해서 고안된 방법이 바로 **Attention(어텐션)** 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Au-MKZRb_Db"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif\" alt=\"seq2seq_7\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2F-34z6cB2Y"
      },
      "source": [
        "Attention은 각 인코더의 Time-step 마다 생성되는 hidden-state 벡터를 간직합니다.<br/>\n",
        "입력 단어가 N개라면 N개의 hidden-state 벡터를 모두 간직하게 됩니다.<br/>\n",
        "모든 단어가 입력되면 생성된 hidden-state 벡터를 모두 디코더에 넘겨줍니다.\n",
        "\n",
        "디코더는 받은 N개의 hidden-state 벡터를 어떻게 활용할까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOv0ZMGcESi"
      },
      "source": [
        "- **검색 시스템의 아이디어 둘러보기**\n",
        "\n",
        "잠시 돌아가 검색 시스템에 대해 알아봅시다.<br/>\n",
        "아래는 구글에서 _\"what is attention in nlp\"_ 라는 검색어를 구글에 입력했을 때의 검색 결과를 나타낸 이미지입니다.\n",
        "\n",
        "<img src=\"https://i.imgur.com/JdCQr1l.png\" alt=\"retrieval_system\" width=\"600\" />\n",
        "\n",
        "그림에서 볼 수 있듯이 검색 시스템은 아래와 같은 3단계를 거쳐 작동합니다.\n",
        "\n",
        "1. 찾고자 하는 정보에 대한 검색어(Query)를 입력합니다.\n",
        "2. 검색 엔진은 검색어와 가장 비슷한 키워드(Key)를 찾습니다.\n",
        "3. 그리고 해당 키워드(Key)와 연결된 페이지(Value)를 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NabC-Q8-hiu7",
        "outputId": "c84d90ee-5694-407a-e1d2-4f76b76ecc6e"
      },
      "source": [
        "# 더 알아보기 : 파이썬의 딕셔너리도 비슷한 형태로 작동합니다.\n",
        "# Query('a')를 던지면 딕셔너리에서 동일한 Key('a')를 찾은 뒤 Value(123)을 반환합니다.\n",
        "\n",
        "dict1 = {'a':123, 'b':425, 'c':236, 'd':945}\n",
        "dict1['a']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "123"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_4UTfufkH4b"
      },
      "source": [
        "### 디코더에서 Attention이 동작하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8cU0WZ8ez7m"
      },
      "source": [
        "디코더에서 단어를 생성하는 과정을 알아보겠습니다.\n",
        "\n",
        "디코더의 각 time-step 마다의 hidden-state 벡터는 쿼리(query)로 작용합니다.<br/>\n",
        "인코더에서 넘어온 N개의 hidden-state 벡터를 키(key)로 여기고 이들과의 연관성을 계산합니다.<br/>\n",
        "이 때 계산은 내적(dot-product)을 사용하고 내적의 결과를 Attention 가중치로 사용합니다.<br/>\n",
        "\n",
        "아래는 디코더 첫 단어 \"I\"(`Time-step 4`)에 대한 어텐션 가중치가 구해지는 과정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_z9iXJ1e2G3"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86044868-ae8b4180-ba85-11ea-9fee-2977edfd47ce.gif\" alt=\"seq2seq_img\" width=\"800\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEEOdnoZnTY7"
      },
      "source": [
        "1. 쿼리(Query)인 디코더의 hidden-state 벡터, 키(Key)인 인코더에서 넘어온 hidden-state 벡터를 준비합니다.\n",
        "2. 각각의 벡터를 내적한 값을 구합니다.\n",
        "3. 이 값에 소프트맥스(softmax) 함수를 취해줍니다.\n",
        "4. 소프트맥스를 취하여 나온 값에 밸류(Value)에 해당하는 인코더에서 넘어온 hidden-state 벡터를 곱해줍니다. \n",
        "5. 이 벡터를 모두 더해줍니다. 이 벡터의 성분 중에는 쿼리-키 연관성이 높은 밸류 벡터의 성분이 더 많이 들어있게 됩니다.\n",
        "6. (그림에는 나와있지 않지만) 최종적으로 5에서 생성된 벡터와 디코더의 hidden-state 벡터를 사용하여 출력 단어를 결정하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDHPM3Ie5Kz"
      },
      "source": [
        "디코더는 인코더에서 넘어온 모든 Hidden state 벡터에 대해 위와 같은 계산을 실시합니다.<br/>\n",
        "그렇기 때문에 Time-step마다 출력할 단어가 어떤 인코더의 어떤 단어 정보와 연관되어 있는지, 즉 어떤 단어에 **집중(Attention)**할 지를 알 수 있습니다.<br/>\n",
        "Attention을 활용하면 디코더가 인코더에 입력되는 모든 단어의 정보를 활용할 수 있기 때문에 장기 의존성 문제를 해결할 수 있습니다.\n",
        "\n",
        "아래는 예시로 제시되었던 문장을 번역(**`Je suis etudiant => I am a student`**)했을 때 각 단어마다의 Attention 스코어를 시각화 한 그림입니다.<br/>\n",
        "\n",
        "> _\"I\"_ -> _\"Je\"_ <br/>\n",
        "> _\"am\"_ -> _\"suis\"_<br/>\n",
        "> _\"a\"_ -> _\"suis\", \"etudiant\"_<br/>\n",
        "> _\"student\"_ -> _\"etudiant\"_\n",
        "\n",
        "왼쪽 단어가 생성될 때 오른쪽 단어와 연관되어 있음을 확인할 수 있습니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXI1tj2we7hR"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/45377884/86047018-29a22700-ba89-11ea-98ee-a90b2fb70a23.gif\" alt=\"attn_visualization\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAShST2e8Ik"
      },
      "source": [
        "### RNN(LSTM) with Attention 코드 실습\n",
        "\n",
        "스페인어-영어 번역하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYnEXYKGewLQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5-fp4gfKxz"
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqxC2SgsfPCp"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYpqyskDfQqi"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iSyCfNrfSvT"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
        "                for line in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66wKsoJYfT5p"
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEBx2J0OfUUu"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOjGqWpsfVmK"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJWIindfXJ5"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6SFHicNfYa1"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWEEaYHlfalW"
      },
      "source": [
        "- 구조와 관련된 파라미터 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc3UtRN7fiI-"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpvSXgBBfi6o"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZBV4UUBfgkr"
      },
      "source": [
        "- 인코더 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3-rfGWfm1k"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehWNiQ_ffokz"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
        "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKG3Y_BafqBJ"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZuX1KCdfr8h"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjbGgc5Efuor"
      },
      "source": [
        "- **디코더 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l1Zd0Nkft6G"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GK7-WsOfx4k"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size)', sample_decoder_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABDicArafzt_"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTeGGZS1f2M6"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20YBdgsYf5gr"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "      \n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0EujLQZf8NJ"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIGrhbkrf-_u"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnamxQ1sgAkK"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ6MY4FtgDe7"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V6aKQ4BgFq3"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyVjn9UeaH0h"
      },
      "source": [
        "## Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zAIH2w0YzRe"
      },
      "source": [
        "- 언어 모델 (Language Model)\n",
        "    - 통계 기반의 언어 모델(Statistical Language Model)\n",
        "    - 신경망 언어 모델(Neural Network Language Model) \n",
        "\n",
        "- 순환 신경망 (Recurrent Neural Network, RNN)\n",
        "    - RNN의 구조\n",
        "    - RNN의 장점과 단점\n",
        "        - 기울기 소실(Gradient Vanishing)\n",
        "\n",
        "- LSTM & GRU\n",
        "    - LSTM\n",
        "        - Cell state\n",
        "    - GRU\n",
        "\n",
        "- Attention\n",
        "    - Attention\n",
        "        - 장기 의존성(Long-term Dependency)\n",
        "    - Query, Key, Vector\n",
        "        - 어떤 벡터가 각 요소에 해당될까요?"
      ]
    }
  ]
}