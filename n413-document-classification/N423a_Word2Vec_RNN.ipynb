{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N423a_Word2Vec_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skn9ZDVioced"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / Assignment 3*\n",
        "\n",
        "--- \n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "LSTM을 이용한 텍스트 입력기에 대해서 배워보았다. <br>\n",
        "오늘의 과제는 아주 간단한 미니 프로젝트처럼 구성되어있다.<br>\n",
        "아래 체크리스트를 점검하면서 과제를 수행해보자. \n",
        "- [ ] Keras, tensorflow, pytorch 등 프레임워크에서 제공하는 예제가 아닌 단어/문장 생성기 등 시퀀스 예제 찾기 (RNN, LSTM, GRU, Attention 사용예제)\n",
        "- [ ] 찾는 과정에서 사용한 **검색어**를 모두 입력하고, 사용한 소스를 찾은 검색어는 **Bold**로 처리한다.\n",
        "- [ ] 찾아본 코드를 Colab, Conda, jupyter등 내 환경에서 구현해본다. (참고, github 등에서 readme.md를 잘 따라하면 수행하기가 쉽다. 샘플링크 참조)\n",
        "- [ ] 만약 찾은 코드가 제대로 돌아가지 않는다면, 수정하는 것도 좋지만, 같은 기능을 하는 다른 github를 찾아보며 위과정을 반복한다.\n",
        "- [ ] 찾은 소스코드가 구현되었다면, 이제 디테일을 파악해보자, 어떤 데이터(데이터 설명)로 처리를 하였고, 어떤 구조(LSTM의 구조 및 node개수 등)의 모델인지 검색해본다. \n",
        "- [ ] 이해하기 어려운 코드를 포함하고 있고 직접 설명하기 어렵다면, 인용된 논문을 찾아 주석을 달아두고 이해한 부분만으로 정리한 뒤 Reference에 해당 논문을 남겨두자.\n",
        "\n",
        "c.f. [샘플링크](https://github.com/keithito/tacotron)를 보면 Git에 잘 구성된 페이지를 볼 수 있다. (**단, 이 링크는 샘플일 뿐, 이 링크를 통해서 과제를 해서는 안 된다**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltj1je1fp5rO"
      },
      "source": [
        "# TODO - Words, words, mere words, no matter from the heart."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- 같은 네트워크에 적용할 다른 데이터를 찾아본다. 예) 셰익스피어 텍스트를 이용한 과제라면, 니체의 글을 적용해보는 등의 다른 데이터를 적용해보기\n",
        "- 해당 모델에서 가장 쉬운 부분을 변경해보기. 예) hidden layer의 node수를 변경, activation function을 바꿔보기, LSTM을 GRU, RNN 등으로 변경해보기 등\n",
        "- readme에서 제공한 자료보다 성능을 올려볼 것\n",
        "\n",
        "## 피해야 할 Resources:\n",
        "- https://www.tensorflow.org/guide/keras/rnn\n",
        "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "- https://victorzhou.com/blog/keras-rnn-tutorial/\n",
        "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "- https://www.kaggle.com/thousandvoices/simple-lstm\n"
      ]
    }
  ]
}