{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
    "\n",
    "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 3*\n",
    "\n",
    "---\n",
    "\n",
    "# Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텍스트에서 특성을 추출하고 문서 분류기를 만들 수 있습니다\n",
    "* 잠재의미분석(Latent Semantic Analysis,LSA)을 수행합니다\n",
    "* Spacy 단어 임베딩을 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 동영상을 시청하세요.\n",
    "- [특이값 분해(SVD)의 기하학적 의미와 활용 소개](https://youtu.be/cq5qlYtnLoY)\n",
    "    - 우리는 SVD를 통해 무엇을 얻고자 하는 것일까요?\n",
    "\n",
    "다음 웹페이지를 읽어보세요. 이해가 안 되는 부분은 넘어가도 좋습니다.\n",
    "- [Text classification](https://developers.google.com/machine-learning/guides/text-classification)\n",
    "    - Introduction\n",
    "    - Step 1: Gather Data\n",
    "    - Step 2: Explore Your Data\n",
    "    - Step 2.5: Choose a Model\n",
    "    - Step 3: Prepare Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러분은 이미 머신러닝을 이용해 분류기를 학습시킬 수 있습니다. 그리고 텍스트 문서에서 어떻게 특성들을 추출하는지 배웠습니다. 이제 텍스트 문서를 분류하는 모델을 만들 차례 입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트에서 특성들을 추출하고 문서 분류기를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn 파이프라인을 사용하면 머신러닝 프로세스에 사용되는 여러 컴포넌트들을 쉽게 연결할 수 있었습니다.\n",
    "\n",
    "파이프라인을 이용해 Raw 데이터 입력, 정제, 학습 프로세스를 하나의 함수에서 실행되는 것과 같이 편리하게 실행할 수 있습니다.\n",
    "\n",
    "파이프라인을 사용하는 또 다른 이유는 하이퍼파라미터 튜닝을 쉽게 할 수 있기 때문입니다. 벡터화 과정중에 n-gram 범위라든지 최대 토큰의 수라든지 최적의 결과를 내기 위해 여러 파라미터들을 바꾸어 가며 실험을 해 보아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20개 뉴스그룹으로 분류된 18,000개의 뉴스그룹 문서 데이터셋 입니다.\n",
    "- [20newsgroups](https://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset)\n",
    "- 전자와 정치에 관한 두 개의 다른 카테고리 뉴스를 가져오겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['sci.electronics',\n",
    "              'talk.politics.misc']\n",
    "\n",
    "ng_train = fetch_20newsgroups(subset='train'\n",
    "                             , remove=('headers', 'footers', 'quotes')\n",
    "                             , categories=categories\n",
    "                             )\n",
    "\n",
    "ng_test = fetch_20newsgroups(subset='test'\n",
    "                             , remove=('headers', 'footers', 'quotes')\n",
    "                             , categories=categories\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습, 테스트 데이터가 분리되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ng_train.data), len(ng_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 문서를 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train.data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문서의 타겟 레이블 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train.target[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터를 살펴봅시다\n",
    "- [Step 2: Explore Your Data](https://developers.google.com/machine-learning/guides/text-classification/step-2)\n",
    "\n",
    "학습 모델을 만드는 일은 데이터 분석 과정 중 한 부분입니다. 모델링 전 데이터의 특성을 확인하고 이해하는 과정을 통해 더욱 좋은 모델을 만들 수 있게 됩니다. 데이터를 미리 잘 살펴보면 더 적은 데이터로 더 높은 성능을 가진 모델을 만들 수도 있습니다.\n",
    "\n",
    "다음 웹페이지를 참고 하세요, https://developers.google.com/machine-learning/guides/text-classification/step-2\n",
    "- explore_data.py 파일은 다음 URL에 있습니다. ipynb 폴더에 다운받아 import 하여 사용하세요\n",
    "- [explore_data.py](https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/explore_data.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 웹페이지를 참고 하세요, https://developers.google.com/machine-learning/guides/text-classification/step-2\n",
    "# explore_data.py 파일은 다음 URL에 있습니다. ipynb 폴더에 다운받아 import 하여 사용하세요\n",
    "## https://github.com/google/eng-edu/blob/master/ml/guides/text_classification/explore_data.py\n",
    "import explore_data as ed\n",
    "import seaborn as sns\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the median number of words per sample given corpus.\n",
    "median_words_per_sample = ed.get_num_words_per_sample(ng_train.data)\n",
    "print('Median words per sample: ', median_words_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.plot_sample_length_distribution(ng_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.plot_class_distribution(ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the frequency distribution of n-grams.\n",
    "# Arguments\n",
    "#     samples_texts: list, sample texts.\n",
    "#     ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "#         Min and mplt are the lower and upper bound values for the range.\n",
    "#     num_ngrams: int, number of n-grams to plot.\n",
    "#         Top `num_ngrams` frequent n-grams will be plotted.\n",
    "ed.plot_frequency_distribution_of_ngrams(ng_train.data\n",
    "                                     , ngram_range=(1, 2)\n",
    "                                     , num_ngrams=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델은 어떻게 선택할까요? \n",
    "- [Step 2.5: Choose a Model](https://developers.google.com/machine-learning/guides/text-classification/step-2-5)\n",
    "\n",
    "앞서 간단히 살펴본 데이터의 수치를 기반해서 2.5단계에서는 어떤 분류 모델을 사용할 것인지 선택을 해 보겠습니다.\n",
    "\n",
    "다음 플로우차트에서 어떻게 분류 모델을 선택해야 하는지 여러 실험을 통한 결과를 가지고 간략한 가이드를 제공합니다. 목표는 주어진 데이터세트에서 가능한 최선의 정확도를 낼 수 있고 동시에 학습시 계산량을 줄이는 것이었습니다. 최적의 방법을 찾기 위해 감성분석, 토픽 분류 등 여러 문제에 대해 12개 데이터 세트를 사용했으며 여러 모델구조를 사용해 45만번 이상의 실험을 수행하였습니다.\n",
    "\n",
    "![flowchart](https://developers.google.com/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S/W ratio를 계산해 봅시다, 구글 flowchar에 따르면,\n",
    "# S/W < 1500 일 경우 BoW 를 사용해 벡터화 하고 simple MLP 모델 or 앙상블 모델을 사용하는것을 추천하고 있습니다.\n",
    "sw_ratio = len(ng_train.data) / median_words_per_sample\n",
    "print('number of samples / median words per sample ratio: ', int(sw_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 모델로 베이스라인을 만들어 봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 구성 요소를 만듭니다\n",
    "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# DTM을 생성합니다.\n",
    "dtm = vect.fit_transform(ng_train.data)\n",
    "\n",
    "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인을 정의합니다\n",
    "pipe = Pipeline([\n",
    "    ('vect',vect)\n",
    "    ,('clf', rfc)\n",
    "])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': (0.7, 1.0) # document frequency(%) 높을 경우 제거\n",
    "    ,'vect__min_df': (2, 5, 10) # document frequency(횟수) 낮을 경우 제거\n",
    "    ,'vect__max_features': (5000, 20000) # 코퍼스에서 term frequency 높은 순서대로 나열하여 제한\n",
    "    ,'clf__n_estimators': (100, 500)\n",
    "    ,'clf__max_depth': (10, 20, None)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(ng_train.data, ng_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 테스트 데이터에 대해 정확도를 구해보겠습니다\n",
    "y_test = grid_search.predict(ng_test.data)\n",
    "accuracy_score(ng_test.target, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재의미분석(Latent Semantic Analysis, LSA)\n",
    "\n",
    "\n",
    "잠재의미분석(LSA)는 BoW방법론을 사용해 만든 문서-단어행렬(DTM) 같은 행렬 데이터의 차원을 축소해 문서들에 숨어있는(latent) 의미(Topics)를 끌어내는 방법입니다.\n",
    "\n",
    "이때 차원 축소에는 Truncated SVD(특이값 분해)를 사용해 원하는 문서나, 단어의 차원을 축소합니다.\n",
    "\n",
    "물론 차원이 축소가 되더라도 기존에 문서나, 단어들 간의 거리관계는 어느정도 보존이 됩니다.\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Konstantinos_Bougiatiotis/publication/321025221/figure/fig9/AS:668660309962763@1536432449448/Singular-value-decomposition-followed-by-rank-lowering-for-latent-semantic-indexing.jpg\" alt=\"Singular value decomposition followed by rank lowering for latent semantic indexing\"/>\n",
    "\n",
    "SVD를 사용해 행렬 A를 $U, Σ, V^{T}$ 세 행렬의 곱으로 분해(decomposion) 합니다.\n",
    "$$A=UΣV^{T}$$\n",
    "$$AV=UΣ$$\n",
    "\n",
    "\n",
    "$U$ 와 $V^T$ 의 열 벡터는 특이벡터(singular vector)라 불리는데\n",
    "Truncated SVD는 특이값(singular value, $Σ$ 대각성분) 가운데 가장 큰 k개만 남기고 해당 특이값에 대응하는 특이벡터들로 원래 행렬 A를 근사하는 방법입니다.\n",
    "\n",
    "물론 0보다 큰 특이값을 제거하면 정보의 손실이 발생하므로 적당히 필요한 차원만큼 k를 선택합니다.\n",
    "\n",
    "여기서 만약 m개 문서, n개 단어로 이루어진 행렬을 truncated SVD로 분해해 다음과 같은 분해를 수행 했다면 다음과 같은 근사 식을 얻을 것이며\n",
    "\n",
    "$$A_k=U_kΣ_kV^{T}_k$$\n",
    "\n",
    "$U_k$와 $V_k$를 사용해 n차원으로 표현 되었던 문서를 k차원으로, 또는 m 차원으로 표현되었던 단어를 k 차원으로 표현할 수 있게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TruncatedSVD 를 사용하여 파이프라인에서 차원을 축소하고 분류문제를 풀어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# SVD를 사용한 차원 축소\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english'\n",
    "                       , ngram_range=(1,2)\n",
    "                       , min_df=2\n",
    "                       , max_df=0.7\n",
    "                       , token_pattern=r'(?u)\\b\\w[A-Za-z]+\\b' # 영문자만 사용\n",
    "                       , max_features=10000\n",
    "                      )\n",
    "\n",
    "svd = TruncatedSVD(algorithm='randomized'\n",
    "                   , n_iter=5\n",
    "                   , random_state=2)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=500, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # 100~500 사이의 정수 크기로 차원을 줄입니다\n",
    "#     'svd__n_components': stats.randint(100, 500)\n",
    "    'svd__n_components': stats.randint(2, 3) # 문서의 차원을 2로 고정\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Tfidf 문서 벡터화, 2. svd 차원축소, 3. 랜덤포레스트 분류기\n",
    "pipe = Pipeline([\n",
    "    ('vect', vect)\n",
    "    , ('svd', svd)\n",
    "    , ('clf', rfc)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "random_search = RandomizedSearchCV(pipe,params, cv=3, n_iter=5, n_jobs=-1, verbose=1)\n",
    "random_search.fit(ng_train.data, ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd__n_components: Random search에서 선택된 줄어든 차원을 확인할 수 있습니다.\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋으로 정확도를 계산합니다\n",
    "y_test = random_search.predict(ng_test.data)\n",
    "accuracy_score(ng_test.target, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(ng_test.target, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD를 따로 수행해서 행렬분해가 어떻게 되는지 확인해 보겠습니다.\n",
    "\n",
    "- randomized_svd를 사용해서 $U$, $\\Sigma$, $V^T$ 행렬을 구해보겠습니다.\n",
    "- randomized_svd는 Truncated SVD에서 내부적으로 사용되는 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 TF-IDF vectorizer로 벡터화하여 사용하겠습니다.\n",
    "A = random_search.best_estimator_.named_steps['vect'].transform(ng_train.data).todense()\n",
    "X_test = random_search.best_estimator_.named_steps['vect'].transform(ng_test.data).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTM\n",
    "A.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized_svd를 사용하여 U, S(Sigma), VT(V transposed) 행렬을 얻습니다\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "U, S, VT = randomized_svd(A\n",
    "                         , n_components=2 # 상위 특이값 2개를 선택합니다\n",
    "                         , n_iter=5\n",
    "                         , random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, S.shape, VT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대각 성분(특이값)만 가져왔습니다.\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V_k^{T} V_k = I_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT @ VT.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A_k=U_kΣ_kV^{T}_k$ \n",
    "\n",
    "=> $A_k V_k = U_k\\Sigma_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A는 A_k는 아니지만 결과는 같습니다.\n",
    "AV = A @ VT.T\n",
    "AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US = U @ np.diag(S)\n",
    "US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 문서들도 $V_k$를 사용해서 문서의 차원을 축소할 수 있습니다.\n",
    "\n",
    "$XV_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trans = X_test @ VT.T\n",
    "X_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### truncatedSVD 결과물로 비교해 보겠습니다.\n",
    "\n",
    "truncatedSVD 속성 `components_` 가 행렬 VT입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = random_search.best_estimator_.named_steps['svd'].components_\n",
    "print((components - VT).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TruncatedSVD 속성을 조금 더 살펴봅시다.\n",
    "\n",
    "먼저 특이값를 확인해 보겠습니다. 위에서 구한 S(Sigma)와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.best_estimator_.named_steps['svd'].singular_values_)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원이 줄어든 데이터(AV, US)의 분산값입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.best_estimator_.named_steps['svd'].explained_variance_)\n",
    "print(np.var(US, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 테스트 문서 샘플을 SVD를 사용해 차원 축소해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시로 테스트 문서 0 벡터를 사용합니다.\n",
    "d0 = X_test[0]\n",
    "print(d0.shape)\n",
    "print(random_search.best_estimator_.named_steps['svd'].transform(d0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터를 모두 변환해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test_trans_2 = random_search.best_estimator_.named_steps['svd'].transform(X_test)\n",
    "print(X_test_trans_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 직접 구한 값과 같습니다. (X_test @ VT.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 0만 보겠습니다.\n",
    "X_test_trans[0], X_test_trans_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA는 SVD를 통해 찾아진 topic들을 가지고 잠재적인 의미를 분석하는 것 입니다.\n",
    "\n",
    "각 차원에 어떤 단어들이 모여 있는지 확인해 봅시다.\n",
    "SVD를 통해 찾아진 두 잠재적 의미군(토픽)에 속하는 단어들이 들어간 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms: 벡터화한 단어\n",
    "terms = random_search.best_estimator_.named_steps['vect'].get_feature_names()\n",
    "for index, topic in enumerate(components[:10]): # topic 최대 10개만 표시)\n",
    "    print('Topic %d: '%(index + 1), [terms[i] for i in topic.argsort()[::-1][:6]]) # 수치가 큰 단어부터 최대 6단어 표시\n",
    "    print('Score %d: '%(index + 1), [topic[i] for i in topic.argsort()[::-1][:6]]) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy 단어 임베딩을 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The tortoise jumped into the lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy는 기본적으로 300차원으로 임베딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spacy = get_word_vectors(ng_train.data)\n",
    "\n",
    "len(X_spacy) == len(ng_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_spacy = get_word_vectors(ng_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤포레스트로 학습해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_spacy, ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_spacy = rfc.predict(X_test_spacy)\n",
    "accuracy_score(ng_test.target, y_test_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP(Multi-layer perceptron classifier)를 간단히 사용해보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs'\n",
    "                   , alpha=1e-5\n",
    "                   , hidden_layer_sizes=(16,2)\n",
    "                   , random_state=2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_spacy, ng_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(X_test_spacy)\n",
    "accuracy_score(ng_test.target, y_test_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고자료\n",
    "\n",
    "- [Singular Value Decomposition (the SVD)](https://youtu.be/mBcLRGuAFUk)\n",
    "- [특이값 분해(Singular Value Decomposition, SVD)의 활용](https://darkpgmr.tistory.com/106)\n",
    "- [numpy 벡터와 행렬연산 참고자료](https://ebbnflow.tistory.com/159)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P41_NLP",
   "language": "python",
   "name": "p41_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
