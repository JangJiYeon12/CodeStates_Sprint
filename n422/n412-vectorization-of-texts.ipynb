{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "P41_NLP",
      "language": "python",
      "name": "p41_nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "n412-vectorization-of-texts.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2shgXaGhA1H"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 2*\n",
        "\n",
        "---\n",
        "\n",
        "# Vectorization of Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDrzKrHhA1W"
      },
      "source": [
        "* 텍스트 문서를 벡터로 표현해 봅시다\n",
        "* 유사도를 이용해 문서를 검색해 봅시다\n",
        "* 문서를 벡터로 만들기 위해 단어 임베딩을 사용해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFs_igVhA1W"
      },
      "source": [
        "### Warm up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQmySnWhhA1X"
      },
      "source": [
        "\n",
        "#### Vectorization of Texts (텍스트 벡터화)\n",
        "\n",
        "컴퓨터는 자연어를 있는 그대로 이해할 수 없습니다. 그래서 자연어를 컴퓨터가 사용할 수 있는 형태로 가공해야 합니다. 이전 강의에서는 정제된 토큰의 수를 기반으로 분석을 진행하였습니다. 오늘은 이 개념을 좀 더 확장시켜 **Bag-of-Words(BoW), Term Frequency-Inverse Document Frequency ([TF-IDF](https://youtu.be/meEchvkdB1U))** 같은 텍스트를 벡터화 하는 방법과 **단어 임베딩(Word embedding) 모델**에 대해 다루어 보겠습니다. 다음 수업에 진행할 검색, 시각화, 분류 작업에 벡터 표현법을 활용할 것입니다.\n",
        "\n",
        "- TF - 문서에서 단어가 많이 등장하는 지를 수치화한 중요도 스코어(score)\n",
        "- $\\space$ ex) \"A new car, used car, car reivew\" <br> - TF score :  A($1 \\over 7$), new($1 \\over 7$), car($3 \\over 7$), used($1 \\over 7$), reivew($1 \\over 7$) \n",
        "- IDF - $log {(총 \\space 문장 \\space 개수) \\over {(단어가 \\space 출현한 \\space 문장의 \\space 개수)}}$\n",
        "-  $\\space$ ex) 'A'가 모든 문장에서 등장 IDF = log(N/N) = 0\n",
        "- TF-IDF score = TF * IDF = (1/7) * 0 = 0\n",
        "\n",
        "\n",
        "머신러닝 모델에서 사용하기 위해 텍스트 데이터를 벡터화하는 것은 컴퓨터가 사용할 수 있는 수치정보()로 변환하는것으로 생각할 수 있습니다.\n",
        "\n",
        "Bag-of-Words(BoW) 모델은 문장이나 문서들에서 문법, 단어 순서 등의 개념을 제거하여 단순히 **단어들의 빈도**만 고려하는 모델입니다.\n",
        "\n",
        "BoW는 문서를 토큰화한 후 토큰의 빈도를 기반으로 벡터화 합니다. 데이터프레임 형태로 보자면 행은 각 문서가 되고 열은 중복되지 않는 각 단어가 됩니다. 열에는 단순히 각 단어가 문서에 얼마나 존재하는지를 카운트한 값을 넣거나(*CountVectorizer* 사용) TF-IDF 값이 오게 할 수 있습니다(*TfidfVectorizer 사용).\n",
        "\n",
        "벡터 표현은 파이썬에서는 `sklearn`, `spacy`패키지를 사용해 구현할 수 있습니다.\n",
        "\n",
        "\n",
        "#### Dataset\n",
        "\n",
        "이번 세션에서는 BBC에서 제공하는 데이터셋을 사용하여 텍스트를 다뤄볼 예정입니다. BBC 웹사이트를 방문하는 고객이 방금 읽은 뉴스(문서)를 기반으로 비슷한 다른 문서를 적절하게 추천하게 만들 수 있으면 좋겠죠. 그런 작업을 한번 시작해 봅니다. \n",
        "\n",
        "* 아래 링크에서 파일을 다운로드 받아 노트 폴더에서 압축을 해제합니다. <br> data folder가 생성되고 001.txt ~ 401.txt 파일이 있는지 확인합니다.\n",
        "* colab 유저들은 업로드 코드를 이용하여 업로드 후 unzip하여 파일을 풀면 사용할 수 있습니다.\n",
        "\n",
        "[bbc_fulltext.zip](https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/bbc_fulltext/bbc_fulltext.zip)\n",
        "\n",
        "\n",
        "#### 레퍼런스\n",
        "[BBC News Tech]('https://www.bbc.com/news/technology')\n",
        ", D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006. [논문링크](http://mlg.ucd.ie/datasets/bbc.html)\n",
        "\n",
        "**(예제에서는 tech 관련 문서만 사용합니다)** <br>\n",
        "파일을 논문링크에서 직접 받을 수 있으나 몇가지 조치를 해야 합니다. 링크에서, <br> Dataset: BBC -> >> Download raw text files -> 압축해제(bbc-fulltext.zip) <br> > tech 폴더의 이름을 변경 한 뒤 \"txt 파일(001.txt ~ 401.txt) 확인\" > 원하는 위치로 이동 후 사용가능.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCHMkiNhA1X"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMP1QpY6hA1Y"
      },
      "source": [
        "## 텍스트 문서를 벡터로 표현해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3iUy8nhA1Y"
      },
      "source": [
        "BoW를 사용해 Document Term Matrices(DTM, 문서-단어행렬)을 만들어 보겠습니다. 각 행은 문서를 나타내고 각 열은 단어를 나타냅니다.\n",
        "- 각 셀의 값은 여러가지 방법으로 표현될 수 있는데\n",
        "    - 단어의 출현 빈도를 나타내거나,\n",
        "    - 단순히 단어의 존재 유무(binary)를 표현할 수 있고,\n",
        "    - TF-IDF 값으로 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIshXRhDhA1Y"
      },
      "source": [
        "# 모듈에서 사용할 라이브러리와 spacy 모델을 불러옵니다.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0UrJckqhA1Z"
      },
      "source": [
        "**Spacy로 텍스트에서 토큰을 추출해 보겠습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zl8SfWFhA1Z"
      },
      "source": [
        "text = \"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMz6Z1FchA1a",
        "outputId": "6f436f07-3233-4a6f-9d5a-67befe9a0ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = nlp(text)\n",
        "\n",
        "print([token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['information', 'retrieval', 'tf', 'idf', 'TFIDF', 'short', 'term', 'frequency', 'inverse', 'document', 'frequency', 'numerical', 'statistic', 'intend', 'reflect', 'important', 'word', 'document', 'collection', 'corpus', 'weight', 'factor', 'search', 'information', 'retrieval', 'text', 'mining', 'user', 'modeling', 'tf', 'idf', 'value', 'increase', 'proportionally', 'number', 'time', 'word', 'appear', 'document', 'offset', 'number', 'document', 'corpus', 'contain', 'word', 'help', 'adjust', 'fact', 'word', 'appear', 'frequently', 'general', 'tf', 'idf', 'popular', 'term', 'weight', 'scheme', 'today', 'survey', 'conduct', '2015', 'show', '83', 'text', 'base', 'recommender', 'system', 'digital', 'library', 'use', 'tf', 'idf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJEsWTtvhA1a"
      },
      "source": [
        "import os \n",
        "\n",
        "# BBC 데이터를 불러오기 위한 함수입니다.\n",
        "\n",
        "def gather_data(filefolder):\n",
        "    \"\"\" 폴더 내 텍스트 파일을 각각 리스트 요소에 저장하는 함수\n",
        "    Args:\n",
        "        filefolder (str): .txt 파일이 존재하는 경로\n",
        "    Returns:\n",
        "        문서를 요소로하는 리스트\n",
        "    \"\"\"\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    files = os.listdir(filefolder)\n",
        "    \n",
        "    for article in files: \n",
        "        path = os.path.join(filefolder, article)\n",
        "\n",
        "        # txt로 끝나는 파일만 읽습니다\n",
        "        if  path[-3:] == 'txt':\n",
        "            # rb:Read the file in Binary mode\n",
        "            with open(path, 'rb') as f:\n",
        "                data.append(f.read())\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmN9EYHt54t9",
        "outputId": "015c2e63-a198-4902-c802-c7af1308c46f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "# for Colab User\n",
        "# Upload files\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!unzip \"bbc_fulltext.zip\" # 업로드 이름이 다르다면 수정해서 사용하세요."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-34624d9d-0e6c-4c84-985e-f1a7c0d7bbad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-34624d9d-0e6c-4c84-985e-f1a7c0d7bbad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cfc86c0372dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for Colab User\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ZNkN-36qcP",
        "outputId": "b36f3544-1236-4c68-f80e-c4ea87ffc6b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 폴더가 제대로 생성되었는 지 확인합니다. \n",
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'bbc_fulltext (1).zip'\t data   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOWp3EEkhA1a"
      },
      "source": [
        "bbc tech 관련 문서들을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7i7-H9mhA1a"
      },
      "source": [
        "data = gather_data('./data')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcFIgb8rhA1b",
        "outputId": "c894b061-f1f0-464c-c632-87ead5cfb13c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Joke e-mail virus tricks users\\n\\nA virus that disguises itself as a joke is spreading rapidly across the net.\\n\\nAnti-virus firms are issuing high-level warnings about the new version of the Bagle e-mail program that seems to be catching a lot of people out. The Windows virus grabs e-mail addresses from Microsoft Outlook and uses its own mail sending software to spread itself to new victims. When it infects a machine, the Bagle variant turns off security measures that usually protect PCs.\\n\\nThe new variant is called Bagle.AT, Bagle.BB and Bagle.AU and the attachment bearing the virus code is labelled as either \"joke\" or \"price\".\\n\\nThe body of the virus usually contains nothing but a smiley or emoticon. The virus can strike computers running Windows 95, 98, ME, NT, 2000 and XP. Users will be infected if they open the attachment that travels with the e-mail. As well as plundering Microsoft Outlook for e-mail addresses to send itself to, Bagle.AT also tries to turn off the firewall and security centre services on Windows XP machines. BBC News Online has received five warnings about the virus from security companies. Finnish company F-Secure gave the virus its second highest threat level. \"We\\'ve had several reports all over the world,\" said Mikko Hypponen, director of anti-virus research for F-Secure. Security firm Network Box said that it stopped more than 30,000 copies an hour of the virus as the outbreak reached a peak. Black Spider said it had stopped more than 1 million copies of Bagle.AT since the outbreak began at 0630 BST (0530 GMT). Anti-virus firms urged users to be wary of unexpected e-mail messages bearing attachments and to update their software to ensure they are protected against the latest threats.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ineq4LzfhA1b"
      },
      "source": [
        "import seaborn as sns\n",
        "# plot 스타일과 폰트 크기를 설정합니다.\n",
        "sns.set(style='whitegrid', font_scale=1.15)\n",
        "\n",
        "# 문서별 단어의 수 분포도 그리는 함수\n",
        "def plot_text_length_dist(text_list):\n",
        "\n",
        "    # 문장이 요소인 리스트를 받아 각 문서의 단어 수를 가진 리스트를 만듭니다\n",
        "    num_words = [len(doc.split()) for doc in text_list]\n",
        "    \n",
        "    sns.displot(num_words)\n",
        "    plt.title('# of words per documents')\n",
        "    plt.xlabel('Number of words')\n",
        "    plt.ylabel('Number of documents')\n",
        "    plt.show()       "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juxrcqHDhA1b"
      },
      "source": [
        "대략 500 단어 정도로 표현된 문서가 가장 많이 보입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riyI9CVThA1b",
        "outputId": "1945fe52-af73-49c6-e5d9-5da7bcd9e736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "plot_text_length_dist(data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iW_tDfwhA1c"
      },
      "source": [
        "### CountVectorizer\n",
        ": 단어들의 출현 빈도로 여러개의 문서를 벡터화하는 함수 <br>\n",
        ": 모든 문자를 소문자로 전환하여 계산함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQpwlq6ghA1c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# wiki 문장들을 리스트에 나누어 입력해봅니다. \n",
        "text = [\"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\"\n",
        ",\"It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\"\n",
        ",\"The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\"\n",
        ",\"tf–idf is one of the most popular term-weighting schemes today.\"\n",
        ",\"A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.\"]\n",
        "\n",
        "# CountVectorizer 생성\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# text를 기반으로 어휘 사전을 생성\n",
        "vect.fit(text) \n",
        "# vect.fit(data[:5])\n",
        "\n",
        "# text를 DTM(document-term matrix)으로 변환(transform)\n",
        "dtm = vect.transform(text)\n",
        "# dtm = vect.transform(data[:5])"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYo0jODwhA1c"
      },
      "source": [
        "vocabulary(모든 토큰)와 맵핑된 인덱스 정보를 확인할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EtyoQchA1c",
        "outputId": "8476bc0d-c8a6-4f7b-829b-a0bffea56ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vect.vocabulary_"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2015': 0,\n",
              " '83': 1,\n",
              " 'adjust': 2,\n",
              " 'and': 3,\n",
              " 'appear': 4,\n",
              " 'appears': 5,\n",
              " 'as': 6,\n",
              " 'based': 7,\n",
              " 'by': 8,\n",
              " 'collection': 9,\n",
              " 'conducted': 10,\n",
              " 'contain': 11,\n",
              " 'corpus': 12,\n",
              " 'digital': 13,\n",
              " 'document': 14,\n",
              " 'documents': 15,\n",
              " 'fact': 16,\n",
              " 'factor': 17,\n",
              " 'for': 18,\n",
              " 'frequency': 19,\n",
              " 'frequently': 20,\n",
              " 'general': 21,\n",
              " 'helps': 22,\n",
              " 'how': 23,\n",
              " 'idf': 24,\n",
              " 'important': 25,\n",
              " 'in': 26,\n",
              " 'increases': 27,\n",
              " 'information': 28,\n",
              " 'intended': 29,\n",
              " 'inverse': 30,\n",
              " 'is': 31,\n",
              " 'it': 32,\n",
              " 'libraries': 33,\n",
              " 'mining': 34,\n",
              " 'modeling': 35,\n",
              " 'more': 36,\n",
              " 'most': 37,\n",
              " 'number': 38,\n",
              " 'numerical': 39,\n",
              " 'of': 40,\n",
              " 'offset': 41,\n",
              " 'often': 42,\n",
              " 'one': 43,\n",
              " 'or': 44,\n",
              " 'popular': 45,\n",
              " 'proportionally': 46,\n",
              " 'recommender': 47,\n",
              " 'reflect': 48,\n",
              " 'retrieval': 49,\n",
              " 'schemes': 50,\n",
              " 'searches': 51,\n",
              " 'short': 52,\n",
              " 'showed': 53,\n",
              " 'some': 54,\n",
              " 'statistic': 55,\n",
              " 'survey': 56,\n",
              " 'systems': 57,\n",
              " 'term': 58,\n",
              " 'text': 59,\n",
              " 'tf': 60,\n",
              " 'tfidf': 61,\n",
              " 'that': 62,\n",
              " 'the': 63,\n",
              " 'times': 64,\n",
              " 'to': 65,\n",
              " 'today': 66,\n",
              " 'use': 67,\n",
              " 'used': 68,\n",
              " 'user': 69,\n",
              " 'value': 70,\n",
              " 'weighting': 71,\n",
              " 'which': 72,\n",
              " 'word': 73,\n",
              " 'words': 74}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZLNl0pUhA1c",
        "outputId": "a5b01ec8-6032-4eba-876f-bb63a626e559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dtm.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4dr7UvhA1d"
      },
      "source": [
        "추출된 토큰을 나열해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_034SHvhA1d",
        "outputId": "fb7971d1-4993-4d8b-82df-e2b9ea2e0cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(vect.get_feature_names())"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015', '83', 'adjust', 'and', 'appear', 'appears', 'as', 'based', 'by', 'collection', 'conducted', 'contain', 'corpus', 'digital', 'document', 'documents', 'fact', 'factor', 'for', 'frequency', 'frequently', 'general', 'helps', 'how', 'idf', 'important', 'in', 'increases', 'information', 'intended', 'inverse', 'is', 'it', 'libraries', 'mining', 'modeling', 'more', 'most', 'number', 'numerical', 'of', 'offset', 'often', 'one', 'or', 'popular', 'proportionally', 'recommender', 'reflect', 'retrieval', 'schemes', 'searches', 'short', 'showed', 'some', 'statistic', 'survey', 'systems', 'term', 'text', 'tf', 'tfidf', 'that', 'the', 'times', 'to', 'today', 'use', 'used', 'user', 'value', 'weighting', 'which', 'word', 'words']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo8ojHRBhA1d",
        "outputId": "0b5fa252-0651-4fac-9fed-b12f5d1b66b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data[4]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Mobiles rack up 20 years of use\\n\\nMobile phones in the UK are celebrating their 20th anniversary this weekend.\\n\\nBritain\\'s first mobile phone call was made across the Vodafone network on 1 January 1985 by veteran comedian Ernie Wise. In the 20 years since that day, mobile phones have become an integral part of modern life and now almost 90% of Britons own a handset. Mobiles have become so popular that many people use their handset as their only phone and rarely use a landline.\\n\\nThe first ever call over a portable phone was made in 1973 in New York but it took 10 years for the first commercial mobile service to be launched. The UK was not far behind the rest of the world in setting up networks in 1985 that let people make calls while they walked. The first call was made from St Katherine\\'s dock to Vodafone\\'s head office in Newbury which at the time was over a curry house. For the first nine days of 1985 Vodafone was the only firm with a mobile network in the UK. Then on 10 January Cellnet (now O2) launched its service. Mike Caudwell, spokesman for Vodafone, said that when phones were launched they were the size of a briefcase, cost about \\xc2\\xa32,000 and had a battery life of little more than 20 minutes.\\n\\n\"Despite that they were hugely popular in the mid-80s,\" he said. \"They became a yuppy must-have and a status symbol among young wealthy business folk.\" This was also despite the fact that the phones used analogue radio signals to communicate which made them very easy to eavesdrop on. He said it took Vodafone almost nine years to rack up its first million customers but only 18 months to get the second million. \"It\\'s very easy to forget that in 1983 when we put the bid document in we were forecasting that the total market would be two million people,\" he said. \"Cellnet was forecasting half that.\" Now Vodafone has 14m customers in the UK alone. Cellnet and Vodafone were the only mobile phone operators in the UK until 1993 when One2One (now T-Mobile) was launched. Orange had its UK launch in 1994. Both newcomers operated digital mobile networks and now all operators use this technology. The analogue spectrum for the old phones has been retired. Called Global System for Mobiles (GSM) this is now the most widely used phone technology on the planet and is used to help more than 1.2 billion people make calls. Mr Caudwell said the advent of digital technology also helped to introduce all those things, such as text messaging and roaming that have made mobiles so popular.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUj5YBA0hA1d"
      },
      "source": [
        "dtm의 타입을 보면 compressed sparse Row matrix임을 알 수 있습니다. csr: Compressed Sparse Row matrix, sparse matrix 형태에서 0을 표현하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EonNdMuvhA1d",
        "outputId": "8686bc2a-a8f1-4a3e-e8c6-8c5503a88d1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "type(dtm)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j_S9jAYhA1e",
        "outputId": "8cb32ae6-0846-4cca-fe3f-81f7a040dcbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# (row, column)  count\n",
        "print(dtm)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 9)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 14)\t2\n",
            "  (0, 18)\t1\n",
            "  (0, 19)\t2\n",
            "  (0, 23)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 25)\t1\n",
            "  (0, 26)\t2\n",
            "  (0, 28)\t1\n",
            "  (0, 29)\t1\n",
            "  (0, 30)\t1\n",
            "  (0, 31)\t3\n",
            "  (0, 39)\t1\n",
            "  (0, 44)\t2\n",
            "  (0, 48)\t1\n",
            "  (0, 49)\t1\n",
            "  (0, 52)\t1\n",
            "  (0, 55)\t1\n",
            "  (0, 58)\t1\n",
            "  (0, 60)\t1\n",
            "  (0, 61)\t1\n",
            "  (0, 62)\t1\n",
            "  (0, 65)\t2\n",
            "  (0, 73)\t1\n",
            "  :\t:\n",
            "  (3, 43)\t1\n",
            "  (3, 45)\t1\n",
            "  (3, 50)\t1\n",
            "  (3, 58)\t1\n",
            "  (3, 60)\t1\n",
            "  (3, 63)\t1\n",
            "  (3, 66)\t1\n",
            "  (3, 71)\t1\n",
            "  (4, 0)\t1\n",
            "  (4, 1)\t1\n",
            "  (4, 7)\t1\n",
            "  (4, 10)\t1\n",
            "  (4, 13)\t1\n",
            "  (4, 24)\t1\n",
            "  (4, 26)\t2\n",
            "  (4, 33)\t1\n",
            "  (4, 40)\t1\n",
            "  (4, 47)\t1\n",
            "  (4, 53)\t1\n",
            "  (4, 56)\t1\n",
            "  (4, 57)\t1\n",
            "  (4, 59)\t1\n",
            "  (4, 60)\t1\n",
            "  (4, 62)\t1\n",
            "  (4, 67)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcfHPTRrhA1e"
      },
      "source": [
        "0을 표현한 형태로 만들면 .todense를 사용할 수 있고, 이런경우 numpy.matrix형태가 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz3m9i8ahA1e",
        "outputId": "6cc0d265-e748-4781-9a58-eb22d9f38498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Return a dense matrix representation\n",
        "# dtm.todense()\n",
        "print(type(dtm))\n",
        "print(type(dtm.todense()))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'numpy.matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZaCXze6hA1e"
      },
      "source": [
        "데이터프레임 형태로 결과를 보고 싶다면"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5h5m4FHhA1e",
        "outputId": "785d5c53-9891-45a9-a562-28d2c6ed59e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "print(type(dtm))\n",
        "dtm"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2015</th>\n",
              "      <th>83</th>\n",
              "      <th>adjust</th>\n",
              "      <th>and</th>\n",
              "      <th>appear</th>\n",
              "      <th>appears</th>\n",
              "      <th>as</th>\n",
              "      <th>based</th>\n",
              "      <th>by</th>\n",
              "      <th>collection</th>\n",
              "      <th>conducted</th>\n",
              "      <th>contain</th>\n",
              "      <th>corpus</th>\n",
              "      <th>digital</th>\n",
              "      <th>document</th>\n",
              "      <th>documents</th>\n",
              "      <th>fact</th>\n",
              "      <th>factor</th>\n",
              "      <th>for</th>\n",
              "      <th>frequency</th>\n",
              "      <th>frequently</th>\n",
              "      <th>general</th>\n",
              "      <th>helps</th>\n",
              "      <th>how</th>\n",
              "      <th>idf</th>\n",
              "      <th>important</th>\n",
              "      <th>in</th>\n",
              "      <th>increases</th>\n",
              "      <th>information</th>\n",
              "      <th>intended</th>\n",
              "      <th>inverse</th>\n",
              "      <th>is</th>\n",
              "      <th>it</th>\n",
              "      <th>libraries</th>\n",
              "      <th>mining</th>\n",
              "      <th>modeling</th>\n",
              "      <th>more</th>\n",
              "      <th>most</th>\n",
              "      <th>number</th>\n",
              "      <th>numerical</th>\n",
              "      <th>of</th>\n",
              "      <th>offset</th>\n",
              "      <th>often</th>\n",
              "      <th>one</th>\n",
              "      <th>or</th>\n",
              "      <th>popular</th>\n",
              "      <th>proportionally</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>schemes</th>\n",
              "      <th>searches</th>\n",
              "      <th>short</th>\n",
              "      <th>showed</th>\n",
              "      <th>some</th>\n",
              "      <th>statistic</th>\n",
              "      <th>survey</th>\n",
              "      <th>systems</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>tfidf</th>\n",
              "      <th>that</th>\n",
              "      <th>the</th>\n",
              "      <th>times</th>\n",
              "      <th>to</th>\n",
              "      <th>today</th>\n",
              "      <th>use</th>\n",
              "      <th>used</th>\n",
              "      <th>user</th>\n",
              "      <th>value</th>\n",
              "      <th>weighting</th>\n",
              "      <th>which</th>\n",
              "      <th>word</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   2015  83  adjust  and  appear  ...  value  weighting  which  word  words\n",
              "0     0   0       0    0       0  ...      0          0      0     1      0\n",
              "1     0   0       0    1       0  ...      0          1      0     0      0\n",
              "2     0   0       1    1       1  ...      1          0      1     2      1\n",
              "3     0   0       0    0       0  ...      0          1      0     0      0\n",
              "4     1   1       0    0       0  ...      0          0      0     0      0\n",
              "\n",
              "[5 rows x 75 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGuj6KyThA1e"
      },
      "source": [
        "세번째 문장과, dtm을 비교해 보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd-D524WhA1f",
        "outputId": "4c93c8bc-47d3-4dee-e57d-00704a2c88df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "text[2]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzTOSWOhA1f"
      },
      "source": [
        "**CountVectorizer를 BBC data에 적용해 봅시다**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9pVcwbRhA1f",
        "outputId": "64605ba0-f22b-4c6c-9909-6b4b9d60e61f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## stop_words = 'english' 영어에 해당하는 불용어 처리를 합니다.\n",
        "## max_features=n, 빈도 순서대로 top n 단어만 사용합니다.\n",
        "vect = CountVectorizer(stop_words='english'\n",
        "                       , max_features=10000)\n",
        "# fit & transform\n",
        "dtm = vect.fit_transform(data)\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsgyNcfHhA1f",
        "outputId": "5226fdb5-4cc6-4d86-90f5-25b6001d2c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>000s</th>\n",
              "      <th>0051</th>\n",
              "      <th>007</th>\n",
              "      <th>01</th>\n",
              "      <th>028</th>\n",
              "      <th>04m</th>\n",
              "      <th>05</th>\n",
              "      <th>0530</th>\n",
              "      <th>056</th>\n",
              "      <th>0630</th>\n",
              "      <th>080</th>\n",
              "      <th>0800</th>\n",
              "      <th>0870</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>100m</th>\n",
              "      <th>100s</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>104</th>\n",
              "      <th>106</th>\n",
              "      <th>106cm</th>\n",
              "      <th>1080</th>\n",
              "      <th>10cm</th>\n",
              "      <th>10m</th>\n",
              "      <th>10s</th>\n",
              "      <th>10th</th>\n",
              "      <th>10x7in</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>115</th>\n",
              "      <th>117</th>\n",
              "      <th>11b</th>\n",
              "      <th>11m</th>\n",
              "      <th>12</th>\n",
              "      <th>120</th>\n",
              "      <th>120bn</th>\n",
              "      <th>...</th>\n",
              "      <th>yeob</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yeun</th>\n",
              "      <th>yh</th>\n",
              "      <th>yle</th>\n",
              "      <th>yoda</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yonca</th>\n",
              "      <th>yoran</th>\n",
              "      <th>york</th>\n",
              "      <th>yorker</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngsters</th>\n",
              "      <th>youth</th>\n",
              "      <th>yuppy</th>\n",
              "      <th>yusuf</th>\n",
              "      <th>zafi</th>\n",
              "      <th>zander</th>\n",
              "      <th>zar</th>\n",
              "      <th>zdnet</th>\n",
              "      <th>zealous</th>\n",
              "      <th>zed</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zenith</th>\n",
              "      <th>zens</th>\n",
              "      <th>zero</th>\n",
              "      <th>zhang</th>\n",
              "      <th>zip</th>\n",
              "      <th>zodiac</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "      <th>zonealarm</th>\n",
              "      <th>zones</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zooms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 10000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   00  000  000s  0051  007  01  ...  zombies  zone  zonealarm  zones  zoom  zooms\n",
              "0   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "1   0    0     0     0    0   0  ...        0     0          0      0     0      0\n",
              "2   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "3   0    0     0     0    0   0  ...        0     0          0      0     0      0\n",
              "4   0    1     0     0    0   0  ...        0     0          0      0     0      0\n",
              "\n",
              "[5 rows x 10000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp_BuYEYhA1f"
      },
      "source": [
        "### TfidfVectorizer\n",
        "\n",
        "#### Term Frequency - Inverse Document Frequency (TF-IDF, 단어빈도-역문서빈도)\n",
        "- [TF-IDF 참고](https://mungingdata.wordpress.com/2017/11/25/episode-1-using-tf-idf-to-identify-the-signal-from-the-noise/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFv0E6ZIhA1g"
      },
      "source": [
        "*t=단어, d=문서, n=총 문서수*\n",
        "\n",
        "TF-IDF Score <br> $ = \\large tf(t,d) \\times idf(t)$\n",
        "<br></br>\n",
        "\n",
        "TF(Term Frequency, 단어의 빈도), <br> **특정 문서 d에서 특정 단어 t가 쓰인 빈도**:\n",
        "\n",
        "$\\large tf(t,d) = \\frac{Term\\; t\\; frequency\\; in\\; document}{Total\\; words\\; in\\; document}$\n",
        "<br></br>\n",
        "\n",
        "DF(Document Frequency, 문서의 빈도),<br>  **특정 단어 t가 나타난 문서의 갯수**:\n",
        "\n",
        "$\\large df(t) = \\small {documents\\; with\\; term\\; t}$\n",
        "<br></br>\n",
        "\n",
        "IDF : log(n/(DF 역수(inverse)))\n",
        "\n",
        "$\\large idf(t) = log(\\frac{n}{1+df(t)})$\n",
        "<br></br>\n",
        "\n",
        "TF-IDF 를 사용하는 이유는 문서를 구분하는데 어떤 단어가 중요한지(**unique**) 찾는 것 입니다.\n",
        "\n",
        "수식을 살펴보면, \n",
        "<br> 여러 문서에서 많이 등장하는 단어일 수록 중요도가 낮다고 판단하며(IDF), <br> 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다(TF).\n",
        "\n",
        "TF-IDF를 통한 자연어 처리는 간단하고 빠르게 구현할 수 있으므로 좋은 Baseline으로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceXM5l7yhA1g"
      },
      "source": [
        "#### TF-IDF(Tfidf) vectorizer vs Count vectorizer\n",
        "TF-IDF vectorizer를 생성하고 dtm을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHOIOh_hA1g",
        "outputId": "dc2e0399-f06e-473f-a0c9-fe15ea269c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한하였습니다.\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=15)\n",
        "\n",
        "# Fit 후 dtm을 만듭니다.(문서, 단어마다 tf-idf 값을 계산합니다)\n",
        "dtm = tfidf.fit_transform(text)\n",
        "\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "dtm"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>document</th>\n",
              "      <th>frequency</th>\n",
              "      <th>idf</th>\n",
              "      <th>information</th>\n",
              "      <th>number</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>searches</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>weighting</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.475965</td>\n",
              "      <td>0.589946</td>\n",
              "      <td>0.166183</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.294973</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.237982</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166183</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.237982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.526778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425001</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.276074</td>\n",
              "      <td>0.276074</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.684374</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.192782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.552149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.404837</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.579748</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.404837</td>\n",
              "      <td>0.579748</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.372642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.661438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.533644</td>\n",
              "      <td>0.372642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     corpus  document  frequency  ...        tf  weighting      word\n",
              "0  0.237982  0.475965   0.589946  ...  0.166183   0.000000  0.237982\n",
              "1  0.000000  0.000000   0.000000  ...  0.000000   0.425001  0.000000\n",
              "2  0.276074  0.276074   0.000000  ...  0.192782   0.000000  0.552149\n",
              "3  0.000000  0.000000   0.000000  ...  0.404837   0.579748  0.000000\n",
              "4  0.000000  0.000000   0.000000  ...  0.372642   0.000000  0.000000\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqNXLX1yhA1h"
      },
      "source": [
        "같은 파라미터로 CountVectorizer를 사용해 tfidf 결과와 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfy8eVKOhA1h",
        "outputId": "d24488e0-5110-4f57-e010-ed24316a15a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "vect = CountVectorizer(stop_words='english', max_features=15)\n",
        "dtm = vect.fit_transform(text)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names())\n",
        "dtm"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>corpus</th>\n",
              "      <th>document</th>\n",
              "      <th>frequency</th>\n",
              "      <th>idf</th>\n",
              "      <th>information</th>\n",
              "      <th>number</th>\n",
              "      <th>recommender</th>\n",
              "      <th>reflect</th>\n",
              "      <th>retrieval</th>\n",
              "      <th>searches</th>\n",
              "      <th>term</th>\n",
              "      <th>text</th>\n",
              "      <th>tf</th>\n",
              "      <th>weighting</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   corpus  document  frequency  idf  ...  text  tf  weighting  word\n",
              "0       1         2          2    1  ...     0   1          0     1\n",
              "1       0         0          0    0  ...     1   0          1     0\n",
              "2       1         1          0    1  ...     0   1          0     2\n",
              "3       0         0          0    1  ...     0   1          1     0\n",
              "4       0         0          0    1  ...     1   1          0     0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7nUg4rphA1h"
      },
      "source": [
        "#### BBC 데이터에 tfidf vectorizer를 적용해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHnIK0uqhA1h",
        "outputId": "6cc7c1fa-1bd3-49a7-ea10-92f016eba9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "dtm = tfidf.fit_transform(data)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "\n",
        "dtm.head()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>104</th>\n",
              "      <th>10m</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>120</th>\n",
              "      <th>13</th>\n",
              "      <th>130</th>\n",
              "      <th>14</th>\n",
              "      <th>149</th>\n",
              "      <th>15</th>\n",
              "      <th>150</th>\n",
              "      <th>16</th>\n",
              "      <th>167</th>\n",
              "      <th>17</th>\n",
              "      <th>17m</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>1980s</th>\n",
              "      <th>1982</th>\n",
              "      <th>1983</th>\n",
              "      <th>1985</th>\n",
              "      <th>1990s</th>\n",
              "      <th>1993</th>\n",
              "      <th>1994</th>\n",
              "      <th>1995</th>\n",
              "      <th>1bn</th>\n",
              "      <th>1m</th>\n",
              "      <th>20</th>\n",
              "      <th>200</th>\n",
              "      <th>2000</th>\n",
              "      <th>2001</th>\n",
              "      <th>2002</th>\n",
              "      <th>2003</th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>...</th>\n",
              "      <th>worry</th>\n",
              "      <th>worrying</th>\n",
              "      <th>worse</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthwhile</th>\n",
              "      <th>wouldn</th>\n",
              "      <th>wow</th>\n",
              "      <th>wright</th>\n",
              "      <th>wristwatch</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writers</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrongful</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wsis</th>\n",
              "      <th>x1</th>\n",
              "      <th>xbox</th>\n",
              "      <th>xenon</th>\n",
              "      <th>xp</th>\n",
              "      <th>xxx</th>\n",
              "      <th>yahoo</th>\n",
              "      <th>yang</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yen</th>\n",
              "      <th>yes</th>\n",
              "      <th>yoda</th>\n",
              "      <th>yoran</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngsters</th>\n",
              "      <th>youth</th>\n",
              "      <th>zafi</th>\n",
              "      <th>zen</th>\n",
              "      <th>zombies</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.026072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.051915</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102341</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066212</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.034386</td>\n",
              "      <td>0.079190</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087326</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027346</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.028192</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.028940</td>\n",
              "      <td>0.066646</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073493</td>\n",
              "      <td>0.241386</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.071006</td>\n",
              "      <td>0.067081</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.122338</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05528</td>\n",
              "      <td>0.058505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        000        10  100  101  104  ...  youngsters  youth  zafi  zen  zombies\n",
              "0  0.026072  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "1  0.000000  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "2  0.034386  0.079190  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "3  0.000000  0.000000  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "4  0.028940  0.066646  0.0  0.0  0.0  ...         0.0    0.0   0.0  0.0      0.0\n",
              "\n",
              "[5 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoiVe1n7hA1h"
      },
      "source": [
        "#### 이번에는 조금 더 파라미터를 튜닝하고, spacy tokenizer 를 사용해서 벡터화를 진행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Oq9Yq4hA1i"
      },
      "source": [
        "# spacy tokenizer 함수\n",
        "def tokenize(document):\n",
        "    \n",
        "    doc = nlp(document)\n",
        "    # punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.is_alpha == True)]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOsy1gVhA1i"
      },
      "source": [
        "파라미터 튜닝을 더 해보겠습니다. 여러 파라미터들을 변경해 가며 결과를 비교해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa8za2l8hA1i",
        "outputId": "e85dc98b-51fa-4950-9361-78903c8b50d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "# ngram_range = (min_n, max_n), min_n 개~ max_n 개를 갖는 n-gram(n개의 연속적인 토큰)을 토큰으로 사용합니다.\n",
        "# min_df = n, 최소 n개의 문서에 나타나는 토큰만 사용합니다\n",
        "# max_df = .7, 70% 이상 문서에 나타나는 토큰은 제거합니다\n",
        "tfidf = TfidfVectorizer(stop_words='english'\n",
        "                        ,tokenizer=tokenize\n",
        "                        ,ngram_range=(1,2)\n",
        "                        ,max_df=.7\n",
        "                        ,min_df=3\n",
        "#                         ,max_features = 4000\n",
        "                       )\n",
        "\n",
        "dtm = tfidf.fit_transform(data)\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-PRON-</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability record</th>\n",
              "      <th>able</th>\n",
              "      <th>able access</th>\n",
              "      <th>able choose</th>\n",
              "      <th>able control</th>\n",
              "      <th>able handle</th>\n",
              "      <th>able offer</th>\n",
              "      <th>able play</th>\n",
              "      <th>able store</th>\n",
              "      <th>able thing</th>\n",
              "      <th>able use</th>\n",
              "      <th>able watch</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>abuse</th>\n",
              "      <th>academic</th>\n",
              "      <th>academy</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>access</th>\n",
              "      <th>access available</th>\n",
              "      <th>access datum</th>\n",
              "      <th>access device</th>\n",
              "      <th>access e</th>\n",
              "      <th>access grow</th>\n",
              "      <th>access home</th>\n",
              "      <th>access information</th>\n",
              "      <th>access internet</th>\n",
              "      <th>access medium</th>\n",
              "      <th>access mobile</th>\n",
              "      <th>access net</th>\n",
              "      <th>access point</th>\n",
              "      <th>access service</th>\n",
              "      <th>accessible</th>\n",
              "      <th>accident</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>...</th>\n",
              "      <th>year launch</th>\n",
              "      <th>year microsoft</th>\n",
              "      <th>year million</th>\n",
              "      <th>year motorola</th>\n",
              "      <th>year new</th>\n",
              "      <th>year number</th>\n",
              "      <th>year old</th>\n",
              "      <th>year people</th>\n",
              "      <th>year portable</th>\n",
              "      <th>year predict</th>\n",
              "      <th>year real</th>\n",
              "      <th>year release</th>\n",
              "      <th>year report</th>\n",
              "      <th>year say</th>\n",
              "      <th>year service</th>\n",
              "      <th>year think</th>\n",
              "      <th>year time</th>\n",
              "      <th>year uk</th>\n",
              "      <th>year use</th>\n",
              "      <th>year year</th>\n",
              "      <th>yen</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>york</th>\n",
              "      <th>york base</th>\n",
              "      <th>york state</th>\n",
              "      <th>york times</th>\n",
              "      <th>young</th>\n",
              "      <th>young americans</th>\n",
              "      <th>young people</th>\n",
              "      <th>young user</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zen micro</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie bot</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7604 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -PRON-  abandon   ability  ability record  ...  zombie  zombie bot  zone  zoom\n",
              "0     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "1     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "2     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "3     0.0      0.0  0.024268             0.0  ...     0.0         0.0   0.0   0.0\n",
              "4     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 7604 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2fjGZhRhA1i",
        "outputId": "b72c1924-b80b-4e5d-dde3-334c1c34e184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dtm.shape"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 7604)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76FurcFFhA1j"
      },
      "source": [
        "## 유사도를 이용해 문서를 검색해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyZVc10EhA1j"
      },
      "source": [
        "네이버, 구글과 같은 검색엔진의 원리는 무엇입니까? \n",
        "<br> 검색어를 인터넷에 존재하는 여러 문서들과 단순히 같은지만 비교 하는것은 아닙니다. 쿼리와 문서들을 매칭(matching) 하는 방법은 여러가지가 있습니다. 그중 가장 클래식한 방법인 \"유사도 측정 방법\"을 시도해 봅니다. \n",
        "<br> 이를 위해 n-차원 거리를 사용하는 방법을 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "przZffV_hA1j"
      },
      "source": [
        "### 코사인 유사도(Cosine Similarity, Brute Force 방법)\n",
        "\n",
        "$\\Large similarity=cos(Θ)=\\frac{A⋅B}{||A||\\ ||B||}=\\frac{\\sum_{i=1}^{n}{A_{i}×B_{i}}}{\\sqrt{\\sum_{i=1}^{n}(A_{i})^2}×\\sqrt{\\sum_{i=1}^{n}(B_{i})^2}}$\n",
        "\n",
        "<img align=\"center\" src=\"https://images.deepai.org/glossary-terms/cosine-similarity-1007790.jpg\" width=700 title=\"Cosine Similarity\" alt=\"https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity\">\n",
        "\n",
        "\n",
        "코사인 유사도는 두 벡터(문서벡터) 간의 각의 코사인 값을 이용하여 구할 수 있는 유사도 입니다.\n",
        "- 두 벡터(문서)가 \n",
        "    - 완전히 같을 경우 1이며\n",
        "    - 90도의 각을 이루면 0\n",
        "    - 완전히 반대방향을 이루면 -1 입니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nruxP2slhA1j"
      },
      "source": [
        "#### TF-IDF 벡터들의 거리를 계산해 보겠습니다\n",
        "[sklearn.metrics.pairwise.cosine_similarity](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn-metrics-pairwise-cosine-similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6Jm7UfhA1j"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# input, X:(n_samples_X, n_features)\n",
        "distance_matrix  = cosine_similarity(dtm)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9c8KIXUhA1k"
      },
      "source": [
        "df = pd.DataFrame(distance_matrix)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m_so22thA1k"
      },
      "source": [
        "유사도는 문서 x 문서 행렬로 표현됩니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNP1zFs2hA1k",
        "outputId": "3f6c1104-3c34-4dc7-894a-c78dc8cf5ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(401, 401)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMii6L8AhA1k",
        "outputId": "26a2cf07-f90b-4c1c-8ff0-f7b82c5ffdfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "      <th>372</th>\n",
              "      <th>373</th>\n",
              "      <th>374</th>\n",
              "      <th>375</th>\n",
              "      <th>376</th>\n",
              "      <th>377</th>\n",
              "      <th>378</th>\n",
              "      <th>379</th>\n",
              "      <th>380</th>\n",
              "      <th>381</th>\n",
              "      <th>382</th>\n",
              "      <th>383</th>\n",
              "      <th>384</th>\n",
              "      <th>385</th>\n",
              "      <th>386</th>\n",
              "      <th>387</th>\n",
              "      <th>388</th>\n",
              "      <th>389</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "      <th>400</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.014998</td>\n",
              "      <td>0.041514</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.021492</td>\n",
              "      <td>0.008861</td>\n",
              "      <td>0.019746</td>\n",
              "      <td>0.308156</td>\n",
              "      <td>0.019956</td>\n",
              "      <td>0.027771</td>\n",
              "      <td>0.020529</td>\n",
              "      <td>0.007588</td>\n",
              "      <td>0.045374</td>\n",
              "      <td>0.019767</td>\n",
              "      <td>0.047641</td>\n",
              "      <td>0.017975</td>\n",
              "      <td>0.019942</td>\n",
              "      <td>0.018530</td>\n",
              "      <td>0.034216</td>\n",
              "      <td>0.015849</td>\n",
              "      <td>0.055570</td>\n",
              "      <td>0.021976</td>\n",
              "      <td>0.035695</td>\n",
              "      <td>0.024257</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.009431</td>\n",
              "      <td>0.028583</td>\n",
              "      <td>0.033699</td>\n",
              "      <td>0.031327</td>\n",
              "      <td>0.041125</td>\n",
              "      <td>0.023959</td>\n",
              "      <td>0.346372</td>\n",
              "      <td>0.097859</td>\n",
              "      <td>0.012726</td>\n",
              "      <td>0.020045</td>\n",
              "      <td>0.014379</td>\n",
              "      <td>0.030776</td>\n",
              "      <td>0.372439</td>\n",
              "      <td>0.210640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050091</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.007387</td>\n",
              "      <td>0.291799</td>\n",
              "      <td>0.014685</td>\n",
              "      <td>0.031490</td>\n",
              "      <td>0.021084</td>\n",
              "      <td>0.118558</td>\n",
              "      <td>0.064996</td>\n",
              "      <td>0.021972</td>\n",
              "      <td>0.021492</td>\n",
              "      <td>0.080722</td>\n",
              "      <td>0.024713</td>\n",
              "      <td>0.024085</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.018448</td>\n",
              "      <td>0.634388</td>\n",
              "      <td>0.030939</td>\n",
              "      <td>0.017447</td>\n",
              "      <td>0.014625</td>\n",
              "      <td>0.021464</td>\n",
              "      <td>0.038563</td>\n",
              "      <td>0.008291</td>\n",
              "      <td>0.052448</td>\n",
              "      <td>0.041029</td>\n",
              "      <td>0.026842</td>\n",
              "      <td>0.025648</td>\n",
              "      <td>0.005956</td>\n",
              "      <td>0.040467</td>\n",
              "      <td>0.034726</td>\n",
              "      <td>0.162121</td>\n",
              "      <td>0.229296</td>\n",
              "      <td>0.011844</td>\n",
              "      <td>0.018227</td>\n",
              "      <td>0.028107</td>\n",
              "      <td>0.011830</td>\n",
              "      <td>0.019738</td>\n",
              "      <td>0.013596</td>\n",
              "      <td>0.009184</td>\n",
              "      <td>0.052448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.014998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.024811</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.029222</td>\n",
              "      <td>0.221671</td>\n",
              "      <td>0.031656</td>\n",
              "      <td>0.036121</td>\n",
              "      <td>0.025296</td>\n",
              "      <td>0.043338</td>\n",
              "      <td>0.028627</td>\n",
              "      <td>0.040830</td>\n",
              "      <td>0.039587</td>\n",
              "      <td>0.039457</td>\n",
              "      <td>0.041103</td>\n",
              "      <td>0.022156</td>\n",
              "      <td>0.024584</td>\n",
              "      <td>0.036916</td>\n",
              "      <td>0.018690</td>\n",
              "      <td>0.085091</td>\n",
              "      <td>0.012136</td>\n",
              "      <td>0.016791</td>\n",
              "      <td>0.054708</td>\n",
              "      <td>0.042296</td>\n",
              "      <td>0.028373</td>\n",
              "      <td>0.190720</td>\n",
              "      <td>0.112254</td>\n",
              "      <td>0.009806</td>\n",
              "      <td>0.014573</td>\n",
              "      <td>0.021385</td>\n",
              "      <td>0.040554</td>\n",
              "      <td>0.054340</td>\n",
              "      <td>0.017043</td>\n",
              "      <td>0.037784</td>\n",
              "      <td>0.033729</td>\n",
              "      <td>0.032849</td>\n",
              "      <td>0.040577</td>\n",
              "      <td>0.077081</td>\n",
              "      <td>0.032485</td>\n",
              "      <td>0.028017</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.066005</td>\n",
              "      <td>0.054850</td>\n",
              "      <td>0.017615</td>\n",
              "      <td>0.036496</td>\n",
              "      <td>0.023888</td>\n",
              "      <td>0.055365</td>\n",
              "      <td>0.031680</td>\n",
              "      <td>0.057967</td>\n",
              "      <td>0.221671</td>\n",
              "      <td>0.068739</td>\n",
              "      <td>0.078008</td>\n",
              "      <td>0.025764</td>\n",
              "      <td>0.077448</td>\n",
              "      <td>0.037202</td>\n",
              "      <td>0.025191</td>\n",
              "      <td>0.020430</td>\n",
              "      <td>0.050165</td>\n",
              "      <td>0.049785</td>\n",
              "      <td>0.044212</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.032681</td>\n",
              "      <td>0.022416</td>\n",
              "      <td>0.024362</td>\n",
              "      <td>0.037098</td>\n",
              "      <td>0.017768</td>\n",
              "      <td>0.038760</td>\n",
              "      <td>0.022998</td>\n",
              "      <td>0.019019</td>\n",
              "      <td>0.015845</td>\n",
              "      <td>0.012626</td>\n",
              "      <td>0.022334</td>\n",
              "      <td>0.019251</td>\n",
              "      <td>0.042788</td>\n",
              "      <td>0.016940</td>\n",
              "      <td>0.052463</td>\n",
              "      <td>0.032662</td>\n",
              "      <td>0.037820</td>\n",
              "      <td>0.022416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.041514</td>\n",
              "      <td>0.024811</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>0.102332</td>\n",
              "      <td>0.030127</td>\n",
              "      <td>0.022638</td>\n",
              "      <td>0.019852</td>\n",
              "      <td>0.040847</td>\n",
              "      <td>0.024582</td>\n",
              "      <td>0.021479</td>\n",
              "      <td>0.012252</td>\n",
              "      <td>0.020282</td>\n",
              "      <td>0.044144</td>\n",
              "      <td>0.017505</td>\n",
              "      <td>0.032934</td>\n",
              "      <td>0.010031</td>\n",
              "      <td>0.034399</td>\n",
              "      <td>0.056063</td>\n",
              "      <td>0.050406</td>\n",
              "      <td>0.009653</td>\n",
              "      <td>0.013851</td>\n",
              "      <td>0.053768</td>\n",
              "      <td>0.027988</td>\n",
              "      <td>0.043993</td>\n",
              "      <td>0.039948</td>\n",
              "      <td>0.026018</td>\n",
              "      <td>0.010103</td>\n",
              "      <td>0.016034</td>\n",
              "      <td>0.027391</td>\n",
              "      <td>0.305195</td>\n",
              "      <td>0.009918</td>\n",
              "      <td>0.035028</td>\n",
              "      <td>0.036705</td>\n",
              "      <td>0.014177</td>\n",
              "      <td>0.027256</td>\n",
              "      <td>0.047349</td>\n",
              "      <td>0.033202</td>\n",
              "      <td>0.042810</td>\n",
              "      <td>0.051493</td>\n",
              "      <td>...</td>\n",
              "      <td>0.056372</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>0.022204</td>\n",
              "      <td>0.010895</td>\n",
              "      <td>0.039594</td>\n",
              "      <td>0.026958</td>\n",
              "      <td>0.025563</td>\n",
              "      <td>0.049380</td>\n",
              "      <td>0.068558</td>\n",
              "      <td>0.030127</td>\n",
              "      <td>0.063957</td>\n",
              "      <td>0.027046</td>\n",
              "      <td>0.130258</td>\n",
              "      <td>0.008647</td>\n",
              "      <td>0.086470</td>\n",
              "      <td>0.038239</td>\n",
              "      <td>0.018313</td>\n",
              "      <td>0.030108</td>\n",
              "      <td>0.038741</td>\n",
              "      <td>0.009442</td>\n",
              "      <td>0.015001</td>\n",
              "      <td>0.018815</td>\n",
              "      <td>0.025128</td>\n",
              "      <td>0.037379</td>\n",
              "      <td>0.039582</td>\n",
              "      <td>0.014557</td>\n",
              "      <td>0.018246</td>\n",
              "      <td>0.024711</td>\n",
              "      <td>0.094204</td>\n",
              "      <td>0.006628</td>\n",
              "      <td>0.040698</td>\n",
              "      <td>0.008616</td>\n",
              "      <td>0.019255</td>\n",
              "      <td>0.021837</td>\n",
              "      <td>0.097403</td>\n",
              "      <td>0.033770</td>\n",
              "      <td>0.058408</td>\n",
              "      <td>0.055404</td>\n",
              "      <td>0.025128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.026519</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>0.035101</td>\n",
              "      <td>0.024513</td>\n",
              "      <td>0.044580</td>\n",
              "      <td>0.056627</td>\n",
              "      <td>0.041961</td>\n",
              "      <td>0.027201</td>\n",
              "      <td>0.028365</td>\n",
              "      <td>0.025973</td>\n",
              "      <td>0.047236</td>\n",
              "      <td>0.035606</td>\n",
              "      <td>0.036020</td>\n",
              "      <td>0.028055</td>\n",
              "      <td>0.027350</td>\n",
              "      <td>0.012107</td>\n",
              "      <td>0.041708</td>\n",
              "      <td>0.130271</td>\n",
              "      <td>0.036489</td>\n",
              "      <td>0.045174</td>\n",
              "      <td>0.073751</td>\n",
              "      <td>0.070594</td>\n",
              "      <td>0.056464</td>\n",
              "      <td>0.048537</td>\n",
              "      <td>0.026928</td>\n",
              "      <td>0.010123</td>\n",
              "      <td>0.017588</td>\n",
              "      <td>0.074734</td>\n",
              "      <td>0.022392</td>\n",
              "      <td>0.014626</td>\n",
              "      <td>0.085479</td>\n",
              "      <td>0.023353</td>\n",
              "      <td>0.036605</td>\n",
              "      <td>0.043523</td>\n",
              "      <td>0.071219</td>\n",
              "      <td>0.054669</td>\n",
              "      <td>0.030636</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088916</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.024633</td>\n",
              "      <td>0.064317</td>\n",
              "      <td>0.026145</td>\n",
              "      <td>0.080492</td>\n",
              "      <td>0.032622</td>\n",
              "      <td>0.061752</td>\n",
              "      <td>0.078756</td>\n",
              "      <td>0.058491</td>\n",
              "      <td>0.035101</td>\n",
              "      <td>0.030452</td>\n",
              "      <td>0.054023</td>\n",
              "      <td>0.023876</td>\n",
              "      <td>0.034721</td>\n",
              "      <td>0.037571</td>\n",
              "      <td>0.021993</td>\n",
              "      <td>0.419053</td>\n",
              "      <td>0.039765</td>\n",
              "      <td>0.033855</td>\n",
              "      <td>0.049457</td>\n",
              "      <td>0.096033</td>\n",
              "      <td>0.011827</td>\n",
              "      <td>0.044153</td>\n",
              "      <td>0.028183</td>\n",
              "      <td>0.046056</td>\n",
              "      <td>0.029091</td>\n",
              "      <td>0.019028</td>\n",
              "      <td>0.288912</td>\n",
              "      <td>0.023389</td>\n",
              "      <td>0.054317</td>\n",
              "      <td>0.050017</td>\n",
              "      <td>0.029934</td>\n",
              "      <td>0.021308</td>\n",
              "      <td>0.052214</td>\n",
              "      <td>0.017941</td>\n",
              "      <td>0.028565</td>\n",
              "      <td>0.032798</td>\n",
              "      <td>0.035675</td>\n",
              "      <td>0.044153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.011299</td>\n",
              "      <td>0.029222</td>\n",
              "      <td>0.102332</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.015927</td>\n",
              "      <td>0.021267</td>\n",
              "      <td>0.042578</td>\n",
              "      <td>0.017994</td>\n",
              "      <td>0.037723</td>\n",
              "      <td>0.019082</td>\n",
              "      <td>0.037993</td>\n",
              "      <td>0.024628</td>\n",
              "      <td>0.025644</td>\n",
              "      <td>0.026537</td>\n",
              "      <td>0.044846</td>\n",
              "      <td>0.018199</td>\n",
              "      <td>0.040985</td>\n",
              "      <td>0.214319</td>\n",
              "      <td>0.043961</td>\n",
              "      <td>0.029113</td>\n",
              "      <td>0.004304</td>\n",
              "      <td>0.070805</td>\n",
              "      <td>0.046170</td>\n",
              "      <td>0.013404</td>\n",
              "      <td>0.030189</td>\n",
              "      <td>0.035723</td>\n",
              "      <td>0.034755</td>\n",
              "      <td>0.040293</td>\n",
              "      <td>0.025615</td>\n",
              "      <td>0.023515</td>\n",
              "      <td>0.022184</td>\n",
              "      <td>0.008476</td>\n",
              "      <td>0.032747</td>\n",
              "      <td>0.014660</td>\n",
              "      <td>0.014011</td>\n",
              "      <td>0.183162</td>\n",
              "      <td>0.079023</td>\n",
              "      <td>0.075975</td>\n",
              "      <td>0.020922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046772</td>\n",
              "      <td>0.033863</td>\n",
              "      <td>0.022562</td>\n",
              "      <td>0.041920</td>\n",
              "      <td>0.017766</td>\n",
              "      <td>0.023313</td>\n",
              "      <td>0.013541</td>\n",
              "      <td>0.068659</td>\n",
              "      <td>0.022806</td>\n",
              "      <td>0.235065</td>\n",
              "      <td>0.015927</td>\n",
              "      <td>0.215408</td>\n",
              "      <td>0.066968</td>\n",
              "      <td>0.367712</td>\n",
              "      <td>0.011235</td>\n",
              "      <td>0.040682</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.042264</td>\n",
              "      <td>0.067912</td>\n",
              "      <td>0.051215</td>\n",
              "      <td>0.055502</td>\n",
              "      <td>0.032859</td>\n",
              "      <td>0.024812</td>\n",
              "      <td>0.010446</td>\n",
              "      <td>0.098037</td>\n",
              "      <td>0.100497</td>\n",
              "      <td>0.051738</td>\n",
              "      <td>0.036786</td>\n",
              "      <td>0.026930</td>\n",
              "      <td>0.186684</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>0.010800</td>\n",
              "      <td>0.018684</td>\n",
              "      <td>0.021447</td>\n",
              "      <td>0.054540</td>\n",
              "      <td>0.327685</td>\n",
              "      <td>0.040275</td>\n",
              "      <td>0.174066</td>\n",
              "      <td>0.245809</td>\n",
              "      <td>0.010446</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       398       399       400\n",
              "0  1.000000  0.014998  0.041514  ...  0.013596  0.009184  0.052448\n",
              "1  0.014998  1.000000  0.024811  ...  0.032662  0.037820  0.022416\n",
              "2  0.041514  0.024811  1.000000  ...  0.058408  0.055404  0.025128\n",
              "3  0.035916  0.036935  0.026519  ...  0.032798  0.035675  0.044153\n",
              "4  0.011299  0.029222  0.102332  ...  0.174066  0.245809  0.010446\n",
              "\n",
              "[5 rows x 401 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SWlGndPhA1k"
      },
      "source": [
        "#### 문서0과 문서0은 같으므로 유사도가 1입니다. 문서0과 문서1~4 와의의 유사도를 확인해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnyM8LnehA1k",
        "outputId": "8e1bdaaf-7f7f-4f5a-f936-9787ebc50a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df[0][:5]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1.000000\n",
              "1    0.014998\n",
              "2    0.041514\n",
              "3    0.035916\n",
              "4    0.011299\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7x3f9d1hA1l"
      },
      "source": [
        "#### 문서0과 유사도가 큰 문서를 순서대로 정렬해서 살펴보겠습니다 (TF-IDF vectorization 방법에 따라 많이 다를 수 있습니다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2EiAUChA1l",
        "outputId": "00cba837-067c-434d-ba41-95a28012676e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ind = df[df[0] < 1][0].sort_values(ascending=False)[:5]\n",
        "ind"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "377    0.634388\n",
              "301    0.634388\n",
              "222    0.595379\n",
              "38     0.372439\n",
              "32     0.346372\n",
              "Name: 0, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh0LsGb4dwgZ"
      },
      "source": [
        "index = 377"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUk_QuWNhA1l",
        "outputId": "62eb7fc4-b698-44e7-ed22-687d2f5e7407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data[0][:100])\n",
        "print(data[index][:100])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Joke e-mail virus tricks users\\n\\nA virus that disguises itself as a joke is spreading rapidly across '\n",
            "b'Virus poses as Christmas e-mail\\n\\nSecurity firms are warning about a Windows virus disguising itself '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XI2RDt4hA1l"
      },
      "source": [
        "코사인 유사도와 같은 Brute Force 방법은 비교해야 할 문서의 양이 많아 질 수록 많은 계산을 필요로합니다. <br> 실제 어플리케이션 환경에서는 더 빠른 비교 방법을 사용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmTXnGIChA1l"
      },
      "source": [
        "### NearestNeighbor (K-NN, K-최근접 이웃) \n",
        "\n",
        "K-최근접 이웃법은 쿼리와 가장 가까운 상위 K개의 근접한 데이터를 찾아서 K개 데이터의 유사성을 기반으로 **점을 추정하거나 분류**하는 예측 분석에 사용됩니다.\n",
        "최근접 이웃 방법은 non-generalizing 머신러닝 방법인데, 모든 학습 데이터를 KD Tree 나 Ball Tree같은 빠른 색인 구조(indexing structure)에 단순히 저장하기 때문입니다.\n",
        "\n",
        "[K-D Tree](https://scikit-learn.org/stable/modules/neighbors.html?highlight=very%20distant%20from%20point#k-d-tree)의 기본아이디어는 점 A와 B가 멀고, B가 C와 가까우면 A가 C와 멀다는 것을 알 수 있는데 이때 명시적으로 A와 C의 거리를 계산할 필요가 없다는 것입니다.\n",
        "\n",
        "<img src=\"https://i.imgur.com/CKKoz5W.png\" width=\"500\"/>\n",
        "\n",
        "[Ball Tree](https://scikit-learn.org/stable/modules/neighbors.html?highlight=very%20distant%20from%20point#ball-tree)는 K-D 트리를 더욱 효율적으로 만들기 위해 개발되었습니다. KD 트리는 데이터를 Cartesian 축으로 분할하지만 Ball Tree는 nesting hyper-spheres 형태로 분할하여 트리구성에 비용이 더 들지만, 매우 구조화된 데이터나 높은 차원의 데이터에 더 효율적입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhso8V_WhA1l",
        "outputId": "6d0fe15f-2778-41f0-842a-68e0142dce50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "dtm.head()"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-PRON-</th>\n",
              "      <th>abandon</th>\n",
              "      <th>ability</th>\n",
              "      <th>ability record</th>\n",
              "      <th>able</th>\n",
              "      <th>able access</th>\n",
              "      <th>able choose</th>\n",
              "      <th>able control</th>\n",
              "      <th>able handle</th>\n",
              "      <th>able offer</th>\n",
              "      <th>able play</th>\n",
              "      <th>able store</th>\n",
              "      <th>able thing</th>\n",
              "      <th>able use</th>\n",
              "      <th>able watch</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absorb</th>\n",
              "      <th>abuse</th>\n",
              "      <th>academic</th>\n",
              "      <th>academy</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>acceptance</th>\n",
              "      <th>access</th>\n",
              "      <th>access available</th>\n",
              "      <th>access datum</th>\n",
              "      <th>access device</th>\n",
              "      <th>access e</th>\n",
              "      <th>access grow</th>\n",
              "      <th>access home</th>\n",
              "      <th>access information</th>\n",
              "      <th>access internet</th>\n",
              "      <th>access medium</th>\n",
              "      <th>access mobile</th>\n",
              "      <th>access net</th>\n",
              "      <th>access point</th>\n",
              "      <th>access service</th>\n",
              "      <th>accessible</th>\n",
              "      <th>accident</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>...</th>\n",
              "      <th>year launch</th>\n",
              "      <th>year microsoft</th>\n",
              "      <th>year million</th>\n",
              "      <th>year motorola</th>\n",
              "      <th>year new</th>\n",
              "      <th>year number</th>\n",
              "      <th>year old</th>\n",
              "      <th>year people</th>\n",
              "      <th>year portable</th>\n",
              "      <th>year predict</th>\n",
              "      <th>year real</th>\n",
              "      <th>year release</th>\n",
              "      <th>year report</th>\n",
              "      <th>year say</th>\n",
              "      <th>year service</th>\n",
              "      <th>year think</th>\n",
              "      <th>year time</th>\n",
              "      <th>year uk</th>\n",
              "      <th>year use</th>\n",
              "      <th>year year</th>\n",
              "      <th>yen</th>\n",
              "      <th>yepp</th>\n",
              "      <th>yes</th>\n",
              "      <th>york</th>\n",
              "      <th>york base</th>\n",
              "      <th>york state</th>\n",
              "      <th>york times</th>\n",
              "      <th>young</th>\n",
              "      <th>young americans</th>\n",
              "      <th>young people</th>\n",
              "      <th>young user</th>\n",
              "      <th>youngster</th>\n",
              "      <th>youth</th>\n",
              "      <th>zelda</th>\n",
              "      <th>zen</th>\n",
              "      <th>zen micro</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombie bot</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.068952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.024268</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.069914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.052588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.049556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7604 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   -PRON-  abandon   ability  ability record  ...  zombie  zombie bot  zone  zoom\n",
              "0     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "1     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "2     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "3     0.0      0.0  0.024268             0.0  ...     0.0         0.0   0.0   0.0\n",
              "4     0.0      0.0  0.000000             0.0  ...     0.0         0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 7604 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBToNaAhhA1m"
      },
      "source": [
        "#### sklearn에서 비지도학습을 위한 NearestNeighbors 모델을 사용합니다\n",
        "[sklearn.neighbors.NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn-neighbors-nearestneighbors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7z2v2aHhA1m",
        "outputId": "0817e9a1-a9b3-483b-c4c0-1f44638f7a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# dtm을 사용히 NN 모델을 학습시킵니다. (디폴트)최근접 5 이웃.\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrmaUZU5hA1m"
      },
      "source": [
        "#### 문서0과 가장 가까운 문서 (0 포함) 5개의 거리(값이 작을수록 유사합니다)와, 문서의 인덱스를 알 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8VirpVdhA1m",
        "outputId": "ed34f898-b14b-4667-8a7f-02dab9940626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn.kneighbors([dtm.iloc[0].values])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.85511637, 0.85511637, 0.89957864, 1.12032242]]),\n",
              " array([[  0, 377, 301, 222,  38]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOAsaBRhA1m"
      },
      "source": [
        "#### 문서0의 이웃인 문서377로 검색해 보겠습니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mQaJkM4hA1m",
        "outputId": "b35b8a44-f57b-4a2d-be62-85b3c7581d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn.kneighbors([dtm.iloc[377]])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.        , 0.        , 0.85511637, 0.91629024, 1.10942847]]),\n",
              " array([[377, 301,   0, 222,  38]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uUn6P5OhA1m",
        "outputId": "1e777f87-9018-47e9-9634-aa9e77b0194b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data[222][:300])\n",
        "print(data[301][:300])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Toxic web links help virus spread\\n\\nVirus writers have begun using the power of the web to spread their malicious wares.\\n\\nA Windows virus called Bofra is turning infected machines into distributors of its malicious code. Those clicking on the poisoned links in e-mail messages sent out by infected mac'\n",
            "b'Virus poses as Christmas e-mail\\n\\nSecurity firms are warning about a Windows virus disguising itself as an electronic Christmas card.\\n\\nThe Zafi.D virus translates the Christmas greeting on its subject line into the language of the person receiving infected e-mail. Anti-virus firms speculate that this'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ7KPCgJhA1n"
      },
      "source": [
        "CNN 에서 tech 기사를 가져왔습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh7h7IlhA1n"
      },
      "source": [
        "# https://edition.cnn.com/2020/07/30/tech/huawei-samsung-q2-hnk-intl/index.html\n",
        "cnn_tech_article = [ \"\"\"\n",
        "Hong Kong (CNN Business)Huawei became the world's top smartphone seller last quarter, overtaking Samsung for the first time ever, according to an independent market research report released Thursday.\n",
        "The Chinese tech company shipped 55.8 million phones in the three months ended in June, surpassing longtime rival Samsung, which shipped 53.7 million, according to the Canalys report.\n",
        "\"Taking first place is very important for Huawei,\" said Canalys analyst Mo Jia. \"It is desperate to showcase its brand strength to domestic consumers, component suppliers and developers.\"\n",
        "A years-long US pressure campaign against Huawei has handicapped the Shenzhen-based firm's global business.\n",
        "Huawei still suffered an annual decline in smartphone shipments of 5%. But Samsung's was a lot bigger at 30%, according to Canalys.\n",
        "The market research firm said Huawei's victory over Samsung wouldn't have happened without Covid-19. The company was able to take advantage of the economic recovery in China, where Huawei now sells over 70% of its smartphones. Samsung has a very small presence in China.\n",
        "Huawei&#39;s hopes of global domination have been dashed\n",
        "Huawei's hopes of global domination have been dashed\n",
        "Huawei's global smartphone and telecom gear business continues to suffer the fallout from US sanctions that cut the company off from key American tech and supplies.\n",
        "Without access to popular Google (GOOGL GOOGLE) apps such as YouTube, maps and Gmail, Huawei's latest smartphones are a lot less attractive to international buyers. That will make it very difficult for Huawei to hold on to the global No. 1 position, according to Jia.\n",
        "\"It will be hard for Huawei to maintain its lead in the long term. Its major channel partners in key regions, such as Europe, are increasingly wary of ranging Huawei devices, taking on fewer models, and bringing in new brands to reduce risk. Strength in China alone will not be enough to sustain Huawei at the top once the global economy starts to recover,\" he said.\n",
        "\"Our business has demonstrated exceptional resilience in these difficult times,\" Huawei spokeswoman Evita Cao said. Cao did not respond to questions on how the company can maintain its lead going forward.\n",
        "Huawei's victory came on the same day Samsung posted a big profit bump for the second quarter, with strong chip demand helping the company weather the fallout from the coronavirus pandemic.\n",
        "Samsung reported operating profit of 8.15 trillion won ($6.8 billion) for the three months that ended in June, up more than 23% compared to the same period last year.\n",
        "Samsung said sales fell about 6% to 53 trillion won ($44.6 billion).\n",
        "Shares in Samsung were last up 0.7% in Seoul. South Korea's Kospi (KOSPI) rose 0.1%.\n",
        "Taiwan&#39;s TSMC is becoming one of the world&#39;s top companies. Intel&#39;s problems are helping\n",
        "Taiwan's TSMC is becoming one of the world's top companies. Intel's problems are helping\n",
        "Despite the double digit declines in annual smartphone shipments for the quarter noted by the Canalys report, Samsung reported that the unit remained profitable thanks to savings on marketing costs. (Samsung does not break out specifics about its smartphone shipments, but noted that they declined.)\n",
        "For the second half of 2020, however, Samsung is warning that \"uncertainties related to Covid-19 linger\" for its mobile business.\n",
        "That could be enough to drag the company to revenue losses for the year, according to research firm Crisp Idea.\n",
        "The consumer electronics unit, which includes smartphones and TVs, is \"expected to decline significantly as Covid-19 affects demand and leads to store and plant closures globally,\" Crisp Idea analysts wrote in a note earlier this month.\n",
        "Smartphone shipments worldwide are expected to fall about 18% in the first half of the year as the pandemic continues to affect consumer spending, analysts at IDC said in a note last month.\n",
        "The market research firm added that global smartphone shipments are not expected to return to growth until the first quarter of 2021.\n",
        "That would also hurt Samsung's memory chip business, because the company supplies chips for rival smartphone companies such as Apple (AAPL) and Huawei.\"\"\"]\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vOr8kVjhA1n"
      },
      "source": [
        "CNN Tech 뉴스를 쿼리로 쓰기 위해 학습된 tfidf vectorizer를 통해 변환하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsCn9IkPhA1n"
      },
      "source": [
        "new = tfidf.transform(cnn_tech_article)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pVXr1YzhA1n",
        "outputId": "485e7b90-b601-4f83-bd01-c0a7b4aa128e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.31624638, 1.3174223 , 1.31770469, 1.31778057, 1.31848904]]),\n",
              " array([[297, 218,  56, 374, 232]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iSQ0s5-hA1n",
        "outputId": "dac271f7-625c-4bcf-ea4f-2731d78dca2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 가장 가깝게 나온 문서를 확인합니다 \n",
        "data[297]"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Gadget growth fuels eco concerns\\n\\nTechnology firms and gadget lovers are being urged to think more about the environment when buying and disposing of the latest hi-tech products.\\n\\nAt the Consumer Electronics Show in Las Vegas earlier this month, several hi-tech firms were recognised for their strategies to help the environment. Ebay also announced the Rethink project bringing together Intel, Apple, and IBM among others to promote recycling. The US consumer electronics market is set to grow by over 11% in 2005. But more awareness is needed about how and where old gadgets can be recycled as well as how to be more energy efficient, said the US Environmental Protection Agency (EPA). Of particular growing concern is how much energy it takes to recharge portable devices, one of the fastest growing markets in technology. The Consumer Electronics Association (CEA) has predicted that shipments of consumer technologies in 2005 will reach more than $125.73 billion (nearly \\xc2\\xa368 billion).\\n\\nEbay\\'s initiative pulls together major technology firms, environment groups, government agencies and eBay users to give information about what to do with old computers and where to send them. The online auction house thinks that its already-established community of loyal users could be influential. \"We really became aware of the e-waste issue and we saw that our 125 million users can be a powerful force for good,\" eBay\\'s David Stern told the BBC News website.\\n\\n\"We saw the opportunity to meet the additional demand we have on the site for used computers and saw the opportunity too to good some good for the environment.\" But it is not just computers that cause a problem for the environment. Teenagers get a new mobile every 11 months, adults every 18 months and a 15 million handsets are replaced in total each year. Yet, only 15% are actually recycled. This year, a predicted two billion people worldwide will own a mobile, according to a Deloitte report. Schemes in the US, like RIPMobile, could help in targeting younger generations with recycling messages. The initiative, which was also launched at CES, rewards 10 to 28-year-olds for returning unused phones. \"This system allows for the transformation of a drawer full of unused mobile phones into anything from music to clothes to electronics or games,\" said Seth Heine from RIPMobile.\\n\\nOne group of students collected 1,000 mobiles for recycling in just three months. Mr Heine told the BBC News website that what was important was to raise awareness amongst the young so that recycling becomes \"learned behaviour\". Europe is undoubtedly more advanced than the US in terms of recycling awareness and robust \"end of life\" programmes, although there is a tide change happening in the rest of the world too. Intel showcased some its motherboards and chips at CES which are entirely lead free.\\n\\n\"There is more and more awareness on the consumer side, but the whole industry is moving towards being lead free,\" Intel\\'s Allen Wilson told the BBC News website. \"There is still low-level awareness right now, but it is on the rise - the highest level of awareness is in Europe.\" A European Union (EU) directive, WEEE (Waste Electronic and Electrical Equipment), comes into effect in August. It puts the responsibility on electrical manufacturers to recycle items that are returned to them. But developments are also being made to design better technologies which are more energy efficient and which do not contain harmful substances. Elements like chromium, lead, and cadmium - common in consumer electronics goods - will be prohibited in all products in the EU by 2006.\\n\\nBut it is not just about recycling either. The predicted huge growth in the gadget market means the amount of energy used to power them up is on the rise too. The biggest culprit, according to the EPA, is the innocuous power adaptor, nicknamed \"energy vampires\". They provide vital juice for billions of mobile phones, PDAs (personal digital assistants), digital cameras, camcorders, and digital music players.\\n\\nAlthough there is a focus on developing efficient and improved circuits in the devices themselves, the technologies inside rechargers are still outdated and so eat up more energy than is needed to power a gadget. On 1 January, new efficiency standards for external power supplies came into effect as part of the European Commission Code of Conduct. But at CES, the EPA also unveiled new guidelines for its latest Energy Star initiative which targets external power adapters. These map out the framework for developing better adaptors that can be labelled with an Energy Star logo, meaning they are about 35% more efficient. The initiative is a global effort and more manufacturers\\' adaptors are being brought on board. Most are made in China. About two billion are shipped global every year, and about three billion are in use in the US alone. The EPA is already working with several companies which make more than 22% of power supplies on the market. \"We are increasingly finding companies that not only want to provide neat, hi-tech devices, but also bundle with it a hi-tech, efficient power supply,\" the EPA\\'s Andrew Fanara said. Initiatives like this are critical; if power adaptors continue to be made and used as they are now, consumer electronics and other small appliances will be responsible for more than 40% of electricity used in US homes, said the EPA.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3wNHJ-YhA1n"
      },
      "source": [
        "## 문서를 벡터로 만들기 위해 단어 임베딩을 사용해 봅시다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUK5HtpbhA1o"
      },
      "source": [
        "### BoW와 달리 Word2Vec과 같은 단어 임베딩 방법은 문맥(context)정보를 보존합니다\n",
        "\n",
        "BoW는 단어의 존재 여부와 그 빈도 정보를 중요하게 다룹니다. <br> 대신 단어의 순서 정보를 무시하여 단어 주변 문맥정보를 잃어버린다는 단점이 있습니다.\n",
        "\n",
        "이와 달리 단어 임베딩 방법인 Word2Vec은 문장에서 인접한 단어들의 정보를 중요시 하여 벡터화할 때 해당 문맥 정보를 보존합니다. 그래서 의미적 또는 구조적으로 비슷한 사용법을 가진 단어들을 알 수 있게 됩니다.\n",
        "\n",
        "#### *임베딩이란?*\n",
        "자연어를 컴퓨터가 이해할 수 있는 수의 나열인 벡터 형태로 바꾸는 과정 또는 결과를 의미합니다. 앞서 살펴본 BoW 방법들은 문서를 벡터화 한 것이라 볼 수 있습니다. \n",
        "\n",
        "### Word2Vec 이란?\n",
        "Word2Vec은 구글 연구팀이 발표한(Mikolov et al., 2013) 기법으로 가장 널리 쓰이는 단어 임베딩 모델 중 한 가지 입니다. 단어 임베딩 방법으로는 skip-gram과 CBOW 두 모델이 제안되었습니다. \n",
        "\n",
        "#### 분포가설(Distributional Hypothesis)\n",
        "\n",
        "Word2Vec이 어떻게 문맥 정보를 보존하는지 이해하려면 분포가설([Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics))을 알아야 합니다. \n",
        "분포가설은 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 지닌다는 것 입니다. 여기서 분포(distribution)란 특정 윈도우(window) 범위 안에 동시에 등장하는 이웃 단어나 문맥의 집합을 말합니다. \n",
        "\n",
        "예를 들어 두 문장\n",
        "\n",
        "- I found **good** stores.\n",
        "- I found **bad** stores.\n",
        "\n",
        "에서 **good**과 **bad**은 주변에 분포한 문맥 단어들이 매우 유사함으로 추축하건데 비슷한 의미를 지닐 것이다 라고 가정하는 것 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn6NOAjphA1o"
      },
      "source": [
        "### Word2Vec을 구현하는 방법으로 Skip-Gram을 살펴보겠습니다\n",
        "\n",
        "CBOW, Skip-Gram은 Word2Vec을 구현하는 두 가지 방법입니다.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/304163783/figure/fig11/AS:668313973698561@1536349876279/Neural-Network-Architecture-for-CBoW-and-Skip-Gram-Model.ppm\"/>\n",
        "\n",
        "여기서는 Skip-Gram만 살펴 보겠습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIbrZQqChA1o"
      },
      "source": [
        "<img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=\"800\" />\n",
        "\n",
        "위 그림은 Skip-Gram 신경망 모식도 입니다.([Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/))\n",
        "\n",
        "입력으로 원핫인코딩된 단어벡터가 들어오고 출력부분에서는 입력단어와 다른 모든 단어들이 문맥 단어일 확률값을 계산하게 됩니다.\n",
        "\n",
        "신경망 학습을 위해서는 학습데이터를 구성하는 방법이 중요한데,\n",
        "\n",
        "Skip-Gram 모델은 만들어진 학습데이터에서 타겟단어를 입력받아 주변 단어를 예측하는 과정에서 학습이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24rU-tnfhA1o"
      },
      "source": [
        "\n",
        "예를 들어서 **\"The tortoise jumped into the lake\"** 라는 문장이 있고 윈도우 크기가 2인 경우 다음과 같이 skip-Gram을 학습하기 위한 데이터 쌍을 구축할 수 있습니다.\n",
        "\n",
        "* 타겟: **The**, 주변 문맥단어: tortoise, jumped\n",
        "    * 학습 샘플: (the, tortoise), (the, jumped)\n",
        "* 타겟: **tortoise**, 주변 문맥단어: tortoise, jumped\n",
        "    * 학습 샘플: (tortoise, the), (tortoise, jumped), (tortoise, into)\n",
        "* 타겟: **jumped**, 주변 문맥단어: tortoise, jumped\n",
        "    * 학습 샘플: (jumped, the), (jumped, tortoise), (jumped, into), (jumped, the)\n",
        "* 타겟: **into**, 주변 문맥단어: tortoise, jumped\n",
        "    * 학습 샘플: (into, tortoise), (into, jumped), (into, the), (into, lake)\n",
        "                    ...\n",
        "\n",
        "이런 방법으로 학습 데이터를 만들면 다음과 같은 데이터쌍이 만들어 집니다. 타겟단어를 입력으로, 문맥단어를 레이블로 하여 classification 학습을 진행한다고 생각하시면 됩니다. 전체 코퍼스에서 단어별로 슬라이딩하여 가능한 학습데이터를 모두 생성하고 신경망을 학습합니다.\n",
        "\n",
        "|타겟단어|문맥단어|\n",
        "|---------|---------|\n",
        "|the|tortoise|\n",
        "|the|jumped|\n",
        "|tortoise|the|\n",
        "|tortoise|jumped|\n",
        "|tortoise|into|\n",
        "|jumped|the|\n",
        "|jumped|tortoise|\n",
        "|jumped|into|\n",
        "|jumped|the|\n",
        "|into|tortoise|\n",
        "|into|jumped|\n",
        "|into|the|\n",
        "|into|lake|\n",
        "|...|...|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gMwyC8ahA1o"
      },
      "source": [
        "학습이 다 끝난 후 은닉층의 가중치가 임베딩벡터의 차원이 됩니다. 은닉층의 노드 수를 조절하여 임베딩벡터의 차원을 조절할 수 있겠지요.\n",
        "<img src=\"https://i.imgur.com/1ETMljf.png\" width=\"600\" />\n",
        "\n",
        "결과적으로 skip-gram 모델을 통해 결과로 단어 임베딩 벡터들을 얻게되어 단어, 문장들 간의 관련도 계산, 문서 분류같은 작업에 사용될 수 있습니다.\n",
        "\n",
        "CountVectorizer, TF-IDF을 사용할 때 문맥정보를 보기위해 할 수 있는 최선의 방법은 bi-gram, tri-grams 같은 n-gram을 사용하는 것이었습니다. 하지만 skip-grams은 그 이상을 넘어 보다 강력한 문맥 정보를 제공해 줄 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H427pSbnhA1o"
      },
      "source": [
        "#### CBOW(Continuous Bag of Words)\n",
        "\n",
        "Word2Vec을 구현하는 방법 중 CBOW는 skip-gram과 반대로 주변에 있는 문맥단어들을 가지고 타겟 단어 하나를 맞추는 과정을 통해 학습이 이루어 집니다.\n",
        "예를 들어 입력이 (jumped, the, lake) 일 때 타겟 단어로 'into' 를 예측하며 학습을 합니다.  \n",
        "\n",
        "하지만 Skip-gram이 같은 코퍼스를 사용했을 때 더 많은 학습 데이터를 만들어 낼 수 있기 때문에 임베딩 결과의 품질이 CBOW 보다 좋은 것으로 알려져 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yILpDeQfhA1p"
      },
      "source": [
        "\n",
        "#### 그럼 임베딩 모델 학습은 어떻게 해야 할까요?\n",
        "Word2Vec을 학습시키기 위해서는 충분히 큰 코퍼스를 학습시켜야 한다는 것을 알 수 있습니다. 다행인 것은 이미 충분히 큰 코퍼스들로 학습된 단어 임베딩을 쉽게 찾아 사용할 수 있다는 것입니다. 여러분이 지금까지 사용해온 Spacy 라이브러리의 모델도 Word2Vec 와 유사한 방식으로 학습한 임베딩을 제공합니다. 무지막지하게 큰 [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl) 데이터를 학습에 사용해 만든 모델이기 때문에 영어의 경우 충분히 대표성을 가진 임베딩이 나올 수 있을 것이라 생각할 수 있겠습니다.\n",
        "\n",
        "이제 spacy를 통해 임베딩을 어떻게 사용하는지 살펴 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXa4BhQBhA1p"
      },
      "source": [
        "tokens = nlp(\"Dogs and cats school gagasdf\")\n",
        "\n",
        "# vector_norm: 벡터의 크기\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9UAwka0hA1p"
      },
      "source": [
        "입력한 문장에 대한 임베딩 벡터를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg8AotxuhA1p"
      },
      "source": [
        "vects = tokens.vector\n",
        "print(vects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YVdOPpqhA1p"
      },
      "source": [
        "# 벡터의 차원을 보겠습니다\n",
        "len(vects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3X32Su6hA1p"
      },
      "source": [
        "# 두 문서를 만들어 코사인 유사도를 측정해 보겠습니다\n",
        "doc1 = nlp(\"I found a wonderful restaurant\")\n",
        "doc2 = nlp(\"the food is delicious\")\n",
        "\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHtJyZ1UhA1p"
      },
      "source": [
        "doc3 = nlp(\"The restaurant we found yesterday is wonderful\")\n",
        "\n",
        "print(doc1.similarity(doc3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHdtgC7hA1q"
      },
      "source": [
        "car = nlp('car')\n",
        "bus = nlp('bus')\n",
        "human = nlp('human')\n",
        "monkey = nlp('monkey')\n",
        "lion = nlp('lion')\n",
        "gorilla = nlp('gorilla')\n",
        "avengers = nlp('avengers')\n",
        "marvel = nlp('marvel')\n",
        "\n",
        "print('car vs bus : ', car.similarity(bus))\n",
        "print('bus vs human : ', bus.similarity(human))\n",
        "print('human vs monkey : ', human.similarity(monkey))\n",
        "print('human vs lion : ', human.similarity(lion))\n",
        "print('monkey vs gorilla : ', monkey.similarity(gorilla))\n",
        "print('avengers vs marvel : ', avengers.similarity(marvel))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyuM91FKhA1q"
      },
      "source": [
        "### PCA를 사용한 벡터 시각화\n",
        "300 차원인 벡터들은 시각화 하기 어렵기 때문에 PCA를 사용해 2차원으로 변환해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iswx1u7QhA1q"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def get_word_vectors(words):\n",
        "    # 단어 벡터로 변환합니다\n",
        "    return [nlp(word).vector for word in words]\n",
        "\n",
        "words = ['car', 'truck', 'suv', 'bus', 'human', 'man', 'woman', 'monkey', 'fish' , 'shark', 'lion', 'tiger', 'avengers', 'marvel', 'thor', 'comics', 'superhero']\n",
        "\n",
        "# PCA 모델의 차원을 설정하여 \n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit & Transform\n",
        "word_vect_2d = pca.fit_transform(get_word_vectors(words))\n",
        "\n",
        "# 각 벡터가 300 차원에서 2차원으로 줄어 든 것을 확인 할 수 있습니다\n",
        "word_vect_2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNfb8VM6hA1q"
      },
      "source": [
        "# 결과가 잘 보이도록 크기를 설정합니다\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "# 단어벡터를 그립니다\n",
        "plt.scatter(word_vect_2d[:,0], word_vect_2d[:,1])\n",
        "\n",
        "# 점 옆에 단어를 표시합니다\n",
        "for word, coord in zip(words, word_vect_2d):\n",
        "    x, y = coord\n",
        "    plt.text(x, y, word, size= 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EqyObEghA1q"
      },
      "source": [
        "# 벡터 연산을 통해 단어간의 관계를 추론할 수 있습니다\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "king = nlp(\"king\").vector\n",
        "queen = nlp(\"queen\").vector\n",
        "man = nlp(\"man\").vector\n",
        "woman = nlp(\"woman\").vector\n",
        "\n",
        "# 단어벡터가 의미를 가진다면 다음과 같은 연산을 통해 result는 queen과 비슷한 뜻이 되겠지요?\n",
        "result = king - man + woman\n",
        "\n",
        "print(\"similarity between queen and (king - man + woman) : \", 1 - cosine(queen, result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQYkoHSzhA1q"
      },
      "source": [
        "### 문서에서 벡터화 하여 KNN으로 검색해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ6qDrFEhA1q"
      },
      "source": [
        "Spacy를 사용해 문서를 임베딩 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjCkq9v2hA1r"
      },
      "source": [
        "X = [nlp(str(d)).vector for d in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbXjpf6PhA1r"
      },
      "source": [
        "pd.DataFrame(X).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYAE40oAhA1r"
      },
      "source": [
        "단어 queen 과 가장 유사한 단어를 찾아 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3PMKPF-hA1r"
      },
      "source": [
        "import numpy as np\n",
        "most_similar= nlp.vocab.vectors.most_similar(np.array([queen]), n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myjv71MkhA1r"
      },
      "source": [
        "# return tuple: (keys, best_rows, scores)\n",
        "most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWlv3yJohA1r"
      },
      "source": [
        "most_similar[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMnzHArwhA1s"
      },
      "source": [
        "for key in most_similar[0][0]:\n",
        "    print(nlp.vocab[key].text,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUyU7kc0hA1s"
      },
      "source": [
        "문서 임베딩 벡터로 NN 모델을 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34LX951IhA1s"
      },
      "source": [
        "nn_spacy = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
        "nn_spacy.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcEBHPE0hA1s"
      },
      "source": [
        "Tfidf 벡터로 찾은 0번째 문서와 가장 유사한 문서 5개를 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQJ08muHhA1s"
      },
      "source": [
        "nn.kneighbors([dtm.iloc[0].values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tylepD3phA1s"
      },
      "source": [
        "Spacy 임베딩 모델로 찾아 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sq6Azd0hA1s"
      },
      "source": [
        "nn_spacy.kneighbors([X[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycSUflRVhA1t"
      },
      "source": [
        "문서62는 문서0과 동일하고 그 외, 문서 92를 가장 가깝다고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k7Autk6hA1t"
      },
      "source": [
        "print(data[0][:150],'\\n')\n",
        "print(data[92][:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VSJ47dNhA1t"
      },
      "source": [
        "# spacy\n",
        "print(data[83][:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGJqcYrKhA1t"
      },
      "source": [
        "# tfidf\n",
        "print(data[297][:150])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDWESooKhA1t"
      },
      "source": [
        "지금까지 자연어를 벡터로 표현하는 방법들에 대해 살펴보았습니다. Bag-of-Words 모델 중 단어의 출현 빈도를 사용해 텍스트 문서를 벡터로 변환하는 CounterVectorizer를 사용해 보았고, 문서별 단어의 빈도를 계산해 가중치를 적용한 TfidfVectorizer를 사용해 보았습니다.\n",
        "\n",
        "이렇게 변환된 벡터는 코사인 유사도와 같은 방법을 통해 문서들 간 유사성을 수치로 나타낼 수 있었습니다. 문서들이 많을 때 코사인 유사도를 다 적용해서 가장 가까운 문서를 찾는 방법은 효율적이지 않습니다. 그래서 K-NN과 같은 트리 기반 알고리즘을 사용해 가장 가까운 K개의 문서를 빠르게 검색할 수 있었습니다.\n",
        "\n",
        "BoW는 단어의 존재와 빈도를 중요시 여기는 대신 단어들의 순서정보를 무시하여 주변 문맥 정보가 없어지는 단점이 있었습니다. Word2Vec과 같은 담어 임베딩 방법은 벡터 생성 과정 중에 문맥 정보를 보존하여 유사한 의미를 가진 단어나 문장은 는 유사도가 큰 벡터가 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvuOc9NOhA1t"
      },
      "source": [
        "## 참고자료\n",
        "\n",
        "- [NLTK Book](https://www.nltk.org/book/)\n",
        "- [KD tree algorithm: how it works](https://youtu.be/TLxWtXEbtFE)\n",
        "- [k-NN 7: how to make it faster](https://youtu.be/sCvJ3zo4DUA)\n",
        "- [An Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)\n",
        "- [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "- [A step by step explanation of PCA(principal component analysis)](http://localhost:8888/?token=ebeea486e072a985fe9597e449bd0f9cecbaec6ae8648532)"
      ]
    }
  ]
}