{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n133-high-dimensional-data.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM/KYVkBQuXknCxXhiqCXuo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codestates/ds-cs-section1-sprint3/blob/master/n133_high_dimensional_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCIUFXg-VPW4"
      },
      "source": [
        "<img src='https://i.imgur.com/RDAD11M.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 1 / SPRINT 3 / NOTE 3*\n",
        "\n",
        "---\n",
        "\n",
        "# High dimensional data\n",
        "\n",
        "## 🏆 학습 목표 \n",
        "\n",
        "- Vector transformation의 목적과 사용예시를 설명 할 수 있다.\n",
        "- eigenvector / eigenvalue를 설명 할 수 있다.\n",
        "- 데이터의 feature 수가 늘어나면 생기는 문제점과 이를 handling 하기 위한 방법을 설명 할 수 있다.\n",
        "- PCA의 목적과 기본원리를 설명 할 수 있다.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9mPUpZpjQp"
      },
      "source": [
        "# Vector transformation\n",
        "\n",
        "\n",
        "오늘은, $\\mathbb{R}^2$ 공간에서 벡터를 변환하는 것부터 시작하도록 하겠습니다.\n",
        "\n",
        "여기서 말하는 변환, 즉 선형 변환은 임의의 두 벡터를 더하거나 혹은 스칼라 값을 곱하는 것을 의미합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "\\begin{align}\n",
        "T(u+v) = T(u)+T(v)\n",
        "\\\\\n",
        "T(cu) = cT(u)\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89977531-4a73b400-dca6-11ea-9f43-f0c1f124b70b.jpg' width = 600>\n",
        "\n",
        "- 어제 배웠던 Linear projection도 일종의 vector transformation입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p5k0VeRoxIT"
      },
      "source": [
        "## 벡터변환으로써의 매트릭스-벡터의 곱 \n",
        "\n",
        "- `f` 라는 transformation을 사용하여 \n",
        "- 임의의 벡터 `[x1, x2]`에 대해서, \n",
        "- `[2x1 + x2, x1 -3x2 ]`로 변환을 한다\n",
        "\n",
        "라는 것은 아래와 같이 표현 될 수 있습니다.\n",
        "\n",
        "\\begin{align}\n",
        "f(\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}) = \\begin{bmatrix} 2x_1 + x_2 \\\\ x_1 -3x_2 \\\\  \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "여기서 원래 벡터 `[x1, x2]`는 유닛벡터를 이용하여 아래처럼 분리 할 수 있는데요, \n",
        "\n",
        "$x_1 \\cdot \\hat{i} + x_2 \\cdot \\hat{j}$ \n",
        "\n",
        "분리된 각 유닛벡터는 transformation를 통해서 각각 \n",
        "- $2x_1$, $x_1$과\n",
        "- $x_2$, $-3x_2$라는 결과가 나와야 한다는 것을 알 수 있습니다.\n",
        "\n",
        "이를 매트릭스의 형태로 합치게 되면\n",
        "\n",
        "\\begin{align}\n",
        "T = \\begin{bmatrix} 2 & 1 \\\\ 1 & -3 \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "위와 같은 $T$ 라는 매트릭스를 얻을 수 있고 \n",
        "\n",
        "이 매트릭스를 처음 벡터 `[x1,x2]`에 곱했을 경우 transformation이 원하는 대로 이루어진다는 것을 알 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "즉, 임의의 $\\mathbb{R}^2$ 벡터를 다른 $\\mathbb{R}^2$ 내부의 벡터로 변환 하는 과정은, \n",
        "\n",
        "특정 $T$라는 매트릭스를 곱하는 것과 동일한 과정입니다.\n",
        "\n",
        "새로운 벡터 (3,4)에 대하여 동일한 필터로 transform 하는 경우, 방금 구한 $T$라는 매트릭스에 곱하는 것으로 쉽게 할 수 있습니다.\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} 2 & 1 \\\\ 1 & -3 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ -9 \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "벡터 transformation은, 선형 즉 곱하고 더하는 것으로만 이뤄진 transformation이기 때문에 매트릭스와 벡터의 곱으로 표현 할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2zRiorVwA4w"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_vector = [2, 3] \n",
        "output_vector = [7, -7] \n",
        "\n",
        "plt.arrow(0, 0, input_vector[0], input_vector[1], head_width = .05, head_length = .05, color ='#d63031')\n",
        "plt.arrow(0, 0, output_vector[0], output_vector[1], head_width = .05, head_length = .05, color ='#0984e3')\n",
        "plt.xlim(0, 8)\n",
        "plt.ylim(-8, 5)\n",
        "plt.title(\"Transformed Vector\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_aNp4uk8cRl"
      },
      "source": [
        "## 고유벡터 (Eigenvectors)\n",
        "\n",
        "Transformation은 matrix를 곱하는 것을 통해, 벡터(데이터)를 다른 위치로 옮긴다라는 의미를 가지고 있습니다. \n",
        "\n",
        "이번 예시에는 $\\mathbb{R^3}$ 공간에서의 transformation을 사용해보도록 하겠습니다.\n",
        "\n",
        "아래의 회전하는 지구본은 $\\mathbb{R^3}$ 공간에서의 임의의 위치에서 다른 위치로 옮겨진다는 것을 설명하고 있습니다. \n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89986497-9d099c00-dcb7-11ea-81d1-3104ec05d818.png' width = 600>\n",
        "\n",
        "\n",
        "$\\mathbb{R^3}$ 공간이 회전 할때, 위치에 따라서 **변화 하는 정도**가 다르다는걸 눈치 채셨나요? \n",
        "\n",
        "- 가령 적도 부근에 있는 점의 변화되는 거리와, 극지방에 있는 점의 위치 변화의 크기는 다를 것입니다.\n",
        "- 이는 회전축으로 가까이 갈 수록 / 멀어질 수록 더욱 명확해지며, 정확하게 회전축에 위치 해있는 경우, transformation을 통해 위치가 변하지 않습니다. \n",
        "\n",
        "이러한 transformation에 영향을 받지 않는 회전축, (혹은 벡터)을 공간의 **고유벡터 (Eigenvector)**라고 부릅니다.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IDI6N7IcTnR"
      },
      "source": [
        "\n",
        "## 고유값 \n",
        "\n",
        "앞서 봤던 고유벡터는 주어진 transformation에 대해서 크기만 변하고 방향은 변화 하지 않는 벡터입니다. \n",
        "\n",
        "여기서 변화하는 크기는 결국 스칼라 값으로 변화 할 수 밖에 없는데, 이 **특정 스칼라 값**을 **고유값** (eigenvalue)이라고 합니다. \n",
        "\n",
        "eigenvector와 eigenvalue는 항상 쌍을 이루고 있다는 점에 유념하시길 바랍니다.\n",
        "\n",
        "$T \\cdot v = v' = \\lambda \\cdot v $ \n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} ax+by \\\\ cx+dy \\end{bmatrix} = \\lambda \\begin{bmatrix} x \\\\ y \\end{bmatrix}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PckxjtNdLwQ"
      },
      "source": [
        "### Example\n",
        "\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} 4 & 2 \\\\ 2 & 4 \\end{bmatrix}\\begin{bmatrix} 3 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -6 \\end{bmatrix} = 2 \\begin{bmatrix} 3 \\\\ -3 \\end{bmatrix}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ds9iohcNnc"
      },
      "source": [
        "## 고유값의 표기 \n",
        "\n",
        "$\\lambda$ 로 표현합니다.\n",
        "\n",
        "\\begin{align}\n",
        "T(v) = \\lambda v\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLpyyeXecPmK"
      },
      "source": [
        "## 고유값, 고유벡터 계산하기 \n",
        "\n",
        "이 내용은 Matrix Diagonalization 과 Gaussian Elimination등, 선형대수의 복잡한 내용들을 포함하고 있기 때문에 자세히 설명하지는 않겠습니다. \n",
        "\n",
        "대신 이를 응용하는 **Principle Component Analysis (PCA)** 에 관련된 기본 컨셉들을 더 공부해보도록 하겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb-Hu7ESdTXQ"
      },
      "source": [
        "## 고유값을 왜 배우는걸까요?\n",
        "\n",
        "Vector transformation은 결국 궁극적으로는 데이터를 변환한다 라는 큰 목적의 단계 중 하나입니다.\n",
        "\n",
        "그러면 우리는, 이런 생각을 하게 될 겁니다.\n",
        "\n",
        "> 과연 데이터를 왜? 어떻게? transformation 해야할까\n",
        "\n",
        "예를 들어 \n",
        "\n",
        "\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} 라는 데이터는 \n",
        "\n",
        "\\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} 로 transformation 할 수도 있고.\n",
        "\n",
        "\\begin{bmatrix} 7 \\\\ 8 \\end{bmatrix} 로 transformation 할 수도 있고.\n",
        "\n",
        "목적에 따라서, 거의 무한한 방법으로 transformation을 할 수가 있을텐데. \n",
        "\n",
        "그 중 어떤 목적으로, 어떤 transformation을 하냐에 따라서 고유값이 하나의 선택지가 된다는 겁니다.\n",
        "\n",
        "그렇다면 고유값은 어떤 목적으로 쓰일까요?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJTGZRWtR40J"
      },
      "source": [
        "# 고차원의 문제 (The Curse of Dimensionality)\n",
        "\n",
        "The Curse of Dimensionality란 피쳐의 수가 많은 (100 혹은 1000개 이상의) 데이터셋을 모델링하거나 분석할때에 생기는 여러 문제점들을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgh7mTh02rwW"
      },
      "source": [
        "## Dimension\n",
        "\n",
        "임의의 50개 수로 이루어진 데이터셋을 가정해보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j2kbuwd2-8Q"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "# 50개 데이터 생성 후 데이터 프레임에 저장\n",
        "N = 50\n",
        "x = np.random.rand(N)*100\n",
        "\n",
        "data = {\"x\": x}\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_QWXZ4E51Jm"
      },
      "source": [
        "# 선에 데이터 표기\n",
        "\n",
        "def setup(ax):\n",
        "    ax.spines['right'].set_color('none')\n",
        "    ax.spines['left'].set_color('none')\n",
        "    ax.yaxis.set_major_locator(ticker.NullLocator())\n",
        "    ax.spines['top'].set_color('none')\n",
        "    ax.xaxis.set_ticks_position('bottom')\n",
        "    ax.tick_params(which = 'major', width = 1)\n",
        "    ax.tick_params(which = 'major', length = 5)\n",
        "    ax.tick_params(which = 'minor', width = .75)\n",
        "    ax.tick_params(which = 'minor', length = 2.5)\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.patch.set_alpha(0)\n",
        "    \n",
        "plt.figure(figsize=(8, 6))\n",
        "n = 8\n",
        "\n",
        "df['y'] = pd.Series(list(np.zeros(50)))\n",
        "\n",
        "ax = plt.subplot(n, 1, 2)\n",
        "setup(ax)\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(5))\n",
        "ax.text(0, 0.5, \"Number Line\", fontsize = 14, transform = ax.transAxes)\n",
        "\n",
        "plt.subplots_adjust(left = .05, right = .95, bottom = .05, top = 1.05)\n",
        "plt.scatter(df.x, df.y, alpha = .5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgaFf9mgBLnR"
      },
      "source": [
        "### 2D 데이터셋\n",
        "\n",
        "이번에는 feature가 2개입니다. (여전히 50 sample 입니다)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIXbR9pxH5tn"
      },
      "source": [
        "plt.scatter(df.x, df.y, alpha = .5)\n",
        "plt.title(\"Not a good use of a 2D Graph\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H__fjvFHJFR2"
      },
      "source": [
        "# 임의의 50개 feature값을 생성\n",
        "df['y'] = pd.Series(list(np.random.rand(N)*100))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhA0WuNJWqm"
      },
      "source": [
        "plt.scatter(df['x'], df['y'], alpha = .5)\n",
        "plt.title(\"A Better Use of a 2D Graph\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iesGye7YKck7"
      },
      "source": [
        "### 3D의 데이터셋을 그려봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWc5yMZPLe5B"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# z 값을 추가\n",
        "df['z'] = pd.Series(list(np.random.rand(N)*100))\n",
        "\n",
        "threedee = plt.figure().gca(projection = '3d')\n",
        "threedee.scatter(df['x'], df['y'], df['z'])\n",
        "threedee.set_xlabel('X')\n",
        "threedee.set_ylabel('Y')\n",
        "threedee.set_zlabel('Z')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zst4Ae8AOw9L"
      },
      "source": [
        "### 만약 4D의 데이터셋은 어떻게 될까요?\n",
        "\n",
        "물론 이론적으로는 아직 4D 그래프 까지는 가능합니다만 결과는 3D 그래프에 그리는 것에 비해서 전혀 의미가 없습니다.\n",
        "\n",
        "<img src='https://mathworld.wolfram.com/images/eps-gif/HypercubeGraphUnitDistance_1000.gif' width = 600>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEsEK8SUR1zH"
      },
      "source": [
        "### 20D의 데이터셋 (Feature가 20개)\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89989454-f1167f80-dcbb-11ea-954d-2cffc7c3f62d.png' width = 400>\n",
        "\n",
        "- 사실 사람의 뇌는 3차원 이상의 정보를 공간적으로 다루는 것이 **거의** 불가능 합니다. \n",
        "\n",
        "- 다시 말해 이는 여러 차원의 데이터셋을 다루는 데에 있어서 큰 이슈가 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cENzttkdUilw"
      },
      "source": [
        "## 고차원 데이터를 분석하기 위한 더 복잡한 시각화\n",
        "\n",
        "<img src=\"https://seaborn.pydata.org/_images/pairplot_1_0.png\" width = 400>\n",
        "\n",
        "위의 pairplot은 4차원 데이터셋에 포함 되어 있는 변수들의 가능한 조합에 대해서 scatter plot을 그리고, 동일한 변수 조합에 대해서는 histogram을 그리고 있습니다.\n",
        "\n",
        "문제 : 만약 20차원의 데이터셋이라면 scatter plot은 몇개를 그려야 할까요? \n",
        "\n",
        "- 한가지 유념할 것은, pairplot에는 상당히 많은 양의 불필요한 scatterplot이 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoHEMJlwctqZ"
      },
      "source": [
        "## 추가 Feature 사용과 결과물\n",
        "\n",
        "데이터셋에서 인사이트를 찾기 위해 쓰이는 모든 feature가 동일하게 중요하지는 않습니다. \n",
        "Feature를 추가로 사용하는 것이 실제적으로 얼마나 의미있게 더 좋은 결과를 모델링 하게 되는지는 고민해야 할 문제라는 거죠.\n",
        "\n",
        "다음은 인공지능 (딥러닝) 이미지 인식의 좋은 예시입니다\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89990525-7babae80-dcbd-11ea-8e02-bbe27aa8c6bd.jpg' width = 500>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src='https://image.fmkorea.com/files/attach/new/20190429/486616/667852209/1774330158/b26f88e6f915d658d5d858fe0d93a452.png' width = 500>\n",
        "\n",
        "데이터의 일부를 제한하더라도, 의미 파악에는 큰 차이가 없다는걸 안다면, feature의 수와 관련하여 어느 시점에서는 feature를 더 사용하는 것이 비효율적일것 입니다.\n",
        "\n",
        "- <https://en.wikipedia.org/wiki/Dilution_(neural_networks)>\n",
        "\n",
        "만약 여러분이 나중에 nlp / text mining 분야로 취업을 한다면 해당 분야도 연구해주세요...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUutPQn4ctqZ"
      },
      "source": [
        "## 높은 feature의 다른 문제점\n",
        "\n",
        "샘플 수에 비해서 feature의 수가 너무 많은 경우, overfitting의 문제 또한 발생합니다. \n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89990876-f4126f80-dcbd-11ea-9046-8a4d7181d2ea.jpg'>\n",
        "\n",
        "> 지금은 이 그림의 의미를 모르겠지만, 3주가 지난 후, 이해하게 될 겁니다.\n",
        "\n",
        "물론 이에 대해서 정확하게 정해진 기준이 있는 건 아니지만, \n",
        "\n",
        "일반적으로 feature의 수를 $P$, sample의 수를 $N$이라 할 때 \n",
        "\n",
        "**P** $\\geq$ **N** 인 경우 매우 높은 overfitting 이슈가 생긴다고 할 수 있습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW2DajrOPCp7"
      },
      "source": [
        "# Dimension Reduction\n",
        "\n",
        "데이터의 시각화나 탐색이 어려워지는 것 뿐만 아니라 모델링에서의 overfitting 이슈를 포함하는 등 \"빅데이터\"인 데이터셋의 feature가 많으면 많을 수록 이로 인해 발생하는 문제는 점점 많아질 것입니다. \n",
        "\n",
        "만약 \"빅데이터\"를 적절한 처리를 통해 충분한 의미를 유지하면서 더 작은 부분만 선택 할 수 있다면 어떨까요? \n",
        "\n",
        "머신러닝에서는 이를 위한 다양한 차원축소 기술들이 이미 연구되어 있습니다. 사실 지금도 연구 중이죠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8jR-MdQgdHV"
      },
      "source": [
        "## Feacture Selection: \n",
        "\n",
        "분석해야할 데이터셋에 100개의 feature가 있다고 해봅시다. 100종류의 feature를 전부 사용 하는 대신에, 데이터셋에서 제일 다양하게 **분포**되어있는 (1개의) feature를 사용하는 것입니다. 이처럼 Feature Selection이란 데이터셋에서 덜 중요한 feature를 제거 하는 방법을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KazJzMcgaNO"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "`Feature Extraction`이란 앞서 `Feature engineering` 부분에서 접했던 것처럼, \n",
        "- 기존에 있는 Feature 혹은 \n",
        "- 그들을 바탕으로 조합된 Feature를 사용 하는 것으로 \n",
        "\n",
        "PCA도 Feature extraction의 한 예시로 볼 수 있습니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJnX5Tfkiiud"
      },
      "source": [
        "### ❓ Selection 과 Extraction의 차이\n",
        "\n",
        "<img src='https://i.imgur.com/yuFQkLa.png' width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-1dvyzVQlnB"
      },
      "source": [
        "# Principal Component Analysis (PCA) \n",
        "\n",
        "PCA란, 고차원의 데이터셋을 원본 데이터의 정보를 최대한 유지하면서 낮은 차원의 데이터셋으로 변환하는 기술을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXPGw6hf8p72"
      },
      "source": [
        "## 데이터의 분산 == 정보\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89992786-c11dab00-dcc0-11ea-9136-51f619596738.png' width = 500>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyQpHtZkKLV"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [-2.2, -2, -2, -1, -1, 0, 0, 1, 1, 2, 2, 2.2]\n",
        "y = [0, .5, -.5, .8, -.8, .9, -.9, .8, -.8, .5, -.5, 0]\n",
        "\n",
        "data = {\"x\": x, \"y\": y}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "axes = plt.gca()\n",
        "axes.set_aspect('equal')\n",
        "plt.scatter(df['x'], df['y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKeYAldByDk"
      },
      "source": [
        "위의 scatter plot에 그려진 각 포인트들은, 2차원의 데이터셋을 의미한다고 가정해봅시다. \n",
        "\n",
        "2개의 feature중에서 1개만을 분석에 사용 해야 한다면, X와 Y 중 어느 feature를 사용 해야 할까요??\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZAoMCOaArYk"
      },
      "source": [
        "import math\n",
        "\n",
        "x1 = [-2.2,-2,-2,-1,-1,0,0,1,1,2,2,2.2]\n",
        "y1 = [0,.5,-.5,.8,-.8,.9,-.9,.8,-.8,.5,-.5,0]\n",
        "\n",
        "data = {\"x\": x1, \"y\": y1}\n",
        "\n",
        "df1 = pd.DataFrame(data)\n",
        "\n",
        "df1[\"x_rotate\"] = df1.apply(lambda x: (x.x+x.y)/math.sqrt(2), axis=1)\n",
        "df1[\"y_rotate\"] = df1.apply(lambda x: (x.y-x.x)/math.sqrt(2), axis=1)\n",
        "\n",
        "axes = plt.gca()\n",
        "axes.set_aspect('equal')\n",
        "plt.scatter(df1['x_rotate'], df1['y_rotate'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_pFspXxFhIq"
      },
      "source": [
        "이번엔 조금 다른 예시 입니다. 만약 데이터가 위 처럼 X, Y 축에 평행하지 않다면 X와 Y중 어느 feature를 사용하는게 효율적일까요?\n",
        "\n",
        "정답은 X, Y도 아닌 데이터의 흩어진 정도를 가장 크게 하는 벡터 축이 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz5yaQc5GhBe"
      },
      "source": [
        "## PCA Process\n",
        "\n",
        "<https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/>\n",
        "\n",
        "\n",
        "<img src='https://user-images.githubusercontent.com/6457691/89995442-6be39880-dcc4-11ea-9422-fb62f8500a86.png'>\n",
        "\n",
        "\n",
        "### 1) 데이터를 X와 Y변수로 나눔\n",
        "\n",
        "여기서 Y 란 \"라벨\" 혹은 \"타겟\"으로 우리가 궁극적으로 예측 하고자 하는 목표를 의미 합니다. \n",
        "\n",
        "그리고 X란 데이터셋에서 Y를 예측하기 위한 모든 feature들을 의미합니다. \n",
        "\n",
        "### 2) 각 행에 대해서 평균을 빼는 것으로 스케일을 조정함\n",
        "\n",
        "이로써 각 행의 평균 값은 0이 됩니다. 이를 하지 않으면, 데이터 별로 스케일이 다르기 때문에 더 유의미한 분산을 찾는 과정의 효과가 사라집니다.\n",
        "\n",
        "### 3) 각 행에 대해서 표준편차로 나누어줌 \n",
        "\n",
        "2)와 3)과정을 통해 데이터셋의 모든 행은 각각 평균 0, 표준편차 1의 값을 가지게 될 것이고 이 두 과정을 합쳐서 데이터의 표준화, 정규화 (\"standardizing\") 라고 합니다. \n",
        "\n",
        "이후, 정규화된 매트릭스를 $Z$ 매트릭스라고 부르겠습니다.\n",
        "\n",
        "### 4) Z의 분산-공분산 매트릭스를 계산함\n",
        "\n",
        "$Z^{T}Z$를 통해 계산 할 수 있습니다.\n",
        "\n",
        "### 5) 분산-공분산 매트릭스의 고유벡터와 고유값을 계산함\n",
        "\n",
        "고유벡터는, 주어진 데이터에 대해서 분산을 가장 크게 유지시키는 역할을 합니다.\n",
        "\n",
        "### 6) 고유값, 고유벡터 쌍을 크기에 따라 정렬\n",
        "\n",
        "일반적으로 고유값,고유 벡터는 여러개가 계산되며 이들의 크기는 데이터를 얼마나 **분산**시킬 수 있는 가와 연관이 있습니다. 즉, 가장 큰 쌍은 첫번째 Principal Component 로써 사용됩니다.\n",
        "\n",
        "### 7) 데이터를 고유 벡터에 projection 시키는 것으로 매트릭스 변환을 함.\n",
        "\n",
        "마지막 단계를 마치고 나면, 원래 데이터의 정보를 최대한 유지 하면서 차원을 줄이는 PCA를 마무리 지을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsA0bJKEM6Su"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import cov\n",
        "from numpy.linalg import eig\n",
        "\n",
        "X = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"Data: \", X)\n",
        "means = mean(X.T, axis=1)\n",
        "print(\"\\n Means: \\n\", means)\n",
        "centered_data = X - means\n",
        "print(\"\\n Centered Data: \\n\", centered_data)\n",
        "std_devs = std(X.T, axis=1)\n",
        "print('\\n Standard Deviations: \\n', std_devs)\n",
        "standardized_data = centered_data / std_devs\n",
        "print(\"\\n Standardized Data: \\n\", standardized_data)\n",
        "covariance_matrix = cov(standardized_data.T)\n",
        "print(\"\\n Covariance Matrix: \\n\", covariance_matrix)\n",
        "values, vectors = eig(covariance_matrix)\n",
        "print(\"\\n Eigenvectors: \\n\", vectors)\n",
        "print(\"\\n Eigenvalues: \\n\", values)\n",
        "P = vectors.T.dot(standardized_data.T)\n",
        "print(\"\\n Projected Data: \\n\", P.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6yHvmJ6Pk65"
      },
      "source": [
        "x = [1,3,5] \n",
        "y = [2,4,6]\n",
        "data = {\"x\": x, \"y\": y}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "plt.scatter(df['x'], df['y'])\n",
        "plt.title(\"Data Before PCA\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncjBR4o1QUIB"
      },
      "source": [
        "x = [-1.732,0,1.732] \n",
        "y = [0,0,0]\n",
        "data = {\"x\": x, \"y\": y}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "plt.scatter(df['x'], df['y'])\n",
        "plt.title(\"Data After PCA\")\n",
        "plt.xlabel('PC1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBNMa7qLOzW0"
      },
      "source": [
        "# 📕 라이브러리를 사용한 PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsR_xy2YOI-S"
      },
      "source": [
        "from numpy import array\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = array([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"Data: \\n\", X)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "Z = scaler.fit_transform(X)\n",
        "print(\"\\n Standardized Data: \\n\", Z)\n",
        "\n",
        "pca = PCA(2)\n",
        "\n",
        "pca.fit(Z)\n",
        "\n",
        "print(\"\\n Eigenvectors: \\n\", pca.components_)\n",
        "print(\"\\n Eigenvalues: \\n\",pca.explained_variance_)\n",
        "\n",
        "B = pca.transform(Z)\n",
        "print(\"\\n Projected Data: \\n\", B)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}