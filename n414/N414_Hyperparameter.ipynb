{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N414_Hyperparameters.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEjsZ-UJyd-J"
      },
      "source": [
        "<img align=\"right\" src=\"https://ds-cs-images.s3.ap-northeast-2.amazonaws.com/Codestates_Fulllogo_Color.png\" width=100>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 1 / NOTE 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2psQ6JRyfPA"
      },
      "source": [
        "# N414. Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GJWyLbcyqAs"
      },
      "source": [
        "## 🛫 Warm Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEdTF63OylyS"
      },
      "source": [
        "- [Hyperparameter Tuning Guide](https://www.youtube.com/watch?v=-i8b-srMhGM) - 딥러닝 홀로서기\n",
        "- [Gradient Descent With Momentum](https://youtu.be/yWQZcdJ4k8s?t=34)\n",
        "    - 학습해왔던 관성의 법칙을 유지하는 방식으로 학습 개선\n",
        "- [Batch Size](https://youtu.be/U4WB9p6ODjM?t=29)\n",
        "    - Batch를 크게하면 좋은 이유\n",
        "    - 그러나 항상 크게할 수 없는 이유\n",
        "    - 일반적으로 Batch라고 하면 Mini-batch를 의미한다는 점\n",
        "\n",
        "- 강의자료 맨 아래, \"실험 기록 프레임워크\"의 Wandb [QuickStart](https://docs.wandb.com/quickstart)를 보고 회원가입 등을 해두세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5jPGbUyyr2R"
      },
      "source": [
        "### 지난 시간 내용 복습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-iQyUWGyuIZ"
      },
      "source": [
        "- 신경망의 순전파와 역전파 (Note 1-2)\n",
        "    - 신경망의 순전파 (Note 1)\n",
        "    - 신경망의 역전파 (Note 2)\n",
        "    - 모델 생성과 모델 초기화 (Note 3)\n",
        "    - 경사하강법의 다양성 (Note 2-3)\n",
        "    - 학습 과정에서 알아야 할 Tricks (Note 3)\n",
        "        - 가중치 감소/제한(Weight Decay/Constraint)\n",
        "        - 드롭아웃(Dropout)\n",
        "        - 학습률 계획(Learning Rate Scheduling)\n",
        "\n",
        "- 그간 다뤄본 데이터\n",
        "    - 손글씨 MNIST\n",
        "    - Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWTT4dpLy1TD"
      },
      "source": [
        "## 🏆 학습 목표"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBrDO4FLyxNJ"
      },
      "source": [
        "- 하이퍼파라미터 탐색 방법에 어떤 것이 있으며 신경망에서 조정할 수 있는 주요 하이퍼파라미터에는 어떤 것이 있는지 설명할 수 있습니다\n",
        "- Scikit-learn, Keras Tuner 등을 사용하여 구축한 신경망에 하이퍼파라미터 탐색 방법을 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQuWaNgry3KC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PI1IG0h6byq"
      },
      "source": [
        "## 하이퍼파라미터(Hyperparameter) 튜닝으로 성능 올리기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-SsRyN60Va"
      },
      "source": [
        "신경망에서는 신경써야 할 **<font color=\"ff6f61\">하이퍼파라미터(Hyperparameter)</font>**가 굉장히 많습니다.<br/>\n",
        "지금까지 다뤄온 머신러닝 알고리즘은 많아야 20개 정도의 하이퍼파라미터를 탐색하면 되었습니다.<br/>\n",
        "하지만 신경망은 층이 깊어짐에 따라서 훨씬 더 조정해주어야 할 하이퍼파라미터가 많아지게 됩니다.\n",
        "\n",
        "하이퍼파라미터 조정(Tuning)은 모델 성능에 엄청난 영향을 끼치는 요소이기 때문에 시간이 많이 소요되더라도 반드시 해주어야 합니다.<br/>\n",
        "좋은 하이퍼파라미터를 찾기란 결코 쉽지 않습니다.<br/>\n",
        "운좋게도 임의로 입력한 하이퍼파라미터가 만족스런 성능을 보일 수는 있지만 '기도메타'가 언제나 우리에게 성공을 보장하지는 않죠.<br/>\n",
        "그렇다면 결정한 하이퍼파라미터로 구축한 모델이 좋은 성능을 보이는지를 어떻게 알 수 있을까요?\n",
        "\n",
        "머신러닝 알고리즘을 다룰 때에 일반적인 모델의 성능을 평가하기 위해서 **<font color=\"ff6f61\">교차 검증(Cross-Validation)</font>**을 사용하였습니다.<br/>\n",
        "신경망도 역시 교차 검증을 사용하여 일반화 성능을 평가할 수 있습니다.\n",
        "\n",
        "아래 코드를 통해 신경망에 교차 검증을 적용하는 방법에 대해 알아보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurGphTOjUoa"
      },
      "source": [
        "### 신경망으로 Boston 집값 데이터 예제 해결하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAxXZWvzzNB3"
      },
      "source": [
        "보스턴 집값 데이터셋(`boston_housing`) 예제를 신경망으로 풀어보겠습니다.<br/>\n",
        "문제를 푸는 과정에서 교차 검증을 적용하여 풀어보도록 하겠습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cerYfDfyEhHL"
      },
      "source": [
        "1. **데이터셋을 불러온 후에 확인합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ZsMjdQyPmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90caa33-8bc4-48df-94c5-a18edb86f3d2"
      },
      "source": [
        "# 데이터를 불러옵니다. \n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gCRjbZMzT7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a788a09-9e4c-40d5-defd-afa0b555bcad"
      },
      "source": [
        "print(x_train[:2])\n",
        "print(y_train[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.23247e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 6.14200e+00\n",
            "  9.17000e+01 3.97690e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 0.00000e+00 4.15000e-01 7.61000e+00\n",
            "  1.57000e+01 6.27000e+00 2.00000e+00 3.48000e+02 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]]\n",
            "[15.2 42.3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mJIYb08pwrR"
      },
      "source": [
        "### 신경망에 교차 검증(Cross-Validation) 적용해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG_q669kzQO4"
      },
      "source": [
        "> ❗️ ***머신러닝에서 배운 교차 검증이 기억이 잘 안난다면 [링크](https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538)를 참조해주세요!***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGuKDqWEmCz"
      },
      "source": [
        "2. **필요한 라이브러리를 import 합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qAKKzvpzUQ7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aNBG2G8E_XK"
      },
      "source": [
        "3. **`KFold`를 통해 학습 데이터셋을 몇 개(k)로 나눌지를 결정합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A7XVMcQEzNf",
        "outputId": "b66331d5-68b7-4759-bf8e-ea32030d522c"
      },
      "source": [
        "# kf 와 skf 에 각각 학습 데이터를 5개 로 나누도록 지정합니다.\n",
        "kf = KFold(n_splits = 5)\n",
        "skf = StratifiedKFold(n_splits = 5, random_state = 100, shuffle = True) \n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUXqIKI4FSfp"
      },
      "source": [
        "> ❓ ***`KFold`와 `StratifiedKFold`의 차이는 무엇일지 다시 떠올려봅시다.<br/>\n",
        "어떤 경우에 `KFold`가 아닌 `StratifiedKFold`를 써주어야 할까요?***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki-Bk4Z-zZHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceddde21-85e3-4092-cff5-61abc7d88157"
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lliEB-aNzSnI"
      },
      "source": [
        "> ❗️ ***아래부터 등장하는 코드는 고의적으로 에러를 발생하도록 쓰여 있습니다. 설명을 충분히 읽으면서 실행해 주세요!***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1WAKAjnzbVC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3eb0873f-cdb8-4abd-8526-c7c3f3920e8d"
      },
      "source": [
        "training_data = x_train.iloc[train_index]\n",
        "validation_data = x_train.iloc[val_index]\n",
        "\n",
        "# for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\n",
        "#   training_data = x_train.iloc[train_index]\n",
        "#   validation_data = x_train.iloc[val_index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-53c6661f6ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   training_data = x_train.iloc[train_index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p420wHyzzd33"
      },
      "source": [
        "위처럼 **Numpy 어레이(array)를 쓴다면 `.iloc` 을 쓸 수 없겠죠?**<br/>\n",
        "그러니 `pd.DataFrame()` 을 이용합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh4JEEryzhTU"
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0]), y_train):\n",
        "  training_data = x_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index]\n",
        "  training_y = y_train.iloc[train_index]\n",
        "  validation_y = y_train.iloc[val_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79hlwGhzhhg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "e9f73b34-2f39-4370-b9f8-6eb1f7581527"
      },
      "source": [
        "# 데이터 확인\n",
        "# training_data, validation_data, training_y, validation_y\n",
        "validation_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>19.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>41.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>20.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>20.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>13.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>25.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>19.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>29.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0\n",
              "324  19.3\n",
              "325  41.3\n",
              "326  20.4\n",
              "327  20.5\n",
              "328  13.8\n",
              "..    ...\n",
              "399  19.4\n",
              "400  25.2\n",
              "401  19.4\n",
              "402  19.4\n",
              "403  29.1\n",
              "\n",
              "[80 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9ACHSxzkl8"
      },
      "source": [
        "아래 코드에서는 모델을 불러오는데 에러가 납니다! 무엇 때문에 나는 에러일까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-s8hCNrzmlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "105064e1-ae58-40d5-9e8a-064800b36a40"
      },
      "source": [
        "# CREATE NEW MODEL\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-28c7a5557c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CREATE NEW MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgWBVQRVzort"
      },
      "source": [
        "에러명을 살펴보면 `NameError: name 'Sequential' is not defined` 입니다.<br/>\n",
        "**`Sequential`이 defined 되지 않았다는 뜻이므로 해당 패키지(`Sequential`)를 import** 해주어 해결해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HouKt58HzqW0"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "# CREATE NEW MODEL\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEvnNciwzuj_"
      },
      "source": [
        "이번에는 Dense를 추가해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axPe91tuzwJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "020d895b-a4be-4ba0-aa56-4ba1fee956ca"
      },
      "source": [
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8a2b07a9a70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhs-b8Alzw5L"
      },
      "source": [
        "에러명 `NameError: name 'Dense' is not defined` 을 살펴보니 동일한 에러임을 알 수 있습니다.<br/>\n",
        "**같은 유형의 에러이므로 같은 방법으로 해결**해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A4RfxG9zyUI"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "# CREATE NEW MODEL\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# COMPILE NEW MODEL\n",
        "model.compile(loss='mean_squared_logarithmic_error',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOLwqWJdz0hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bbc24f-f2ae-43eb-8da0-9eae645a5e7c"
      },
      "source": [
        "model.fit(training_data, training_y,\n",
        "\t\t\t    epochs=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "11/11 [==============================] - 1s 2ms/step - loss: 0.9897 - accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6e7661590>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKgNETVzz8U"
      },
      "source": [
        "위 코드까지 모델이 잘 돌아가는 것을 확인하였습니다.\n",
        "\n",
        "이제는 **교차 검증(Cross-Validation)을 적용할 차례**입니다.<br/>\n",
        "다시 학습 데이터셋(`x_train, y_train`)을 k개 의 set으로 나누어주겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO7GJKEFz0YZ"
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0]),y_train):\n",
        "  training_data = x_train.iloc[train_index, :]\n",
        "  training_data_label = y_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index, :]\n",
        "  validation_data_label = y_train.iloc[val_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSjaGn8LISTq"
      },
      "source": [
        "다시 모델을 학습시켜줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jAr9QxQz7G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d40da4c-9764-4aee-ce6d-4a66669128b1"
      },
      "source": [
        "model.fit(training_data, training_data_label,\n",
        "\t\t\tepochs=10,\n",
        "            batch_size=64,\n",
        "\t\t\tvalidation_data=(validation_data, validation_data_label),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 0.3301 - accuracy: 0.0000e+00 - val_loss: 0.1409 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 0.1566 - accuracy: 0.0000e+00 - val_loss: 0.1735 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.0000e+00 - val_loss: 0.1215 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.0000e+00 - val_loss: 0.1052 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.0000e+00 - val_loss: 0.1041 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1010 - accuracy: 0.0000e+00 - val_loss: 0.0901 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.0910 - accuracy: 0.0000e+00 - val_loss: 0.0887 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1148 - accuracy: 0.0000e+00 - val_loss: 0.0840 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1010 - accuracy: 0.0000e+00 - val_loss: 0.1010 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 0.1726 - accuracy: 0.0000e+00 - val_loss: 0.0937 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6e75aad10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTE8_O53z9tc"
      },
      "source": [
        "데이터가 잘 나누어져 들어갔는 지 확인을 해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYhxLVWDz9in",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a19f2329-7869-42b4-9527-70d983635866"
      },
      "source": [
        "print(training_data[:2])\n",
        "print(training_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0     1     2    3      4   ...   8      9     10      11     12\n",
            "0  1.23247   0.0  8.14  0.0  0.538  ...  4.0  307.0  21.0  396.90  18.72\n",
            "1  0.02177  82.5  2.03  0.0  0.415  ...  2.0  348.0  14.7  395.38   3.11\n",
            "\n",
            "[2 rows x 13 columns]\n",
            "(324, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUzUjOoG0Bft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "b34cbb5d-e880-457d-c63b-dda4752820af"
      },
      "source": [
        "training_data_label[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0\n",
              "0  15.2\n",
              "1  42.3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6XOWNZL0BdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d167c92-8ba6-4f6a-cf24-6d8653009d5a"
      },
      "source": [
        "# COMPILE NEW MODEL\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(training_data, training_data_label,\n",
        "\t\t    epochs=10,\n",
        "            batch_size=30,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 76.0882\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 79.1451\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 65.2776\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 56.1669\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 52.3630\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 50.8348\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 51.6367\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 50.1368\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 51.3770\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 2ms/step - loss: 45.9958\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6e5dc2510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJsu0llu0EJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7ad433-9f15-485a-ffba-43dfca0e106b"
      },
      "source": [
        "# COMPILE NEW MODEL\n",
        "# 다양한 loss로 테스트도 해봅니다. \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam') # binary_crossentropy # mean_squared_error\n",
        "model.fit(x_train, y_train,\n",
        "\t\t\tepochs=10,\n",
        "            batch_size=30,\n",
        "            validation_data = (validation_data, validation_data_label),\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 15ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: -326.2582 - val_loss: -345.8718\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: -326.2582 - val_loss: -345.8718\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6e747b650>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQIBr1tn0GhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05385e1-7a87-43d4-ff97-39d0356757e4"
      },
      "source": [
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print(\"test loss, test mse:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 427ms/step - loss: -336.6792\n",
            "test loss, test mse: -336.6792297363281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPAM9RT-0IJK"
      },
      "source": [
        "이제 한번에 테스트를 수행해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX9WoRHF0IbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cbdada-1fe5-40d0-b725-ec738f951426"
      },
      "source": [
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "for train_index, val_index in kf.split(np.zeros(x_train.shape[0])):\n",
        "  training_data = x_train.iloc[train_index, :]\n",
        "  training_data_label = y_train.iloc[train_index]\n",
        "  validation_data = x_train.iloc[val_index, :]\n",
        "  validation_data_label = y_train.iloc[val_index]\n",
        "\n",
        "  # CV\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "  model.fit(x_train, y_train,\n",
        "\t\t\t    epochs=10,\n",
        "          batch_size=30,\n",
        "          validation_data = (validation_data, validation_data_label),\n",
        "          )\n",
        "  results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "  print(\"test loss, test mse:\", results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 90.6808 - val_loss: 133.9628\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 82.7211 - val_loss: 76.0921\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 50.4193 - val_loss: 45.0233\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 46.3767 - val_loss: 39.1982\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 46.6952 - val_loss: 46.5693\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 45.5071 - val_loss: 46.4023\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 49.6951 - val_loss: 41.6469\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 44.0159 - val_loss: 37.5868\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 44.8145 - val_loss: 38.7562\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 45.7321 - val_loss: 34.9269\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 53.4831\n",
            "test loss, test mse: 53.483055114746094\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 13ms/step - loss: 77.7608 - val_loss: 61.5221\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 44.8202 - val_loss: 56.6208\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 43.4153 - val_loss: 37.5639\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 48.4203 - val_loss: 40.5958\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 42.1529 - val_loss: 37.2783\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 39.6534 - val_loss: 40.9396\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 40.9752 - val_loss: 34.5762\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 39.7855 - val_loss: 38.2236\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 41.5618 - val_loss: 36.8517\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.6850 - val_loss: 41.4709\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 45.7687\n",
            "test loss, test mse: 45.76872253417969\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 12ms/step - loss: 73.2851 - val_loss: 26.1349\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 37.5495 - val_loss: 23.4891\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.1633 - val_loss: 23.6373\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 39.3346 - val_loss: 31.1647\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 38.9934 - val_loss: 23.0196\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 36.1354 - val_loss: 22.6161\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 36.3422 - val_loss: 24.2353\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 36.6817 - val_loss: 22.6350\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 39.1093 - val_loss: 22.7980\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 37.9897 - val_loss: 23.4745\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 43.3485\n",
            "test loss, test mse: 43.34848403930664\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 85.6230 - val_loss: 89.1841\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 53.0501 - val_loss: 48.6500\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 37.1283 - val_loss: 44.7545\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 37.5344 - val_loss: 45.9425\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 33.4192 - val_loss: 42.4130\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 35.0226 - val_loss: 40.2124\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.9995 - val_loss: 39.3792\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.4471 - val_loss: 42.0385\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 36.6873 - val_loss: 42.8293\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.8234 - val_loss: 42.2015\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 38.3516\n",
            "test loss, test mse: 38.351585388183594\n",
            "Epoch 1/10\n",
            "14/14 [==============================] - 1s 14ms/step - loss: 90.9813 - val_loss: 64.5641\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 0s 5ms/step - loss: 50.1860 - val_loss: 42.6582\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 33.3428 - val_loss: 38.3366\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 33.0288 - val_loss: 37.5452\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 33.7598 - val_loss: 39.4353\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 34.3422 - val_loss: 46.7428\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 34.7877 - val_loss: 37.3384\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 0s 3ms/step - loss: 32.0352 - val_loss: 44.1750\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 33.6727 - val_loss: 37.1042\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 0s 4ms/step - loss: 32.0691 - val_loss: 38.5064\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 35.4677\n",
            "test loss, test mse: 35.46770095825195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYfuwjWG0Lje"
      },
      "source": [
        "이렇게 하면, CV를 통해서 모델을 돌릴 수 있는 것까지 확인해보았습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GH_RfDAF-gH"
      },
      "source": [
        "### 입력 데이터 정규화 (Normalizing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEy7AFvC0Sib"
      },
      "source": [
        "입력 데이터 정규화(Normalizing, Scaling)에 대해서 복습해보겠습니다.\n",
        "\n",
        "신경망에서는 입력 데이터 정규화가 무조건 필요하지는 않습니다.<br/>\n",
        "신경망이 수치형 데이터를 받으면 자체적으로 적절한 가중치를 학습하기 때문인데요.\n",
        "\n",
        "하지만 정규화를 해주면 **학습을 빠르게 해주고, 최적화 과정에서 지역 최적점(Local optimum)에 빠질 위험을 줄여줍니다.**<br/>\n",
        "그렇기 때문에 가능하다면 정규화를 해주는 것이 좋겠죠?\n",
        "\n",
        "> ❗️ ***해당 내용이 담겨 있는 Stackoverflow [링크](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network)입니다. 시간이 나면 읽어보도록 합시다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHwhUzko0UPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008448fa-d437-4e52-d40d-b7776051c221"
      },
      "source": [
        "# 정규화를 위한 함수 호출\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_train[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "   0.8252202 ]\n",
            " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
            "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
            "  -1.32920239]\n",
            " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
            "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
            "  -1.30850006]\n",
            " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
            "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
            "  -0.65292624]\n",
            " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
            "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
            "   0.26349695]\n",
            " [-0.37502238 -0.48361547 -0.54747912 -0.25683275 -0.54935658 -0.78865126\n",
            "   0.18954148  0.48371503 -0.51114231 -0.71552978  0.51145832  0.38669063\n",
            "  -0.13812828]\n",
            " [ 0.58963463 -0.48361547  1.0283258  -0.25683275  1.21764133 -1.03127774\n",
            "   1.11048828 -1.06518235  1.67588577  1.5652875   0.78447637  0.44807713\n",
            "   1.49873604]\n",
            " [ 0.0381708  -0.48361547  1.24588095 -0.25683275  2.67733525 -1.12719983\n",
            "   1.11048828 -1.14833073 -0.51114231 -0.01744323 -1.71818909  0.44807713\n",
            "   1.88793986]\n",
            " [-0.17228416 -0.48361547  1.24588095 -0.25683275  2.67733525 -0.90150078\n",
            "   1.11048828 -1.09664657 -0.51114231 -0.01744323 -1.71818909 -1.97365769\n",
            "   0.53952803]\n",
            " [-0.22932104 -0.48361547  1.58544339 -0.25683275  0.56888847 -1.76056777\n",
            "   1.11048828 -1.13471925 -0.62624905  0.18716835  1.23950646  0.44807713\n",
            "   2.99068404]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKS0C1agWi2b"
      },
      "source": [
        "### 모델 성능을 자동 검증(Validation) 하는 기능을 사용해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JG1w6ak0V8-"
      },
      "source": [
        "이전에는 검증 데이터셋을 나누기 위해서 `sklearn` 의 `train_test_split` 패키지를 사용하였는데요.<br/>\n",
        "케라스에는 보다 쉽게 검증을 할 수 있는 `validation_data`라는 편리한 기능이 있습니다.<br/>\n",
        "모델 학습 시 `validation_data`에 검증 데이터를 입력하면 케라스에서 자동으로 검증용 데이터로 사용하여 성능을 측정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Womy1ef80Xq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a794837f-a89b-4ec6-e98e-a047085e21fa"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 중요한 하이퍼 파라미터들\n",
        "inputs = x_train.shape[1]\n",
        "epochs = 75                 # 전체 반복횟수\n",
        "batch_size = 10             # 한번에 학습하는 사이즈\n",
        "\n",
        "\n",
        "# 모델을 생성합니다\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Sequential인 경우, 아래의 방법으로도 모델을 만들 수 있습니다.\n",
        "# model = Sequential(\n",
        "# [\n",
        "#     Dense(64, activation='relu', input_shape=(inputs,)),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(1)\n",
        "# ]\n",
        "# )\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
        "\n",
        "# Fit Model\n",
        "model.fit(x_train, y_train, \n",
        "          validation_data=(x_test,y_test),  # validation set\n",
        "          epochs=epochs,                    # 전체 반복횟수\n",
        "          batch_size=batch_size             # 한번에 학습하는 사이즈\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "41/41 [==============================] - 1s 6ms/step - loss: 513.8185 - mse: 513.8185 - mae: 20.5771 - val_loss: 440.6191 - val_mse: 440.6191 - val_mae: 18.8570\n",
            "Epoch 2/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 263.8185 - mse: 263.8185 - mae: 13.5170 - val_loss: 114.4852 - val_mse: 114.4852 - val_mae: 8.8211\n",
            "Epoch 3/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 64.3071 - mse: 64.3071 - mae: 5.7884 - val_loss: 49.5342 - val_mse: 49.5342 - val_mae: 5.3628\n",
            "Epoch 4/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 38.1561 - mse: 38.1561 - mae: 4.4052 - val_loss: 36.3478 - val_mse: 36.3478 - val_mae: 4.7159\n",
            "Epoch 5/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 29.1752 - mse: 29.1752 - mae: 3.7983 - val_loss: 30.4761 - val_mse: 30.4761 - val_mae: 4.4106\n",
            "Epoch 6/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 24.8251 - mse: 24.8251 - mae: 3.5279 - val_loss: 28.8393 - val_mse: 28.8393 - val_mae: 4.2481\n",
            "Epoch 7/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 21.4074 - mse: 21.4074 - mae: 3.3158 - val_loss: 26.8906 - val_mse: 26.8906 - val_mae: 3.9821\n",
            "Epoch 8/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 19.0834 - mse: 19.0834 - mae: 3.1029 - val_loss: 27.2510 - val_mse: 27.2510 - val_mae: 3.9235\n",
            "Epoch 9/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 17.6901 - mse: 17.6901 - mae: 2.9870 - val_loss: 26.7169 - val_mse: 26.7169 - val_mae: 3.8206\n",
            "Epoch 10/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 16.0241 - mse: 16.0241 - mae: 2.8144 - val_loss: 25.2538 - val_mse: 25.2538 - val_mae: 3.5959\n",
            "Epoch 11/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 14.8288 - mse: 14.8288 - mae: 2.7055 - val_loss: 25.0650 - val_mse: 25.0650 - val_mae: 3.5492\n",
            "Epoch 12/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.8804 - mse: 13.8804 - mae: 2.6228 - val_loss: 23.6842 - val_mse: 23.6842 - val_mae: 3.4222\n",
            "Epoch 13/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.7662 - mse: 12.7662 - mae: 2.5300 - val_loss: 24.0639 - val_mse: 24.0639 - val_mae: 3.3817\n",
            "Epoch 14/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 12.2726 - mse: 12.2726 - mae: 2.4822 - val_loss: 24.4291 - val_mse: 24.4291 - val_mae: 3.3468\n",
            "Epoch 15/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.6097 - mse: 11.6097 - mae: 2.4174 - val_loss: 23.4471 - val_mse: 23.4471 - val_mae: 3.2424\n",
            "Epoch 16/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.3978 - mse: 11.3978 - mae: 2.3864 - val_loss: 24.6477 - val_mse: 24.6477 - val_mae: 3.3247\n",
            "Epoch 17/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.7148 - mse: 10.7148 - mae: 2.3378 - val_loss: 23.7221 - val_mse: 23.7221 - val_mae: 3.1841\n",
            "Epoch 18/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.5771 - mse: 10.5771 - mae: 2.3146 - val_loss: 24.3325 - val_mse: 24.3325 - val_mae: 3.2318\n",
            "Epoch 19/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.2370 - mse: 10.2370 - mae: 2.2418 - val_loss: 24.6891 - val_mse: 24.6891 - val_mae: 3.2449\n",
            "Epoch 20/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.9648 - mse: 9.9648 - mae: 2.3022 - val_loss: 22.7303 - val_mse: 22.7303 - val_mae: 3.1602\n",
            "Epoch 21/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.7436 - mse: 9.7436 - mae: 2.2184 - val_loss: 23.8169 - val_mse: 23.8169 - val_mae: 3.0993\n",
            "Epoch 22/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.6233 - mse: 9.6233 - mae: 2.1906 - val_loss: 22.8636 - val_mse: 22.8636 - val_mae: 3.0484\n",
            "Epoch 23/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.4577 - mse: 9.4577 - mae: 2.1738 - val_loss: 23.2624 - val_mse: 23.2624 - val_mae: 3.0324\n",
            "Epoch 24/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.0286 - mse: 9.0286 - mae: 2.1351 - val_loss: 22.4163 - val_mse: 22.4163 - val_mae: 2.9842\n",
            "Epoch 25/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 9.0522 - mse: 9.0522 - mae: 2.1261 - val_loss: 22.9044 - val_mse: 22.9044 - val_mae: 3.0287\n",
            "Epoch 26/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.9134 - mse: 8.9134 - mae: 2.1094 - val_loss: 22.8908 - val_mse: 22.8908 - val_mae: 3.0458\n",
            "Epoch 27/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.7395 - mse: 8.7395 - mae: 2.0964 - val_loss: 22.8295 - val_mse: 22.8295 - val_mae: 3.0033\n",
            "Epoch 28/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.5079 - mse: 8.5079 - mae: 2.0614 - val_loss: 22.2930 - val_mse: 22.2930 - val_mae: 2.9294\n",
            "Epoch 29/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.6010 - mse: 8.6010 - mae: 2.0877 - val_loss: 22.7834 - val_mse: 22.7834 - val_mae: 2.9566\n",
            "Epoch 30/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.4837 - mse: 8.4837 - mae: 2.0619 - val_loss: 22.4804 - val_mse: 22.4804 - val_mae: 2.9618\n",
            "Epoch 31/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.2091 - mse: 8.2091 - mae: 2.0211 - val_loss: 21.8380 - val_mse: 21.8380 - val_mae: 2.8930\n",
            "Epoch 32/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.3417 - mse: 8.3417 - mae: 2.0508 - val_loss: 22.6182 - val_mse: 22.6182 - val_mae: 2.9425\n",
            "Epoch 33/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.9587 - mse: 7.9587 - mae: 2.0116 - val_loss: 20.5809 - val_mse: 20.5809 - val_mae: 2.8956\n",
            "Epoch 34/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.9803 - mse: 7.9803 - mae: 1.9780 - val_loss: 22.3796 - val_mse: 22.3796 - val_mae: 2.9909\n",
            "Epoch 35/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.8140 - mse: 7.8140 - mae: 1.9707 - val_loss: 21.3204 - val_mse: 21.3204 - val_mae: 2.8522\n",
            "Epoch 36/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.6402 - mse: 7.6402 - mae: 1.9637 - val_loss: 21.2019 - val_mse: 21.2019 - val_mae: 2.8105\n",
            "Epoch 37/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.5577 - mse: 7.5577 - mae: 1.9233 - val_loss: 21.7433 - val_mse: 21.7433 - val_mae: 2.8778\n",
            "Epoch 38/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.4683 - mse: 7.4683 - mae: 1.9451 - val_loss: 21.2080 - val_mse: 21.2080 - val_mae: 2.8358\n",
            "Epoch 39/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.5789 - mse: 7.5789 - mae: 1.9513 - val_loss: 21.9335 - val_mse: 21.9335 - val_mae: 2.8050\n",
            "Epoch 40/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.3291 - mse: 7.3291 - mae: 1.9422 - val_loss: 20.2355 - val_mse: 20.2355 - val_mae: 2.7814\n",
            "Epoch 41/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.3311 - mse: 7.3311 - mae: 1.9040 - val_loss: 21.5030 - val_mse: 21.5030 - val_mae: 2.8210\n",
            "Epoch 42/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 7.2515 - mse: 7.2515 - mae: 1.9113 - val_loss: 22.2483 - val_mse: 22.2483 - val_mae: 2.9005\n",
            "Epoch 43/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.2311 - mse: 7.2311 - mae: 1.8969 - val_loss: 20.2154 - val_mse: 20.2154 - val_mae: 2.7313\n",
            "Epoch 44/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.9660 - mse: 6.9660 - mae: 1.8766 - val_loss: 21.8660 - val_mse: 21.8660 - val_mae: 2.8483\n",
            "Epoch 45/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.9447 - mse: 6.9447 - mae: 1.9041 - val_loss: 18.5125 - val_mse: 18.5125 - val_mae: 2.6848\n",
            "Epoch 46/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.8153 - mse: 6.8153 - mae: 1.8506 - val_loss: 21.7268 - val_mse: 21.7268 - val_mae: 2.8713\n",
            "Epoch 47/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.7932 - mse: 6.7932 - mae: 1.8539 - val_loss: 19.7704 - val_mse: 19.7704 - val_mae: 2.7176\n",
            "Epoch 48/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.9682 - mse: 6.9682 - mae: 1.8681 - val_loss: 19.0462 - val_mse: 19.0462 - val_mae: 2.6812\n",
            "Epoch 49/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.5643 - mse: 6.5643 - mae: 1.8238 - val_loss: 21.8676 - val_mse: 21.8676 - val_mae: 2.9081\n",
            "Epoch 50/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.6385 - mse: 6.6385 - mae: 1.8294 - val_loss: 19.2720 - val_mse: 19.2720 - val_mae: 2.6748\n",
            "Epoch 51/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.4889 - mse: 6.4889 - mae: 1.7881 - val_loss: 20.4495 - val_mse: 20.4495 - val_mae: 2.7809\n",
            "Epoch 52/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.6249 - mse: 6.6249 - mae: 1.8011 - val_loss: 21.2754 - val_mse: 21.2754 - val_mae: 2.8305\n",
            "Epoch 53/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.4838 - mse: 6.4838 - mae: 1.8000 - val_loss: 19.9330 - val_mse: 19.9330 - val_mae: 2.7284\n",
            "Epoch 54/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.3905 - mse: 6.3905 - mae: 1.7959 - val_loss: 18.5436 - val_mse: 18.5436 - val_mae: 2.6496\n",
            "Epoch 55/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.3913 - mse: 6.3913 - mae: 1.7861 - val_loss: 19.0591 - val_mse: 19.0591 - val_mae: 2.7007\n",
            "Epoch 56/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.0273 - mse: 6.0273 - mae: 1.7417 - val_loss: 18.1511 - val_mse: 18.1511 - val_mae: 2.6193\n",
            "Epoch 57/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.0973 - mse: 6.0973 - mae: 1.7314 - val_loss: 19.5221 - val_mse: 19.5221 - val_mae: 2.7794\n",
            "Epoch 58/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.3323 - mse: 6.3323 - mae: 1.7717 - val_loss: 18.6291 - val_mse: 18.6291 - val_mae: 2.6708\n",
            "Epoch 59/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.0880 - mse: 6.0880 - mae: 1.7376 - val_loss: 17.4848 - val_mse: 17.4848 - val_mae: 2.5663\n",
            "Epoch 60/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 6.1398 - mse: 6.1398 - mae: 1.7498 - val_loss: 19.5780 - val_mse: 19.5780 - val_mae: 2.7333\n",
            "Epoch 61/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.7249 - mse: 5.7249 - mae: 1.6897 - val_loss: 19.5186 - val_mse: 19.5186 - val_mae: 2.7145\n",
            "Epoch 62/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.9390 - mse: 5.9390 - mae: 1.7244 - val_loss: 17.7627 - val_mse: 17.7627 - val_mae: 2.5862\n",
            "Epoch 63/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.7248 - mse: 5.7248 - mae: 1.7078 - val_loss: 17.5538 - val_mse: 17.5538 - val_mae: 2.5991\n",
            "Epoch 64/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.8570 - mse: 5.8570 - mae: 1.7061 - val_loss: 19.3286 - val_mse: 19.3286 - val_mae: 2.7114\n",
            "Epoch 65/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.7100 - mse: 5.7100 - mae: 1.7158 - val_loss: 18.3352 - val_mse: 18.3352 - val_mae: 2.6860\n",
            "Epoch 66/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.8591 - mse: 5.8591 - mae: 1.7260 - val_loss: 18.1536 - val_mse: 18.1536 - val_mae: 2.6673\n",
            "Epoch 67/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.5440 - mse: 5.5440 - mae: 1.6809 - val_loss: 17.6335 - val_mse: 17.6335 - val_mae: 2.6411\n",
            "Epoch 68/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.5592 - mse: 5.5592 - mae: 1.7016 - val_loss: 16.9596 - val_mse: 16.9596 - val_mae: 2.6148\n",
            "Epoch 69/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.5819 - mse: 5.5819 - mae: 1.6703 - val_loss: 18.5200 - val_mse: 18.5200 - val_mae: 2.6903\n",
            "Epoch 70/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.5111 - mse: 5.5111 - mae: 1.6549 - val_loss: 17.8733 - val_mse: 17.8733 - val_mae: 2.6835\n",
            "Epoch 71/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.2540 - mse: 5.2540 - mae: 1.6370 - val_loss: 17.4463 - val_mse: 17.4463 - val_mae: 2.6482\n",
            "Epoch 72/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.2086 - mse: 5.2086 - mae: 1.6161 - val_loss: 17.7344 - val_mse: 17.7344 - val_mae: 2.5712\n",
            "Epoch 73/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.0585 - mse: 5.0585 - mae: 1.6244 - val_loss: 16.0471 - val_mse: 16.0471 - val_mae: 2.5663\n",
            "Epoch 74/75\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.1012 - mse: 5.1012 - mae: 1.6088 - val_loss: 17.1979 - val_mse: 17.1979 - val_mae: 2.5995\n",
            "Epoch 75/75\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 5.1524 - mse: 5.1524 - mae: 1.6113 - val_loss: 16.9176 - val_mse: 16.9176 - val_mae: 2.6122\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6dcd54ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vEpSw0eRjM"
      },
      "source": [
        "## 신경망에서의 하이퍼 파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyAVWcHgMHJ"
      },
      "source": [
        "### 하이퍼파라미터 튜닝 방식의 종류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8hBYEoueZWH"
      },
      "source": [
        "1. **\"Babysitting\"(육아) 혹은 \"Grad Student Descent\"(대학원생 갈아넣기)**\n",
        "\n",
        "    다윈의 진화론을 아시나요? 진화론에서는 '자연 선택'이란 단어가 진화를 주도했다고 말하곤 합니다.<br/>\n",
        "하지만 하이퍼 파라미터 선택은 자연이 해주지 않습니다. 그렇다면 우리가 직접 하는 수 밖에 없겠죠?<br/>\n",
        "이전 프로젝트나 이번 스프린트에서 모델 성능을 높이기 위해 여러 숫자를 직접 넣어보며 하이퍼 파라미터를 수없이 조정했다면,<br/>\n",
        "첫 번째 방법을 수행했다고 말할 수 있겠습니다.\n",
        "\n",
        "    100% **<font color=\"ff6f61\">수작업(Manual)</font>**으로 파라미터를 수정하는 방법입니다.<br/>\n",
        "학계에서 논문을 출간할 수 있을 정도로 놀라운 정확도를 보여주는 하이퍼파라미터의 수치를 찾아내기 위해 쓰는 방법이죠.<br/>\n",
        "이를 위해서 실험자의 경험이나 도메인 지식이 필요하기도 합니다.<br/>\n",
        "~~*(물론 지도교수님들이 이 걸 직접 하시진 않습니다, 교수님의 시간은 소중하니까요...)*~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmebMkiqhadz"
      },
      "source": [
        "2. **Grid Search**\n",
        "\n",
        "    하지만 언제까지나 이렇게 하나하나 수작업으로만 시도해 볼 수는 없겠죠.<br/>\n",
        "1번 방식을 자동화한 방법이 바로 **<font color=\"ff6f61\">\"Grid Search\"</font>**입니다.<br/>\n",
        "이 방법에서는 하이퍼파라미터마다 탐색할 지점을 정해주면 모든 지점에 해당하는 조합을 알아서 수행합니다.\n",
        "\n",
        "    Grid Search는 학습을 실행한 뒤 한참 놀다오면 되는 매우 편한 방법이지만 **장점만 있는 것은 아닙니다.**<br/>\n",
        "범위를 너무 많이 설정하면 '좀 놀다 오면 끝나는' 수준을 넘어 '수료하고 취직을 하고 나서도 끝나지 않을 수도' 있는데요.<br/>\n",
        "만약 5개의 파라미터에 대해 각각 5개의 지점을 지정해주면 Grid Search는 총 $5^5=3,125$ 번의 모델 학습을 진행하게 됩니다.<br/>\n",
        "여기에 5번의 교차 검증까지 진행한다면 모델은 $3,125 \\times 5 = 15,625$ 번이나 학습을 수행합니다.<br/>\n",
        "모델 한 번 학습에 10분만 걸린다고 쳐도 **3달 반**이 걸리는 무시무시한 작업입니다. 실제로 이런 일은 없어야겠죠?\n",
        "\n",
        "    그렇기 때문에 Grid Search 로 너무 많은 하이퍼파라미터 조합을 찾으려고 하지 않는 것이 좋습니다.<br/>\n",
        "1개, 혹은 최대 2개 정도의 파라미터 최적값을 찾는 용도로 적합합니다.<br/>\n",
        "굳이 많은 하이퍼파라미터 조합을 시도할 필요는 없습니다.<br/>\n",
        "모델 성능에 **보다 직접적인 영향을 주는 하이퍼파라미터가 따로 있기 때문**인데요.<br/>\n",
        "이러한 파라미터만 제대로 튜닝해서 최적값을 찾은 후 나머지 하이퍼파라미터도 조정해나가면 못해도 90% 이상의 성능을 확보할 수 있습니다.<br/>\n",
        "이런 식으로 하나씩 접근하다 보면 적어도 무한루프가 발생하는 위험은 줄일 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy3bS48nhclC"
      },
      "source": [
        "3. **Random Search**\n",
        "\n",
        "    **<font color=\"ff6f61\">\"Random Search\"</font>** 는 무한 루프라는 Grid Search의 단점을 해결하기 위해 나온 방법입니다.<br/>\n",
        "Random Search 는 지정된 범위 내에서 무작위로 모델을 돌려본 후 최고 성능의 모델을 반환합니다.<br/> 시도 횟수를 정해줄 수 있기 때문에 Grid Search 에 비해서 훨씬 적은 횟수로도 끝마칠 수 있겠죠?\n",
        "\n",
        "    Grid Search 에서는 파라미터의 중요도가 모두 동등하다고 가정합니다.<br/>\n",
        "하지만 위에서 알아본 것처럼 실제로 더 중요한 하이퍼파라미터가 있는데요.<br/>\n",
        "Random Search 는 **상대적으로 중요한 하이퍼파라미터에 대해서는 탐색을 더 하고, 덜 중요한 하이퍼파라미터에 대해서는 실험을 덜 하도록** 합니다.\n",
        "\n",
        "    Random Search 는 절대적으로 완벽한 하이퍼파라미터를 찾아주지는 않는다는 단점을 가지고 있는데요.<br/>\n",
        "하지만 Grid Search와 비교했을 때, 학습에 걸리는 시간이 훨씬 더 적다는 점으로도 Random Search의 의의를 찾을 수 있습니다.\n",
        "\n",
        "> ❗️ ***아래 그림을 보면서 Grid Search 와 Random Search 의 차이에 대해서 생각해봅시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J5Z0d1Gr4Wa"
      },
      "source": [
        "<img src=\"https://i.imgur.com/qwySX8w.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbCIi2QWhdSX"
      },
      "source": [
        "4. **Bayesian Methods**\n",
        "\n",
        "    \"Baby sitting\" 이나 \"Grid Search\" 등의 방식에서는 탐색 결과를 보고, 결과 정보를 다시 새로운 탐색에 반영하면 성능을 더 높일 수 있었습니다.<br/> **<font color=\"ff6f61\">베이지안 방식(Bayesian Method)</font> 은 이렇게 이전 탐색 결과 정보를 새로운 탐색에 활용하는 방법**입니다.<br/>\n",
        "그렇기 때문에 베이지안 방법을 사용하면 하이퍼파라미터 탐색 효율을 높일 수 있습니다.<br/>\n",
        "`bayes_opt` 나 `hyperopt`와 같은 패키지를 사용하면 베이지안 방식을 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yZ3D2_a0ck-"
      },
      "source": [
        "### 튜닝 가능한 파라미터에는 어떤 것이 있을까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7KVdgV00em0"
      },
      "source": [
        "탐색해 볼 수 있는 하이퍼파라미터의 종류는 다음과 같습니다.\n",
        "\n",
        "- 배치 크기(batch_size)\n",
        "- 반복 학습 횟수(에포크, training epochs)\n",
        "- 옵티마이저(optimizer)\n",
        "- 학습률(learning rate)\n",
        "- 활성화 함수(activation functions)\n",
        "- Regularization(weight decay, dropout 등)\n",
        "- 은닉층(Hidden layer)의 노드(Node) 수\n",
        "\n",
        "> ❗️ ***실제로는 이보다 더 많은 하이퍼파라미터를 튜닝할 수 있습니다.<br/>\n",
        "하지만 일단은 이정도만 기억해도 좋습니다. 반복하여 시도하다 보면 익숙해질 것입니다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfAGBiZ0gC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24fa31f-72ee-490e-9ca5-a8ca8051e560"
      },
      "source": [
        "# 데이터를 불러옵니다. \n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# 정규화를 위한 함수 호출\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "print(x_train[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
            "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
            "   0.8252202 ]\n",
            " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
            "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
            "  -1.32920239]\n",
            " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
            "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
            "  -1.30850006]\n",
            " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
            "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
            "  -0.65292624]\n",
            " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
            "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
            "   0.26349695]\n",
            " [-0.37502238 -0.48361547 -0.54747912 -0.25683275 -0.54935658 -0.78865126\n",
            "   0.18954148  0.48371503 -0.51114231 -0.71552978  0.51145832  0.38669063\n",
            "  -0.13812828]\n",
            " [ 0.58963463 -0.48361547  1.0283258  -0.25683275  1.21764133 -1.03127774\n",
            "   1.11048828 -1.06518235  1.67588577  1.5652875   0.78447637  0.44807713\n",
            "   1.49873604]\n",
            " [ 0.0381708  -0.48361547  1.24588095 -0.25683275  2.67733525 -1.12719983\n",
            "   1.11048828 -1.14833073 -0.51114231 -0.01744323 -1.71818909  0.44807713\n",
            "   1.88793986]\n",
            " [-0.17228416 -0.48361547  1.24588095 -0.25683275  2.67733525 -0.90150078\n",
            "   1.11048828 -1.09664657 -0.51114231 -0.01744323 -1.71818909 -1.97365769\n",
            "   0.53952803]\n",
            " [-0.22932104 -0.48361547  1.58544339 -0.25683275  0.56888847 -1.76056777\n",
            "   1.11048828 -1.13471925 -0.62624905  0.18716835  1.23950646  0.44807713\n",
            "   2.99068404]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXI7EDE2gYQs"
      },
      "source": [
        "#### 배치 사이즈(Batch Size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLE2eIMjhwzi"
      },
      "source": [
        "**<font color=\"ff6f61\">배치 사이즈(Batch size)</font>**는 순전파/역전파를 통해 모델의 가중치를 업데이트 할 때마다,<br/>\n",
        "즉 매 iteration 마다 **몇 개의 입력 데이터를 볼지를** 결정하는 하이퍼파라미터입니다.<br/>\n",
        "최적의 배치 사이즈를 찾는 과정은 왜 중요할까요?\n",
        "\n",
        "**배치 사이즈를 너무 크게 하면** 한 번에 많은 데이터에 대한 Loss를 계산해야 한다는 단점이 생깁니다.<br/>\n",
        "이럴 경우 가중치 업데이트가 빠르게 이루어지지 않는데다, 주어진 Epoch 안에 충분한 횟수의 iteration을 확보할 수 없게 됩니다.<br/>\n",
        "그리고 파라미터가 굉장히 많은 모델에 큰 배치 사이즈를 적용하게 될 경우 메모리를 초과해버리는 현상(Out-of-Memory)이 발생하기도 합니다.<br/>\n",
        "반대로 **배치 사이즈를 너무 작게 설정하면** 학습에 오랜 시간이 걸리고, 노이즈가 많이 발생한다는 단점도 있습니다.\n",
        "\n",
        "일반적으로 배치 사이즈는 $32-512$ 사이의 2의 제곱수로 결정하여 줍니다.<br/>\n",
        "케라스 배치 사이즈의 기본값(Default)은 $32$ 로 설정되어 있습니다.\n",
        "\n",
        "> ❗️ ***아래의 글은 나중에 학습하면서 보아도 좋습니다. 일단은 건너뛰고 추후 학습시에 참조하도록 합시다.<br/>\n",
        "이미지 처리에서 작은 배치 사이즈($<32$)를 잘 설정하면 일반화(Generalization) 성능을 높일 수 있다는 내용의 [논문](https://arxiv.org/abs/1804.07612)입니다.<br/>\n",
        "배치 사이즈를 왜 2의 제곱수로 설정하는지 궁금하다면 다음 Stackoverflow [링크](https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2)를 참조해봅시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA45tZ2gnAzA"
      },
      "source": [
        "배치 사이즈를 조정하여 최적의 배치 사이즈를 찾아보겠습니다.\n",
        "\n",
        "1. **필요한 패키지를 import 합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHljoCgV0jsv"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKv1O8YGnhkl"
      },
      "source": [
        "2. **재현성을 위해 랜덤시드를 고정합니다**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV0abQqmnEU-"
      },
      "source": [
        "numpy.random.seed(1100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVPaC9BKnoKF"
      },
      "source": [
        "3. **데이터셋을 불러온 후에 Feature 와 Label로 분리합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMe4iH0LnEPS"
      },
      "source": [
        "# 데이터셋을 불러옵니다.\n",
        "url =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "\n",
        "dataset = pd.read_csv(url, header=None).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCE86l1XnENJ"
      },
      "source": [
        "# 불러온 데이터셋을 X와 Y로 나눕니다\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuMkvkCCnxUy"
      },
      "source": [
        "4. **모델을 제작합니다.**\n",
        "\n",
        "`KerasClassifier` 로 wrapping 하기 위하여 함수로 정의합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBqWHYPknHe_"
      },
      "source": [
        "def create_model():\n",
        "    # 모델 제작\n",
        "    model = Sequential()\n",
        "    model.add(Dense(100, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # 모델 컴파일링\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkFMgOpAoVCu"
      },
      "source": [
        "4. **`KerasClassifier` 로 wrapping 하여줍니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y459TFBFnNvW"
      },
      "source": [
        "# keras.wrapper를 활용하여 분류기를 만듭니다\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhn1yvGnNq6"
      },
      "source": [
        "# GridSearch\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [30]\n",
        "param_grid = dict(batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEVdKbdgolHD"
      },
      "source": [
        "5. **하이퍼파라미터 탐색을 위한 `GridSearchCV` 를 설정하고 학습합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqwMdrE5nNmQ",
        "outputId": "82cdc5f0-7b01-4dfa-9051-a5f76edd9203"
      },
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa6e72f0680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fa6e709e7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDzoXSrmotJK"
      },
      "source": [
        "6. **최적의 결과를 낸 하이퍼파라미터와 각각의 결과를 출력해봅시다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoubhatunBXs",
        "outputId": "9392207d-5298-46dd-c34b-c113796a9bc3"
      },
      "source": [
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.5793650805950165 using {'batch_size': 80}\n",
            "Means: 0.5728715777397155, Stdev: 0.056264070333286655 with: {'batch_size': 10}\n",
            "Means: 0.5586028277873993, Stdev: 0.06420384197886535 with: {'batch_size': 20}\n",
            "Means: 0.5612596690654754, Stdev: 0.07808589018263502 with: {'batch_size': 40}\n",
            "Means: 0.566581791639328, Stdev: 0.08425900074864029 with: {'batch_size': 60}\n",
            "Means: 0.5793650805950165, Stdev: 0.061918396158844716 with: {'batch_size': 80}\n",
            "Means: 0.4463627874851227, Stdev: 0.11636758131443188 with: {'batch_size': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cshSz0y2gdjf"
      },
      "source": [
        "#### 옵티마이저(Optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qN7a51_0lsO"
      },
      "source": [
        "**<font color=\"ff6f61\">옵티마이저(Optimizer)</font>** 역시 굉장히 중요한 하이퍼파라미터입니다.<br/>\n",
        "`adam` 이라는 옵티마이저가 꽤 좋은 성능을 보장하기 때문에 많이 사용됩니다.<br/>\n",
        "최근에는 `adam` 을 개선하거나 기능을 추가한 `adamW, adamP`와 같은 옵티마이저도 사용되죠.<br/>\n",
        "중요한 점은 **\"모든 경우에 좋은 옵티마이저란 없다\"**는 것인데요.<br/>\n",
        "그렇기 때문에 모델에 따라, 데이터셋에 따라 적절한 옵티마이저를 잘 설정해주어야 합니다.\n",
        "\n",
        "게다가 어떤 옵티마이저를 선택하는 지에 따라서 최적의 하이퍼파라미터 값이 달라집니다.<br/>\n",
        "그렇기 때문에 옵티마이저를 다르게 해줄 때마다 적절한 학습률(`learning rate`)과 모멘텀(`momentum`) 등을 다르게 설정해주는 것이 좋습니다.\n",
        "\n",
        "> ❗️ ***여러 가지 옵티마이저가 잘 정리된 [블로그 글](https://sacko.tistory.com/42)도 읽어보도록 합시다.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z-MucU3gfBy"
      },
      "source": [
        "#### 학습률(Learning Rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLBKaEWS0nl8"
      },
      "source": [
        "**<font color=\"ff6f61\">학습률(Learning rate, `lr`)</font>** 은 옵티마이저에서 지정해 줄 수 있는 하이퍼파라미터 중 하나입니다.\n",
        "\n",
        "Keras에서 학습률의 기본값은 어떻게 설정되어 있을까요?<br/>\n",
        "[링크](https://www.google.com/search?q=keras+default+learning+rate&oq=keras+default+learning+rate&aqs=chrome..69i57j0i22i30l2.4191j0j7&sourceid=chrome&ie=UTF-8)에서 볼 수 있듯 케라스의 기본 학습률은 0.001로 설정되어 있습니다.\n",
        "\n",
        "학습률이 너무 높으면 경사 하강 과정에서 발산하면서 모델이 최적값을 찾을 수 없게 되어버립니다.<br/>\n",
        "반대로 너무 낮게 설정할 경우에는 최적점에 이르기까지 너무 오래 걸리거나, 주어진 iteration 내에서 모델이 수렴하는데 실패하기도 합니다.<br/>\n",
        "그렇기 때문에 최적의 학습률을 찾는 것은 학습에서 중요한 요소입니다.<br/>\n",
        "\n",
        "> ❗️ ***아래는 학습률이 너무 클 때와 작을 때의 경사하강법을 나타낸 그림입니다.<br/>\n",
        "그림을 기억하면서 최적의 학습률이 왜 중요한 지에 대해 생각해봅시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ied5F0zsvRBt"
      },
      "source": [
        "<img src=\"https://i.imgur.com/RfBFgKs.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXQJ8MUyvRVi"
      },
      "source": [
        "처음에는 $[0.001, 0.01, 0.1, 0.2, 0.3, 0.5]$ 정도로 넓은 범위에서 크기 순으로 학습률을 튜닝해봅니다.<br/>\n",
        "학습률을 0.5 정도로 높게 잡는 것은 추천하지 않지만, 이 때의 결과를 분석하며 과하게 높은 학습률이 왜 좋지 않은지를 경험해보는 것도 좋은 공부가 됩니다.\n",
        "\n",
        "위 과정을 통해 최적의 학습률 값을 찾았다면 해당 학습률 주변에서 다시 최적의 학습률 값을 찾아봅시다.<br/>\n",
        "만약 위에서 학습률 값이 0.1일 때, 가장 좋은 성능을 보였다면 $[0.05, 0.08, 0.1, 0.12, 0.15]$ 정도로 시도해보면 좋습니다.\n",
        "\n",
        "학습률을 조정하면 최적값에 도달할 수 있는 iteration의 횟수 역시 변경됩니다.<br/>\n",
        "그렇기 때문에 학습률을 튜닝할 때에는 Epoch 의 횟수도 함께 튜닝해주는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmmfY0IFghKH"
      },
      "source": [
        "#### 모멘텀(Momentum)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEPscoEr0pVO"
      },
      "source": [
        "**<font color=\"ff6f61\">모멘텀(Momentum)</font>**은 옵티마이저에 관성을 부여하는 하이퍼파라미터입니다.<br/>\n",
        "모멘텀은 이전 iteration에서 경사 하강을 한 정도를 새로운 iteration에 반영합니다.<br/>\n",
        "그렇기 때문에 아래 그림에 등장하는 **지역 최저점(Local minima)에 빠지지 않을 수 있도록 합니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0y6_JHN5YJD"
      },
      "source": [
        "<img src=\"https://i.imgur.com/1F0NNID.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IzxZB8gkWL"
      },
      "source": [
        "#### 가중치 초기화(Network Weight Initialization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yuEYZrJ58kr"
      },
      "source": [
        "초기 가중치를 어떻게 설정할 지를 결정하는 **<font color=\"ff6f61\">가중치 초기화(Weight initialization)</font>**는 신경망에서 매우 중요한 요소입니다.\n",
        "\n",
        "신경망의 가중치를 초기화 하는 방법은 여러 가지가 있습니다.<br/>\n",
        "케라스에서는 아래와 같은 가중치 초기화 방법을 제공하고 있습니다.\n",
        "\n",
        "```python\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "```\n",
        "\n",
        "아래에서는 먼저 가중치를 정규분포로 초기화하였을 때의 문제를 알아보고<br/>\n",
        "흔히 사용되는 **Xavier(=`glorot`) 초기화**와 **He(=`he`) 초기화**에 대해 알아보도록 하겠습니다.\n",
        "\n",
        "> ❗️ ***아래 가중치 초기화 방법의 수식을 외울 필요는 없습니다.<br/>\n",
        "일단은 어떤 초기화 방법이 있고 해당 초기화 방법이 언제 사용되는지만 기억하고 넘어갑시다.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWTdsIs78U5D"
      },
      "source": [
        "1. **표준편차를 1인 정규분포로 가중치를 초기화 할 때 각 층의 활성화 값 분포**\n",
        "\n",
        "표준편차가 일정한 정규분포로 가중치를 초기화 해 줄 때에는 대부분의 활성화 값이 0과 1에 위치하는 것을 볼 수 있습니다.<br/>\n",
        "활성값이 고르지 못할 경우 학습이 제대로 이루어지지 않는데요.<br/>\n",
        "그렇기 때문에 가장 간단한 방법임에도 잘 사용되지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHzudijR0rJc"
      },
      "source": [
        "\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/994C2F3C5AB623C526\" width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgdgm0Iq9bwe"
      },
      "source": [
        "2. **Xavier 초기화를 해주었을 때의 활성화 값의 분포**\n",
        "\n",
        "**Xavier 초기화(Xavier initialization)**는 가중치를 표준편차가 고정값인 정규분포로 초기화 했을 때의 문제점을 해결하기 위해 등장한 방법입니다.<br/>\n",
        "Xavier 초기화는 이전 층의 노드가 $n$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{1}{\\sqrt{n}}$ 인 정규분포로 초기화합니다.<br/>\n",
        "*(케라스에서 Xavier 초기화는 이전 층의 노드가 $n$ 개이고 현재 층의 노드가 $m$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{2}{\\sqrt{n+m}}$ 인 정규분포로 초기화합니다.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA6cWaUx-ckK"
      },
      "source": [
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbuwPPz%2FbtquO7Wq9Rp%2Fylz2Qsc0fi9m0TaQNXBYDK%2Fimg.png\" width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq6cLN81_W97"
      },
      "source": [
        "3. **He 초기화를 해주었을 때의 활성화 값의 분포**\n",
        "\n",
        "Xavier 초기화는 활성화 함수가 시그모이드(`Sigmoid`)인 신경망에서는 잘 동작합니다.<br/>\n",
        "하지만 활성화 함수가 ReLU 일 경우에는 층이 지날수록 활성값이 고르지 못하게 되는 문제를 보이는데요.\n",
        "\n",
        "이런 문제를 해결하기 위해 등장한 것이 바로 **He 초기화(He initialization)** 입니다.<br/>\n",
        "He 초기화는 이전 층의 노드가 $n$ 개일 때, 현재 층의 가중치를 표준편차가 $\\frac{2}{\\sqrt{n}}$ 인 정규분포로 초기화합니다.<br/>\n",
        "He 초기화를 적용하면 아래 그림처럼 층이 지나도 활성값이 고르게 유지되는 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqY4cSZO-epo"
      },
      "source": [
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcKBoWH%2FbtquO7B8MfF%2FMs5LyROpCV89EbCFNXja4k%2Fimg.png\" width=500/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OzHTH-nAQZt"
      },
      "source": [
        "가중치 초기화 방법을 요약하면 다음과 같습니다.\n",
        "\n",
        "**c.f. Activation function에 따른 초기값 추천**\n",
        "1. Sigmoid  ⇒  Xavier 초기화를 사용하는 것이 유리 \n",
        "2. ReLU  ⇒  He 초기화 사용하는 것이 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUQJPwIegmzb"
      },
      "source": [
        "#### (복습) 활성화 함수(Activation Function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HATUa-Sv0uo0"
      },
      "source": [
        "은닉층의 활성화 함수 역시 조정해 볼 수 있는 하이퍼파라미터 중 하나입니다.<br/>\n",
        "지난 몇 시간에 걸쳐 이미 몇 가지 활성화 함수를 소개드렸는데요.<br/>\n",
        "보통은 **은닉층에는 ReLU를, 출력층에는 문제에 따라 Sigmoid (이진 분류)나 Softmax(다중 분류)를 적용**하는 것을 볼 수 있었습니다.\n",
        "\n",
        "하지만 모델에 따라서 은닉층에도 시그모이드(`sigmoid`)나 Hyperbolic tangent(`tanh`) 등의 다른 활성화 함수를 적용할 수 있습니다.<br/>\n",
        "여러 경우를 시도해보고 결과가 더 괜찮게 나오는지 확인해 보도록 합시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSR6BtqWgpJd"
      },
      "source": [
        "#### (복습) Regularization(weight decay, dropout 등)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xooTGQQU0wWo"
      },
      "source": [
        "과적합(Overfitting)을 방지하기 위한 Regularization을 적용한다면 이 또한 중요한 하이퍼파라미터입니다.\n",
        "\n",
        "**가중치 감소(Weight decay)를 얼마나 적용**할 것인지 혹은 **가중치 제한(Weight constraint)의 범위를 어떻게 설정**할 것인지에 따라 신경망의 성능이 결정됩니다.\n",
        "\n",
        "드롭아웃 값은 **매 iteration 마다 임의로 비활성화 하고 싶은 뉴런의 비율**입니다.<br/>\n",
        "다음 주에 등장하는 다양한 모델에서는 드롭아웃이 많이 적용되어 있는 것을 보실 수 있습니다.<br/>\n",
        "그 때, '과적합을 해결하기 위해서 드롭아웃을 적용한 것이구나'를 기억하실 수 있으면 좋겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIYvwaVYgrT8"
      },
      "source": [
        "#### (복습) 은닉층 노드 수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpdzmlAX0xpJ"
      },
      "source": [
        "일반적으로 은닉층의 노드 수를 늘림으로써 모델을 복잡하게 만들어 줄 수록 데이터의 복잡한 패턴을 잘 이해할 수 있습니다.<br/>\n",
        "하지만 노드가 많아지고 층이 깊어질수록 학습 시간이 길어지고 과적합에 대한 위험이 늘어나게 되는데요.<br/>\n",
        "그렇기 때문에 **각 층의 노드 수를 잘 조정하는 것 역시 딥러닝의 성능을 높이기 위해서 중요한 요소**\n",
        "입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW76-VK800nN"
      },
      "source": [
        "## 라이브러리를 사용한 하이퍼파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzeyt1jRFenG"
      },
      "source": [
        "### Keras Tuner 를 사용하여 Fashion MNIST 예제를 위한 최적의 하이퍼파라미터 탐색하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO5YYXzw02kI"
      },
      "source": [
        "\n",
        "**<font color=\"ff6f61\">Keras Tuner</font>** 는 케라스 프레임워크에서 하이퍼파라미터를 튜닝하는 데 도움이 되는 라이브러리입니다.<br/>\n",
        "Fashion MNIST 예제에 Keras Tuner를 적용하여 하이퍼파라미터 튜닝을 수행해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPVzHEC0FwQJ"
      },
      "source": [
        "1. **필요한 패키지를 import 합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8zKgeqO1xrI"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjsjRrOE4eJz"
      },
      "source": [
        "2. **Keras Tuner를 설치한 후 import 합니다.**\n",
        "\n",
        "Keras Tuner는 Colab에 내장된 패키지가 아니기 때문에 따로 설치를 해준 후에 import 하여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5usUa4k4ddP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86db0128-0af8-460e-c9b7-70abb9b638e9"
      },
      "source": [
        "!pip install -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.41.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at8ueGzt4gf6"
      },
      "source": [
        "3. **데이터셋을 불러온 후에 정규화(Normalizing) 해줍니다.**\n",
        "\n",
        "Fashion MNIST 데이터셋을 불러온 후에 이미지를 0-1 사이의 값으로 정규화합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySHk6qzu4idS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b2de90-cfdc-4b86-fdf4-f21d8441d18b"
      },
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geA0XOpd4j9A"
      },
      "source": [
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6pNZKnf4msb"
      },
      "source": [
        "4. **Model을 제작합니다.**\n",
        "\n",
        "모델을 제작하고 탐색할 하이퍼파라미터 범위와 지점을 정의합니다.<br/>\n",
        "\n",
        "아래 단계에서 2가지 방법을 적용해 볼 수 있는데요.\n",
        "1. Model builder function(`model_builder`)을 사용\n",
        "2. Keras Tuner API의 `HyperModel` 클래스에 있는 분류기를 사용 ▶️ [링크](https://keras.io/api/keras_tuner/hypermodels/base_hypermodel/)를 참조해봅시다.\n",
        "\n",
        "아래에서는 1번에 해당하는 방법을 수행해보겠습니다.<br/>\n",
        "`model_builder` 라는 함수를 정의하고 해당 함수 내부에서 모델 설계와 하이퍼파라미터 튜닝까지 모두 수행할 수 있도록 하였습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFZazFB3I6-6"
      },
      "source": [
        "> ❗️ ***심화 학습 : 나중에 보도록 합시다.<br/>\n",
        "이미지 처리를 위한 몇 가지 모델에서는 HyperModel의 하위 클래스인 [HyperXception](https://keras.io/api/keras_tuner/hypermodels/hyper_xception/#hyperxception-class) 및 [HyperResNet](https://keras.io/api/keras_tuner/hypermodels/hyper_resnet/#hyperresnet-class) 등을 적용할 수 있습니다.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NQgLcp64nNB"
      },
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(Flatten(input_shape=(28, 28)))\n",
        "  \n",
        "  # 첫 번째 Dense layer에서 노드 수를 조정(32-512)합니다.\n",
        "  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
        "  model.add(Dense(units = hp_units, activation = 'relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # Optimizer의 학습률(learning rate)을 조정[0.01, 0.001, 0.0001]합니다. \n",
        "  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n",
        "  \n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),\n",
        "                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QHPhoG4qFM"
      },
      "source": [
        "5. **하이퍼파라미터 튜닝을 수행할 튜너(Tuner)를 지정합니다.**\n",
        "\n",
        "Keras Tuner 에서는 **Random Search, Bayesian Optimization, Hyperband** 등의 최적화 방법을 수행할 수 있습니다.<br/>\n",
        "아래에서는 `Hyperband` 를 통해서 튜닝을 수행해보도록 하겠습니다.\n",
        "\n",
        "Hyperband 사용 시 Model builder function(`model_builder`), 훈련할 최대 epochs 수(`max_epochs`) 등을 지정해주어야 합니다.<br/>\n",
        "Hyperband 는 리소스를 알아서 조절하고 조기 종료(Early-stopping) 기능을 사용하여 \n",
        "높은 성능을 보이는 조합을 신속하게 통합한다는 장점을 가지고 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBbsQL-a4q07"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'val_accuracy', \n",
        "                     max_epochs = 10,\n",
        "                     factor = 3,\n",
        "                     directory = 'my_dir',\n",
        "                     project_name = 'intro_to_kt')                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-xByktJ4wZx"
      },
      "source": [
        "6. **Callback 함수를 지정합니다.**\n",
        "\n",
        "하이퍼파라미터 탐색을 실행하기 전에 학습이 끝날 때마다 출력을 지우도록 콜백 함수를 정의해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnacgPFJ4usU"
      },
      "source": [
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "  def on_train_end(*args, **kwargs):\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUiW8GdP40It",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe00d12-110d-4191-b07a-589f6a9c3247"
      },
      "source": [
        "tuner.search(img_train, label_train, epochs = 10, validation_data = (img_test, label_test), callbacks = [ClearTrainingOutput()])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "하이퍼 파라미터 검색이 완료되었습니다. \n",
        "최적화된 첫 번째 Dense 노드 수는 {best_hps.get('units')} 입니다.\n",
        "최적의 학습 속도는 {best_hps.get('learning_rate')} 입니다.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 01m 22s]\n",
            "val_accuracy: 0.8640999794006348\n",
            "\n",
            "Best val_accuracy So Far: 0.886900007724762\n",
            "Total elapsed time: 00h 17m 22s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "\n",
            "하이퍼 파라미터 검색이 완료되었습니다. \n",
            "최적화된 첫 번째 Dense 노드 수는 352 입니다.\n",
            "최적의 학습 속도는 0.001 입니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnVR4xj43cm"
      },
      "source": [
        "7. **최고 성능을 보이는 하이퍼파라미터 조합으로 다시 학습을 진행해봅시다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3046jg3s43zR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088e76f7-192f-4669-c9bc-fc42b43380f6"
      },
      "source": [
        "# 최적의 하이퍼 파라미터를 사용하여 모델을 구축하고 데이터에 대해 교육\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 352)               276320    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                3530      \n",
            "=================================================================\n",
            "Total params: 279,850\n",
            "Trainable params: 279,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjtkdgh845Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcb70b8-4cff-4bd8-f6f0-f4e551214355"
      },
      "source": [
        "model.fit(img_train, label_train, epochs = 10, validation_data = (img_test, label_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4785 - accuracy: 0.8298 - val_loss: 0.4736 - val_accuracy: 0.8252\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3600 - accuracy: 0.8684 - val_loss: 0.3956 - val_accuracy: 0.8545\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3228 - accuracy: 0.8812 - val_loss: 0.3545 - val_accuracy: 0.8738\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2992 - accuracy: 0.8885 - val_loss: 0.3475 - val_accuracy: 0.8718\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2824 - accuracy: 0.8956 - val_loss: 0.3423 - val_accuracy: 0.8742\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2676 - accuracy: 0.8994 - val_loss: 0.3224 - val_accuracy: 0.8789\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2537 - accuracy: 0.9045 - val_loss: 0.3770 - val_accuracy: 0.8722\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2453 - accuracy: 0.9088 - val_loss: 0.3202 - val_accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2328 - accuracy: 0.9126 - val_loss: 0.3400 - val_accuracy: 0.8833\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2255 - accuracy: 0.9161 - val_loss: 0.3372 - val_accuracy: 0.8857\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6dcdaff90>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeCzV6vb47WG"
      },
      "source": [
        "tuner에서 지정한 `my_dir/intro_to_kt` 경로에는 하이퍼파라미터 탐색 중에 실행되는 모든 모델에 대한 세부 로그와 체크포인트가 저장되어 있습니다.<br/>\n",
        "동일한 모델로 하이퍼파라미터 탐색을 다시 실행할 때, Keras Tuner 기존 로그를 참고하여 검색을 시작합니다.<br/>\n",
        "이 동작을 비활성화하려면 튜너 설정시 `overwrite = True` 로 지정해주어야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_y5Kt0_49Ze"
      },
      "source": [
        "> ❗️ ***Keras Tuner 를 더 알아보기 위한 학습 자료<br/>\n",
        "1) [Keras Tuner 텐서플로우 블로그](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)<br/>\n",
        "2) [Keras Tuner 공식문서](https://keras-team.github.io/keras-tuner/)<br/>\n",
        "3) [HParams 대시보드 in Tensorboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) : 하이퍼파라미터 튜닝을 위한 대시보드 만들기***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM8pTH8G5HwM"
      },
      "source": [
        "### 실험 기록 툴(wandb 등)에 대해 알아보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRWcUS0E5Jt1"
      },
      "source": [
        "- **실험 기록을 하는 이유는 무엇일까요?**\n",
        "\n",
        "다양한 하이퍼파라미터를 변경해가면서 장기적으로 실험을 진행하다보면 점점 결과를 관리하기가 어려워집니다.<br/>\n",
        "\"어떤 파라미터 조합이 제일 좋았지?\", \"어제 했던 결과와 차이가 있었던가?\" 와 같은 의문을 품게 되죠.<br/>\n",
        "비록 노트북(`.ipynb`)이 어느정도 출력물을 기록하기는 있지만 모든 실험 결과를 관리하기엔 적절하지 않습니다.\n",
        "\n",
        "**Comet.ml, Weights and Biases(`wandb`)** 등은 이러한 문제를 해결하기 위해 등장한 실험 기록 도구입니다.<br/>\n",
        "이런 실험 기록 도구는 실험 결과를 실시간으로 기록하고 **코드와 결과값을 보관**해주며,<br/>\n",
        "실험 결과를 원하는 기준대로 언제든지 **시각화하여 모델의 성능을 확인**할 수 있도록 도와줍니다.<br/>\n",
        "매 Epoch이 끝날 때마다 데이터가 해당 툴에 보내지기 때문에 **모델이 수렴하고 있는지도 확인**할 수 있습니다.\n",
        "\n",
        "이번 시간에는 Weights and Biases(`wandb`)를 활용하여 실험 기록을 수행해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3HC-sqBRMl_"
      },
      "source": [
        "#### Wandb 이용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1C6vLOT5M2A"
      },
      "source": [
        "- 설치 및 회원가입<br/>\n",
        "먼저 다음 셀을 실행하기 전에 터미널에서 `wandb`에 로그인이 되있어야 합니다. \n",
        "\n",
        "    ```zsh\n",
        "# 아래의 커맨드를 실행합니다\n",
        "wandb.login\n",
        "```\n",
        "구체적인 방법은 Weights and Biases의 [QuickStart](https://docs.wandb.com/quickstart)를 참고해주시면 되겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O63aPu0Q5NaD"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WE9GX205PSY"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqyiIdto5QmL"
      },
      "source": [
        "# group, project 변수를 설정합니다. 반복되는 이름이 많기 때문에 변수로 설정하여 사용하면 편리합니다.\n",
        "wandb_project = \"review\"\n",
        "wandb_group = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uylEpUl5QkG"
      },
      "source": [
        "# !git clone http://github.com/wandb/tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcpElHH-5Qhs"
      },
      "source": [
        "# !cd tutorial; pip install --upgrade -r requirements.txt;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aphtSCEI5Qfz"
      },
      "source": [
        "!wandb login 6a1f7dd199ef2c241cc2dafb7ae52925d6de7385"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAG7-WRx5QdT"
      },
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.python import keras\n",
        "#from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsju3z435XRX"
      },
      "source": [
        "# !python -c \"import keras; print(keras.__version__)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAqrWvBb5XPT"
      },
      "source": [
        "wandb.init(project=wandb_project)  ## 내가 만든 프로젝트 이름을 넣어주어야 합니다.\n",
        "#wandb.init(project=wandb_project, entity=wand_group) \n",
        "\n",
        "# 데이터 및 하이퍼파라미터 설정 \n",
        "X =  x_train\n",
        "y =  y_train\n",
        "\n",
        "inputs = X.shape[1]\n",
        "wandb.config.epochs = 50\n",
        "wandb.config.batch_size = 10\n",
        "\n",
        "# 모델을 구축합니다\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "# 모델을 컴파일 합니다\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
        "\n",
        "# 모델을 학습합니다\n",
        "model.fit(X, y, \n",
        "          validation_split=0.3, \n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80pkhHk35XM_"
      },
      "source": [
        "wandb.init(project=wandb_project)  ## 내가 만든 프로젝트 이름을 넣어주어야 합니다.\n",
        "\n",
        "# 데이터 및 하이퍼파라미터 설정 \n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "wandb.config.epochs = 10\n",
        "wandb.config.batch_size = 64\n",
        "\n",
        "# 모델을 구축합니다\n",
        "model = Sequential() ## 과제시에는 이 모델을 Tre-trained model로 대체하면 됩니다. \n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOGeCn0p5Qaj"
      },
      "source": [
        "# 모델학습방식을 정의함\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습시키기\n",
        "model.fit(train_images, train_labels, \n",
        "          validation_data=(test_images, test_labels),\n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WVv7nu358Ro"
      },
      "source": [
        "성능이 마음에 안든다면 추가로 학습을 더 시키는 방법도 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQxsLXFF5crH"
      },
      "source": [
        "wandb.config.epochs = 20\n",
        "wandb.config.batch_size = 512\n",
        "\n",
        "model.fit(train_images, train_labels, \n",
        "          validation_data=(test_images, test_labels),\n",
        "          epochs=wandb.config.epochs, \n",
        "          batch_size=wandb.config.batch_size, \n",
        "          callbacks=[WandbCallback()]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbjpPKmV5cm9"
      },
      "source": [
        "!ls wandb/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SWu4wC_6BR7"
      },
      "source": [
        "다음으로는 프로그램 웹 페이지로 접속해서 분석해보는 시간을 가집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HviUignb70uS"
      },
      "source": [
        "## 🧐  Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZt9U2JL6Ce0"
      },
      "source": [
        "- 각 키워드에 대해서 간략하게 설명할 수 있는지 확인해 봅시다.\n",
        "    - Activation Functions(활성화 함수)\n",
        "    - Optimizer(옵티마이저)\n",
        "    - Number of Layers\n",
        "    - Number of Neurons\n",
        "    - Batch Size(배치 사이즈)\n",
        "    - Dropout(드롭아웃)\n",
        "    - Learning Rate(학습률)\n",
        "    - Number of Epochs\n",
        "    - and many more\n",
        "- Scikit-learn 과 Keras Tuner 를 통해서 여러분이 구축한 신경망에 여러 가지 하이퍼파라미터 조정 방법을 적용할 수 있는지 확인해봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvMGbVXt8qTv"
      },
      "source": [
        "### 🔝 References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09Fk1jfS6EDA"
      },
      "source": [
        "- [Grid Search Hyperparameters for Deep Learning](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)\n",
        "- [Hyperparameters Optimization for Deep Learning Models](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/)\n",
        "- [Dropout Regularization in Deep Learning](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n",
        "- [Weight Constraints in Deep Learning](https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/)\n",
        "- [Number of Layers and Nodes in a Neural Network](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)\n",
        "- [Batch Normalization](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)\n",
        "\n"
      ]
    }
  ]
}